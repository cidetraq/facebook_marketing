{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Approach</h1>\n",
    "Use CountVectorizer with cleaned and tokenized text with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from sklearn import naive_bayes, metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7516"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isear=pd.read_csv('isear_1.csv', header=-1)\n",
    "isear=isear.drop(2, axis=1)\n",
    "isear[0][isear[0]=='guit']='guilt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                         'even though', 'yet']\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop if i not in negative])\n",
    "    punc_free = \"\".join([ch for ch in stop_free if ch not in exclude])\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Only use below for certain implementations of Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "for index, row in isear.iterrows():\n",
    "    text.append(clean(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=isear[1].apply(clean)\n",
    "y=isear[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "y=np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Below step only for non-neural network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=encoder.fit_transform(train_y)\n",
    "valid_y=encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6012x8511 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 58704 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.5485372340425532\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Outcome of Naive Bayes</h2>\n",
    "\n",
    "NB, Count Vectors:  Accuracy of 0.5485372340425532\n",
    "Now try neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without wrapper\n",
    "classifier=Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 8511))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "#Adding dropout\n",
    "classifier.add(Dropout(0.4, seed=0))\n",
    "\n",
    "#Adding the third hidden layer\n",
    "classifier.add(Dense(units=10, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "#Adding dropout\n",
    "classifier.add(Dropout(0.4, seed=0))\n",
    "\n",
    "# Adding the output layer (Softmax because we want probabilities within 0 and 1 )\n",
    "classifier.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN (gradient descent with logarithmic loss)\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6012/6012 [==============================] - 2s 312us/step - loss: 1.2936 - acc: 0.4892\n",
      "Epoch 2/100\n",
      "6012/6012 [==============================] - 2s 312us/step - loss: 1.3195 - acc: 0.4854\n",
      "Epoch 3/100\n",
      "6012/6012 [==============================] - 2s 312us/step - loss: 1.3029 - acc: 0.4877\n",
      "Epoch 4/100\n",
      "6012/6012 [==============================] - 2s 313us/step - loss: 1.3010 - acc: 0.4840\n",
      "Epoch 5/100\n",
      "6012/6012 [==============================] - 2s 316us/step - loss: 1.3002 - acc: 0.4920\n",
      "Epoch 6/100\n",
      "6012/6012 [==============================] - 2s 311us/step - loss: 1.2870 - acc: 0.4917\n",
      "Epoch 7/100\n",
      "6012/6012 [==============================] - 2s 315us/step - loss: 1.3053 - acc: 0.4879\n",
      "Epoch 8/100\n",
      "6012/6012 [==============================] - 2s 316us/step - loss: 1.2840 - acc: 0.4977\n",
      "Epoch 9/100\n",
      "6012/6012 [==============================] - 2s 330us/step - loss: 1.2935 - acc: 0.4797\n",
      "Epoch 10/100\n",
      "6012/6012 [==============================] - 2s 317us/step - loss: 1.2803 - acc: 0.4933\n",
      "Epoch 11/100\n",
      "6012/6012 [==============================] - 2s 313us/step - loss: 1.3048 - acc: 0.4864\n",
      "Epoch 12/100\n",
      "6012/6012 [==============================] - 2s 314us/step - loss: 1.3063 - acc: 0.4904\n",
      "Epoch 13/100\n",
      "6012/6012 [==============================] - 2s 313us/step - loss: 1.2948 - acc: 0.4882\n",
      "Epoch 14/100\n",
      "6012/6012 [==============================] - 2s 314us/step - loss: 1.3060 - acc: 0.4938\n",
      "Epoch 15/100\n",
      "6012/6012 [==============================] - 2s 318us/step - loss: 1.2916 - acc: 0.4980\n",
      "Epoch 16/100\n",
      "6012/6012 [==============================] - 2s 315us/step - loss: 1.2778 - acc: 0.4953\n",
      "Epoch 17/100\n",
      "6012/6012 [==============================] - 2s 315us/step - loss: 1.2874 - acc: 0.4914\n",
      "Epoch 18/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.2995 - acc: 0.4925\n",
      "Epoch 19/100\n",
      "6012/6012 [==============================] - 2s 316us/step - loss: 1.2909 - acc: 0.4975\n",
      "Epoch 20/100\n",
      "6012/6012 [==============================] - 2s 315us/step - loss: 1.3008 - acc: 0.4910\n",
      "Epoch 21/100\n",
      "6012/6012 [==============================] - 2s 324us/step - loss: 1.3071 - acc: 0.4922\n",
      "Epoch 22/100\n",
      "6012/6012 [==============================] - 2s 312us/step - loss: 1.2640 - acc: 0.4957\n",
      "Epoch 23/100\n",
      "6012/6012 [==============================] - 2s 316us/step - loss: 1.3023 - acc: 0.4940\n",
      "Epoch 24/100\n",
      "6012/6012 [==============================] - 2s 315us/step - loss: 1.3049 - acc: 0.5003\n",
      "Epoch 25/100\n",
      "6012/6012 [==============================] - 2s 317us/step - loss: 1.2899 - acc: 0.4975\n",
      "Epoch 26/100\n",
      "6012/6012 [==============================] - 2s 316us/step - loss: 1.3173 - acc: 0.4912\n",
      "Epoch 27/100\n",
      "6012/6012 [==============================] - 2s 314us/step - loss: 1.3021 - acc: 0.4940\n",
      "Epoch 28/100\n",
      "6012/6012 [==============================] - 2s 313us/step - loss: 1.2974 - acc: 0.4945\n",
      "Epoch 29/100\n",
      "6012/6012 [==============================] - 2s 340us/step - loss: 1.2958 - acc: 0.4985\n",
      "Epoch 30/100\n",
      "6012/6012 [==============================] - 2s 325us/step - loss: 1.2907 - acc: 0.5027\n",
      "Epoch 31/100\n",
      "6012/6012 [==============================] - 2s 322us/step - loss: 1.2844 - acc: 0.4933\n",
      "Epoch 32/100\n",
      "6012/6012 [==============================] - 2s 315us/step - loss: 1.2846 - acc: 0.4988\n",
      "Epoch 33/100\n",
      "6012/6012 [==============================] - 2s 323us/step - loss: 1.2897 - acc: 0.4955\n",
      "Epoch 34/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.2846 - acc: 0.5073\n",
      "Epoch 35/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.2877 - acc: 0.4938\n",
      "Epoch 36/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.3040 - acc: 0.4963\n",
      "Epoch 37/100\n",
      "6012/6012 [==============================] - 2s 330us/step - loss: 1.2985 - acc: 0.4968\n",
      "Epoch 38/100\n",
      "6012/6012 [==============================] - 2s 323us/step - loss: 1.2932 - acc: 0.4940\n",
      "Epoch 39/100\n",
      "6012/6012 [==============================] - 2s 316us/step - loss: 1.3079 - acc: 0.4963\n",
      "Epoch 40/100\n",
      "6012/6012 [==============================] - 2s 322us/step - loss: 1.2881 - acc: 0.4935\n",
      "Epoch 41/100\n",
      "6012/6012 [==============================] - 2s 324us/step - loss: 1.2933 - acc: 0.4967\n",
      "Epoch 42/100\n",
      "6012/6012 [==============================] - 2s 318us/step - loss: 1.3127 - acc: 0.4937\n",
      "Epoch 43/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3333 - acc: 0.4993\n",
      "Epoch 44/100\n",
      "6012/6012 [==============================] - 2s 332us/step - loss: 1.3317 - acc: 0.4933\n",
      "Epoch 45/100\n",
      "6012/6012 [==============================] - 2s 317us/step - loss: 1.3057 - acc: 0.4915\n",
      "Epoch 46/100\n",
      "6012/6012 [==============================] - 2s 321us/step - loss: 1.3157 - acc: 0.5000\n",
      "Epoch 47/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.2996 - acc: 0.5005\n",
      "Epoch 48/100\n",
      "6012/6012 [==============================] - 2s 327us/step - loss: 1.3226 - acc: 0.4972\n",
      "Epoch 49/100\n",
      "6012/6012 [==============================] - 2s 324us/step - loss: 1.3314 - acc: 0.4882\n",
      "Epoch 50/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.3306 - acc: 0.4885\n",
      "Epoch 51/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.3298 - acc: 0.4884\n",
      "Epoch 52/100\n",
      "6012/6012 [==============================] - 2s 323us/step - loss: 1.3401 - acc: 0.4918\n",
      "Epoch 53/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.3213 - acc: 0.4900\n",
      "Epoch 54/100\n",
      "6012/6012 [==============================] - 2s 325us/step - loss: 1.3125 - acc: 0.4972\n",
      "Epoch 55/100\n",
      "6012/6012 [==============================] - 2s 318us/step - loss: 1.3220 - acc: 0.4997\n",
      "Epoch 56/100\n",
      "6012/6012 [==============================] - 2s 318us/step - loss: 1.3248 - acc: 0.4923\n",
      "Epoch 57/100\n",
      "6012/6012 [==============================] - 2s 319us/step - loss: 1.3278 - acc: 0.4938\n",
      "Epoch 58/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3034 - acc: 0.4935\n",
      "Epoch 59/100\n",
      "6012/6012 [==============================] - 2s 323us/step - loss: 1.3310 - acc: 0.4923\n",
      "Epoch 60/100\n",
      "6012/6012 [==============================] - 2s 318us/step - loss: 1.3264 - acc: 0.4938\n",
      "Epoch 61/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3158 - acc: 0.4925\n",
      "Epoch 62/100\n",
      "6012/6012 [==============================] - 2s 326us/step - loss: 1.3221 - acc: 0.4860\n",
      "Epoch 63/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3191 - acc: 0.4942\n",
      "Epoch 64/100\n",
      "6012/6012 [==============================] - 2s 321us/step - loss: 1.3075 - acc: 0.4899\n",
      "Epoch 65/100\n",
      "6012/6012 [==============================] - 2s 318us/step - loss: 1.3319 - acc: 0.4935\n",
      "Epoch 66/100\n",
      "6012/6012 [==============================] - 2s 326us/step - loss: 1.3443 - acc: 0.4914\n",
      "Epoch 67/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3457 - acc: 0.4910\n",
      "Epoch 68/100\n",
      "6012/6012 [==============================] - 2s 321us/step - loss: 1.3432 - acc: 0.4947\n",
      "Epoch 69/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3355 - acc: 0.4935\n",
      "Epoch 70/100\n",
      "6012/6012 [==============================] - 2s 323us/step - loss: 1.3528 - acc: 0.4845\n",
      "Epoch 71/100\n",
      "6012/6012 [==============================] - 2s 327us/step - loss: 1.3576 - acc: 0.4784\n",
      "Epoch 72/100\n",
      "6012/6012 [==============================] - 2s 327us/step - loss: 1.3662 - acc: 0.4716\n",
      "Epoch 73/100\n",
      "6012/6012 [==============================] - 2s 331us/step - loss: 1.3706 - acc: 0.4832\n",
      "Epoch 74/100\n",
      "6012/6012 [==============================] - 2s 326us/step - loss: 1.3605 - acc: 0.4887\n",
      "Epoch 75/100\n",
      "6012/6012 [==============================] - 2s 324us/step - loss: 1.3562 - acc: 0.4879\n",
      "Epoch 76/100\n",
      "6012/6012 [==============================] - 2s 332us/step - loss: 1.3548 - acc: 0.4837\n",
      "Epoch 77/100\n",
      "6012/6012 [==============================] - 2s 328us/step - loss: 1.3742 - acc: 0.4912\n",
      "Epoch 78/100\n",
      "6012/6012 [==============================] - 2s 336us/step - loss: 1.3750 - acc: 0.4879\n",
      "Epoch 79/100\n",
      "6012/6012 [==============================] - 2s 320us/step - loss: 1.3637 - acc: 0.4855\n",
      "Epoch 80/100\n",
      "6012/6012 [==============================] - 2s 310us/step - loss: 1.3821 - acc: 0.4860\n",
      "Epoch 81/100\n",
      "6012/6012 [==============================] - 2s 323us/step - loss: 1.3869 - acc: 0.4785\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012/6012 [==============================] - 2s 308us/step - loss: 1.3779 - acc: 0.4779\n",
      "Epoch 83/100\n",
      "6012/6012 [==============================] - 2s 306us/step - loss: 1.3802 - acc: 0.4854\n",
      "Epoch 84/100\n",
      "6012/6012 [==============================] - 2s 303us/step - loss: 1.3873 - acc: 0.4800\n",
      "Epoch 85/100\n",
      "6012/6012 [==============================] - 2s 303us/step - loss: 1.3886 - acc: 0.4890\n",
      "Epoch 86/100\n",
      "6012/6012 [==============================] - 2s 305us/step - loss: 1.3783 - acc: 0.4837\n",
      "Epoch 87/100\n",
      "6012/6012 [==============================] - 2s 308us/step - loss: 1.3740 - acc: 0.4760\n",
      "Epoch 88/100\n",
      "6012/6012 [==============================] - 2s 303us/step - loss: 1.3420 - acc: 0.4877\n",
      "Epoch 89/100\n",
      "6012/6012 [==============================] - 2s 305us/step - loss: 1.3706 - acc: 0.4772\n",
      "Epoch 90/100\n",
      "6012/6012 [==============================] - 2s 300us/step - loss: 1.3674 - acc: 0.4847\n",
      "Epoch 91/100\n",
      "6012/6012 [==============================] - 2s 306us/step - loss: 1.3624 - acc: 0.4844\n",
      "Epoch 92/100\n",
      "6012/6012 [==============================] - 2s 305us/step - loss: 1.3680 - acc: 0.4852\n",
      "Epoch 93/100\n",
      "6012/6012 [==============================] - 2s 304us/step - loss: 1.3577 - acc: 0.4764\n",
      "Epoch 94/100\n",
      "6012/6012 [==============================] - 2s 308us/step - loss: 1.3692 - acc: 0.4754\n",
      "Epoch 95/100\n",
      "6012/6012 [==============================] - 2s 310us/step - loss: 1.3940 - acc: 0.4696\n",
      "Epoch 96/100\n",
      "6012/6012 [==============================] - 2s 321us/step - loss: 1.4211 - acc: 0.4676\n",
      "Epoch 97/100\n",
      "6012/6012 [==============================] - 2s 382us/step - loss: 1.3954 - acc: 0.4742\n",
      "Epoch 98/100\n",
      "6012/6012 [==============================] - 2s 357us/step - loss: 1.3939 - acc: 0.4664\n",
      "Epoch 99/100\n",
      "6012/6012 [==============================] - 2s 338us/step - loss: 1.3883 - acc: 0.4780\n",
      "Epoch 100/100\n",
      "6012/6012 [==============================] - 2s 312us/step - loss: 1.3655 - acc: 0.4855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ec7ea20>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(xtrain_count, train_y, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results of ANN</h3>\n",
    "~.49 accuracy\n",
    "Now try LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
