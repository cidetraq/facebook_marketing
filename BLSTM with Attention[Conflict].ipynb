{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach to emulate:\n",
    "\n",
    "1.Input Layer\n",
    "\n",
    "2.Embedding layer\n",
    "\n",
    "3.BLSTM layer, with element-wise sum of forward/backward pass outputs\\\n",
    "\n",
    "Classification should have dropout  applied on the embedding layer, LSTM layer and penultimate layer. as Well as L2 regularization\n",
    "\n",
    "4.Attention Layer\n",
    "\n",
    "5.Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AttLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0fbe188dc6c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/emotions_blstm_att_tf.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'AttLayer'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAttLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'AttLayer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"]='theano'\n",
    "os.environ[\"KERAS_BACKEND\"]='tensorflow'\n",
    "import keras\n",
    "keras.backend.backend()\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU, Input, TimeDistributed\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "import lmdb\n",
    "from lmdb_embeddings.reader import LmdbEmbeddingsReader\n",
    "import lmdb_embeddings.exceptions as exceptions\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "classifier=load_model('models/emotions_blstm_att_tf.h5', custom_objects={'AttLayer': AttLayer()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theano\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "        shape=(input_shape[-1],),\n",
    "        initializer='normal',\n",
    "        trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  \n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "        shape=(input_shape[-1],),\n",
    "        initializer='normal',\n",
    "        trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  \n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(np.dot(x, self.W))\n",
    "        ai = K.exp(eij)\n",
    "        product=tf.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        weights = ai/product \n",
    "        weighted_input = x*weights\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()\n",
    "classifier.add(Bidirectional(GRU(units=25, return_sequences=True), input_shape=(6,300)))\n",
    "classifier.add(AttLayer())\n",
    "classifier.add(Dropout(0.3))\n",
    "classifier.add(Dense(units=4, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=LmdbEmbeddingsReader('data/lmdb_databases')\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/isear_plus_semeval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['not', 'no', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                         'even though', 'yet']\n",
    "stop = list(set(stopwords.words('english')))\n",
    "for neg in negative:\n",
    "    for stopword in stop:\n",
    "        if stopword==neg:\n",
    "            stop.remove(stopword)\n",
    "rm=['don\\'t', 'shouldn\\'t', 'doesn\\'t', 'didn\\'t']\n",
    "for r in rm:\n",
    "    stop.remove(r)\n",
    "exclude = set(string.punctuation)\n",
    "exclude.add('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join([ch for ch in stop_free if ch not in exclude])\n",
    "    re.sub(r'\\n', '', punc_free)\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_words(li):\n",
    "    total_vecs=[]\n",
    "    for word in li:\n",
    "        try:\n",
    "            vector = embeddings.get_word_vector(word)\n",
    "        except exceptions.MissingWordError:\n",
    "            # 'google' is not in the database.\n",
    "            vector= np.zeros(300, dtype='float32')\n",
    "        total_vecs.append(vector)\n",
    "    return np.array(total_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    y_1=np_utils.to_categorical(y)\n",
    "    #y_1=np.reshape(y_1, (-1, 4, 1))\n",
    "    return y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_splits(series):\n",
    "    word_splits=series.str.split(' ')\n",
    "    return word_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_6(X,y=None):\n",
    "    X=pd.Series(X).apply(clean).apply(input_duplicator_train)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_words)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1=[]\n",
    "    y_1=[]\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        print(len(doc))\n",
    "        for i in range(6, len(doc)):\n",
    "            X_1.append(doc[i-6:i])\n",
    "            if y is not None:\n",
    "                y_1.append(y.iloc[index])\n",
    "                y_1=transform_y(y_1)\n",
    "    if y is not None:\n",
    "        return np.array(X_1), np.array(y_1)\n",
    "    else:\n",
    "        return np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_duplicator_train(text):\n",
    "    splits=text.split(' ')\n",
    "    while len(splits)<7:\n",
    "        orig_doc=splits.copy()\n",
    "        for word in orig_doc:\n",
    "            splits.append(word)\n",
    "    return ' '.join(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_duplicator(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits\n",
    "    num_docs=len(numbers_series)\n",
    "    for index, doc in enumerate(numbers_series):\n",
    "        while len(doc)<7:\n",
    "            orig_doc=doc.copy()\n",
    "            orig_doc=list(orig_doc)\n",
    "            doc=list(doc)\n",
    "            for word in orig_doc:\n",
    "                doc.append(word)\n",
    "                #doc=np.insert(doc,(len(doc)),word, axis=0)\n",
    "                #doc=np.append(doc, word, axis=1)\n",
    "            modified=True\n",
    "        numbers_series.iloc[index]=np.array(doc)\n",
    "    X_1 = []\n",
    "    if num_docs>1:\n",
    "        for index in range(0, num_docs):\n",
    "            doc=numbers_series.iloc[index]\n",
    "            for i in range(6, len(doc)):\n",
    "                X_1.append(doc[i-6:i])\n",
    "    else:\n",
    "        doc=numbers_series.iloc[0]\n",
    "        print(doc.shape)\n",
    "        for i in range(6, len(doc)):\n",
    "                X_1.append(doc[i-6:i])\n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    X=transform(text)\n",
    "    prediction=classifier.predict(X)\n",
    "    prediction=np.mean(prediction, axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, y_1=transform(data['1'], data['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous best with theano\n",
    "classifier.evaluate(X_test, y_test)\n",
    "#Theano training is many, many times slower than tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Tensorflow\n",
    "classifier.evaluate(X_test, y_test)\n",
    "#Comparable results to without attention. This needs better implementation, closer to the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('''I do not happy''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('models/emotions_blstm_att_tf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hierachical Attention Network With Buckets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Segmentation\n",
    "text='Sentence #one... I hope it picks this up. Sentence LMFAO two! Sentence three?'\n",
    "tokens=nlp(text)\n",
    "for s in tokens.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bucketed_sequence import BucketedSequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from absl import app\n",
    "\n",
    "UNK = -1.0\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "'''flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
    "flags.DEFINE_integer('epochs', 20, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('lstm_units', 50, 'Number of LSTM units in RNN')\n",
    "flags.DEFINE_integer('dense_breadth', 64, 'Number of neurons in the dense ' +\n",
    "                     'layer')\n",
    "\n",
    "flags.DEFINE_integer('dataset_size', 4726, 'Size of training dataset')\n",
    "flags.DEFINE_integer('val_size', 1182, 'Size of validation set')\n",
    "flags.DEFINE_integer('buckets', 4, 'Number of buckets to use (run with ' +\n",
    "                     '0 to disable)')'''\n",
    "\n",
    "'''flags.DEFINE_integer('seqlen_mean', 50, 'Sequence length mean (drawn ' +\n",
    "                     'from normal distribution)')\n",
    "flags.DEFINE_integer('seqlen_stddev', 200, 'Sequence length standard ' +\n",
    "                     'deviation (drawn from normal distribution)')'''\n",
    "\n",
    "batch_size=64\n",
    "epochs=20\n",
    "lstm_units=25\n",
    "dense_breadth=64\n",
    "buckets=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(seqs, maxlen):\n",
    "    # NOTE: prepends data\n",
    "    padded = np.array(pad_sequences(seqs, maxlen=maxlen, value=UNK, \n",
    "                                    dtype=seqs[0].dtype))\n",
    "    return np.vstack([np.expand_dims(x, axis=0) for x in padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    del argv # Ignore other arguments\n",
    "\n",
    "    # Set up a simple network (LSTM + Dense)\n",
    "    inp = Input(shape=(None, 300), dtype=\"float32\", name=\"in\")\n",
    "    lstm = Bidirectional(GRU(lstm_units, return_sequences=False,\n",
    "                name=\"lstm\"))(inp)\n",
    "    dense = Dense(dense_breadth, kernel_initializer='normal',\n",
    "                  activation='relu')(lstm)\n",
    "    outputs = Dense(4, kernel_initializer='normal')(dense)\n",
    "    model = Model(inputs=inp, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", \n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    # Generate dataset\n",
    "    X,y=transform(data['1'], data['0'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "    len_train=len(X_train)+len(y_train)\n",
    "    len_val=len(X_test)+len(y_test)\n",
    "    \n",
    "    if FLAGS.buckets > 0:\n",
    "        # Create Sequence objects\n",
    "        train_generator = BucketedSequence(FLAGS.buckets, FLAGS.batch_size,\n",
    "                                           len_train, x_train, y_train)\n",
    "        val_generator = BucketedSequence(FLAGS.buckets, FLAGS.batch_size,\n",
    "                                         len_val, x_val, y_val)\n",
    "\n",
    "        model.fit_generator(train_generator, epochs=FLAGS.epochs,\n",
    "                            validation_data=val_generator,\n",
    "                            shuffle=True, verbose=True)\n",
    "    else:\n",
    "        # No bucketing\n",
    "        model.fit(x=x_train, y=y_train, epochs=FLAGS.epochs,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  batch_size=FLAGS.batch_size, verbose=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X,y=None):\n",
    "    X=pd.Series(X).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_words)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1=[]\n",
    "    y_1=[]\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for word in doc:\n",
    "            X_1.append(word)\n",
    "        if y is not None:\n",
    "            y_1.append(y.iloc[index])\n",
    "        \n",
    "    if y is not None:\n",
    "        y_1=transform_y(y_1)\n",
    "        return np.array(X_1), np.array(y_1)\n",
    "    else:\n",
    "        return np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "#sentence_input = Input(shape=(None,), dtype='int32')\n",
    "#embedded_sequences = embedding_layer(sentence_input)\n",
    "#l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "sentence_input= Input(shape=(None, 300))\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(sentence_input)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    " \n",
    "#review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_input = Input(shape=(7,None), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "han=Sequential()\n",
    "han.add(Bidirectional(GRU(units=100, return_sequences=True), input_shape=(6,300)))\n",
    "han.add(TimeDistributed(Dense(200)))\n",
    "han.add(AttLayer())\n",
    "classifier.add(Dropout(0.3))\n",
    "classifier.add(Dense(units=4, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>anger, fear, joy, sadness</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
