{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "from flask import jsonify\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import json, codecs\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout, SpatialDropout1D, TimeDistributed, Reshape, Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json, Model\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pdb\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = KeyedVectors.load_word2vec_format('data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_emotions=load_model('models/four_emotions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['not', 'no', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                         'even though', 'yet']\n",
    "stop = list(set(stopwords.words('english')))\n",
    "for neg in negative:\n",
    "    for stopword in stop:\n",
    "        if stopword==neg:\n",
    "            stop.remove(stopword)\n",
    "exclude = set(string.punctuation)\n",
    "exclude.add('\\n')\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join([ch for ch in stop_free if ch not in exclude])\n",
    "    re.sub(r'\\n', '', punc_free)\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_word(li):\n",
    "    total_vecs=[]\n",
    "    for word in li:\n",
    "        if word in en_model.vocab:\n",
    "            vector = en_model[word]\n",
    "            total_vecs.append(vector)\n",
    "    return np.array(total_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_splits(series):\n",
    "    word_splits=series.str.split(' ')\n",
    "    return word_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ISEAR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear=pd.read_csv('isear_1.csv', header=-1)\n",
    "isear=isear.drop(2 , axis=1)\n",
    "isear[0][isear[0]=='guit']='guilt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=isear[1]\n",
    "X=X.apply(clean)\n",
    "y=isear[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=word_splits(X)\n",
    "numbers_series=splits.apply(vec_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    num_words=len(numbers_series)\n",
    "    X_1 = []\n",
    "    for index in range(0, num_words):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(3, len(doc)):\n",
    "            X_1.append(doc[i-3:i])\n",
    "    return np.array(X_1), len(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_textform(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits\n",
    "    X_1 = []\n",
    "    for index in range(0, len(numbers_series)):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(3, len(doc)):\n",
    "            X_1.append(doc[i-3:i])\n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    y_1=np_utils.to_categorical(y)\n",
    "    return y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = []\n",
    "y_1 = []\n",
    "for index in range(0, len(numbers_series)):\n",
    "    doc=numbers_series.iloc[index]\n",
    "    for i in range(3, len(doc)):\n",
    "        X_1.append(doc[i-3:i])\n",
    "        y_1.append(y.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2=np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53460, 3, 300)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(y_1)\n",
    "y_1=encoder.transform(y_1)\n",
    "y=np_utils.to_categorical(y_1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size = 0.2, random_state = 0)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10692, 7)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42768, 7)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42768"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53460, 3, 300)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53460,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=50, return_sequences=True, input_shape=(None, 300...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_dim=300))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "42768/42768 [==============================] - 18s 419us/step - loss: 0.0495 - acc: 0.7586\n",
      "Epoch 2/25\n",
      "42768/42768 [==============================] - 15s 358us/step - loss: 0.0490 - acc: 0.7635\n",
      "Epoch 3/25\n",
      "42768/42768 [==============================] - 15s 358us/step - loss: 0.0488 - acc: 0.7621\n",
      "Epoch 4/25\n",
      "42768/42768 [==============================] - 15s 359us/step - loss: 0.0488 - acc: 0.7621\n",
      "Epoch 5/25\n",
      "42768/42768 [==============================] - 15s 359us/step - loss: 0.0486 - acc: 0.7650\n",
      "Epoch 6/25\n",
      "42768/42768 [==============================] - 15s 360us/step - loss: 0.0487 - acc: 0.7630\n",
      "Epoch 7/25\n",
      "42768/42768 [==============================] - 15s 362us/step - loss: 0.0485 - acc: 0.7640\n",
      "Epoch 8/25\n",
      "42768/42768 [==============================] - 16s 370us/step - loss: 0.0481 - acc: 0.7680\n",
      "Epoch 9/25\n",
      "42768/42768 [==============================] - 16s 367us/step - loss: 0.0481 - acc: 0.7662\n",
      "Epoch 10/25\n",
      "42768/42768 [==============================] - 16s 384us/step - loss: 0.0480 - acc: 0.7675\n",
      "Epoch 11/25\n",
      "42768/42768 [==============================] - 16s 369us/step - loss: 0.0479 - acc: 0.7684\n",
      "Epoch 12/25\n",
      "42768/42768 [==============================] - 16s 364us/step - loss: 0.0476 - acc: 0.7698\n",
      "Epoch 13/25\n",
      "42768/42768 [==============================] - 16s 369us/step - loss: 0.0473 - acc: 0.7725\n",
      "Epoch 14/25\n",
      "42768/42768 [==============================] - 17s 386us/step - loss: 0.0474 - acc: 0.7710\n",
      "Epoch 15/25\n",
      "42768/42768 [==============================] - 16s 371us/step - loss: 0.0474 - acc: 0.7715\n",
      "Epoch 16/25\n",
      "42768/42768 [==============================] - 17s 408us/step - loss: 0.0468 - acc: 0.7753\n",
      "Epoch 17/25\n",
      "42768/42768 [==============================] - 16s 376us/step - loss: 0.0470 - acc: 0.7736\n",
      "Epoch 18/25\n",
      "42768/42768 [==============================] - 16s 378us/step - loss: 0.0466 - acc: 0.7755\n",
      "Epoch 19/25\n",
      "42768/42768 [==============================] - 16s 384us/step - loss: 0.0466 - acc: 0.7751\n",
      "Epoch 20/25\n",
      "42768/42768 [==============================] - 16s 367us/step - loss: 0.0464 - acc: 0.7778\n",
      "Epoch 21/25\n",
      "42768/42768 [==============================] - 16s 367us/step - loss: 0.0462 - acc: 0.7777\n",
      "Epoch 22/25\n",
      "42768/42768 [==============================] - 16s 369us/step - loss: 0.0460 - acc: 0.7771\n",
      "Epoch 23/25\n",
      "42768/42768 [==============================] - 17s 392us/step - loss: 0.0461 - acc: 0.7789\n",
      "Epoch 24/25\n",
      "42768/42768 [==============================] - 16s 365us/step - loss: 0.0462 - acc: 0.7768\n",
      "Epoch 25/25\n",
      "42768/42768 [==============================] - 17s 398us/step - loss: 0.0456 - acc: 0.7794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x53bbc5c0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'mean_squared_error')\n",
    "regressor.fit(X_train, y_train, epochs = 25, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10692/10692 [==============================] - 1s 117us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10446360716760003, 0.48110736999625886]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initial model with .48 test accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving and loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = regressor.to_json()\n",
    "with open(\"emotions_model_6.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "regressor.save_weights(\"emotions_model_6.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('emotions_regressor.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"emotions_regressor.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09755597 0.04315098 0.17637494 0.07367538 0.43375415 0.07726857\n",
      "  0.09988091]\n",
      " [0.56483126 0.07285608 0.13253172 0.04942662 0.09893434 0.02065588\n",
      "  0.0610196 ]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(loaded_model.predict(transform_input('I like pie a whole bunch a lot')))\n",
    "except ValueError:\n",
    "    print('Input size too small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_avg(text):\n",
    "    X, X_len=transform_input(text)\n",
    "    prediction=loaded_model.predict(transform_input(text)[0])\n",
    "    prediction=np.mean(prediction, axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02197286, 0.90678847, 0.01035543, 0.01425846, 0.00490067,\n",
       "       0.01707822, 0.01889859], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pie_mat=predict_avg('I hate all aliens they are evil bastards')\n",
    "pie_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:anger\n",
    "\n",
    "1:disgust\n",
    "\n",
    "2:fear\n",
    "\n",
    "3:guilt\n",
    "\n",
    "4: joy\n",
    "\n",
    "5:sadness\n",
    "\n",
    "6:shame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded_model (model 1) doesn't do very well. Let's try to make another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model with unknown timespan</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=isear[1]\n",
    "X=X.apply(clean)\n",
    "y=isear[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=word_splits(X)\n",
    "numbers_series=splits.apply(vec_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_no_window(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    return numbers_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.apply(transform_no_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.032, 0.0381, -0.0299, -0.0745, -0.0624, -0...\n",
       "1       [[-0.0036, -0.1675, 0.0635, -0.0249, 0.0098, -...\n",
       "2       [[0.079, -0.0397, 0.005, 0.0136, -0.0226, -0.0...\n",
       "3       [[-0.0432, 0.037, -0.0039, 0.0524, -0.1216, -0...\n",
       "4       [[0.0586, -0.037, -0.081, -0.1985, -0.1255, -0...\n",
       "5       [[0.1129, -0.0975, 0.0769, -0.071, 0.0645, -0....\n",
       "6       [[-0.0372, 0.0615, 0.0167, -0.0382, 0.0207, -0...\n",
       "7       [[0.1912, -0.0745, 0.0554, -0.0277, -0.0276, 0...\n",
       "8       [[0.0092, -0.0123, -0.0336, 0.0269, 0.031, 0.0...\n",
       "9       [[-0.016, -0.0003, -0.1684, 0.0899, -0.02, -0....\n",
       "10      [[0.0103, 0.0984, 0.1326, 0.0845, 0.0898, -0.1...\n",
       "11      [[-0.0368, -0.0336, 0.0182, 0.0258, -0.0327, -...\n",
       "12      [[-0.1362, 0.1148, -0.0027, -0.0033, 0.1185, -...\n",
       "13      [[-0.0432, 0.037, -0.0039, 0.0524, -0.1216, -0...\n",
       "14      [[-0.0079, -0.0009, -0.0813, -0.2044, 0.1367, ...\n",
       "15      [[0.0458, -0.0362, 0.0156, 0.0027, -0.053, 0.0...\n",
       "16      [[0.0458, -0.0362, 0.0156, 0.0027, -0.053, 0.0...\n",
       "17      [[0.0458, -0.0362, 0.0156, 0.0027, -0.053, 0.0...\n",
       "18      [[0.0458, -0.0362, 0.0156, 0.0027, -0.053, 0.0...\n",
       "19      [[0.0458, -0.0362, 0.0156, 0.0027, -0.053, 0.0...\n",
       "20      [[0.0458, -0.0362, 0.0156, 0.0027, -0.053, 0.0...\n",
       "21      [[-0.1781, 0.0768, 0.1997, -0.1367, -0.0278, -...\n",
       "22      [[-0.1634, 0.0061, -0.0523, -0.0952, -0.2584, ...\n",
       "23      [[-0.0329, 0.0098, -0.0657, 0.0019, 0.066, -0....\n",
       "24      [[0.0534, 0.023, 0.0731, -0.1413, -0.0934, 0.0...\n",
       "25      [[-0.0072, 0.0567, -0.0194, 0.0643, 0.1007, 0....\n",
       "26      [[0.1449, -0.0717, 0.0755, -0.0863, -0.0139, 0...\n",
       "27      [[-0.1182, 0.0039, 0.074, -0.0782, -0.143, -0....\n",
       "28      [[-0.0217, -0.1088, 0.1682, 0.0173, 0.1315, -0...\n",
       "29      [[-0.0781, -0.0677, 0.1376, -0.0176, 0.0218, 0...\n",
       "                              ...                        \n",
       "7486    [[0.0351, 0.0176, 0.0075, -0.0584, 0.1963, -0....\n",
       "7487    [[-0.224, -0.1629, -0.0855, -0.1952, 0.0535, 0...\n",
       "7488    [[0.0979, -0.1433, 0.0157, 0.0823, 0.0436, 0.0...\n",
       "7489    [[0.1365, -0.03, -0.1251, -0.003, -0.0226, -0....\n",
       "7490    [[-0.0781, -0.0677, 0.1376, -0.0176, 0.0218, 0...\n",
       "7491    [[-0.2699, -0.1004, -0.1057, 0.0608, 0.1755, 0...\n",
       "7492    [[-0.0183, -0.082, -0.1086, -0.0261, -0.0004, ...\n",
       "7493    [[-0.0109, -0.0306, -0.118, -0.0267, 0.2778, 0...\n",
       "7494    [[0.0445, -0.0083, 0.005, -0.1467, -0.0026, -0...\n",
       "7495    [[0.0092, -0.0123, -0.0336, 0.0269, 0.031, 0.0...\n",
       "7496    [[-0.0798, 0.0482, -0.0422, 0.0111, 0.0124, -0...\n",
       "7497    [[-0.0558, -0.0681, -0.0805, 0.0409, -0.2693, ...\n",
       "7498    [[0.0315, -0.0698, -0.0005, 0.0365, 0.0015, -0...\n",
       "7499    [[0.0949, -0.0875, 0.0512, -0.1082, 0.0241, 0....\n",
       "7500    [[0.0789, -0.0313, 0.1218, -0.0199, -0.0013, 0...\n",
       "7501    [[0.0382, 0.083, -0.0656, -0.1296, 0.0654, 0.0...\n",
       "7502    [[-0.0752, -0.0111, -0.136, -0.0281, 0.0766, 0...\n",
       "7503    [[0.0108, -0.0204, -0.0371, 0.0167, -0.0105, 0...\n",
       "7504    [[0.0897, -0.089, -0.1608, -0.0915, 0.1564, 0....\n",
       "7505    [[0.0322, -0.0539, 0.0034, -0.03, -0.0082, 0.1...\n",
       "7506    [[0.0938, 0.014, 0.1099, 0.0604, -0.0615, 0.11...\n",
       "7507    [[0.111, -0.0062, -0.0553, -0.0384, 0.0806, 0....\n",
       "7508    [[0.3411, -0.0711, 0.0519, -0.0978, 0.0126, -0...\n",
       "7509    [[-0.0382, -0.0043, 0.2217, -0.0783, -0.0764, ...\n",
       "7510    [[0.0256, 0.1352, 0.0608, -0.0799, -0.0198, -0...\n",
       "7511    [[0.0134, 0.0171, -0.0344, 0.0912, -0.014, 0.0...\n",
       "7512    [[0.1171, -0.0058, 0.0053, -0.0278, 0.0515, -0...\n",
       "7513    [[0.1576, 0.0211, 0.0195, 0.0959, 0.0006, 0.08...\n",
       "7514    [[-0.1756, 0.0695, 0.0847, 0.0988, -0.1196, -0...\n",
       "7515    [[-0.0817, -0.0154, 0.042, 0.0736, 0.0636, -0....\n",
       "Name: 0, Length: 7516, dtype: object"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1=X_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7516,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2=np.array(X_1)\n",
    "X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[ 0.032 ,  0.0381, -0.0299, ...,  0.1316,  0.1363,  0.044 ],\n",
       "       [-0.0372,  0.0615,  0.0167, ...,  0.1405, -0.0883, -0.1164],\n",
       "       [ 0.0622,  0.0628,  0.2393, ...,  0.1802, -0.0394, -0.0243],\n",
       "       ...,\n",
       "       [ 0.0104,  0.1101, -0.0982, ...,  0.1598, -0.0329,  0.0798],\n",
       "       [ 0.0055, -0.1195,  0.0729, ...,  0.2087, -0.0779, -0.0624],\n",
       "       [-0.1902, -0.1442, -0.1249, ...,  0.2267, -0.0037,  0.0915]],\n",
       "      dtype=float32),\n",
       "       array([[-0.0036, -0.1675,  0.0635, ...,  0.1275,  0.1066, -0.0399],\n",
       "       [ 0.0863, -0.0405,  0.0437, ...,  0.1703, -0.3191, -0.0287],\n",
       "       [-0.1006, -0.0988, -0.1582, ...,  0.2289,  0.1009, -0.018 ],\n",
       "       ...,\n",
       "       [ 0.1084,  0.1093, -0.1632, ..., -0.0037, -0.2236, -0.2337],\n",
       "       [-0.0112, -0.1144, -0.0171, ...,  0.1761,  0.0193, -0.0004],\n",
       "       [ 0.1013,  0.0365,  0.0062, ...,  0.0657,  0.0512, -0.0055]],\n",
       "      dtype=float32),\n",
       "       array([[ 0.079 , -0.0397,  0.005 , ...,  0.1426, -0.1237, -0.0863],\n",
       "       [ 0.1821, -0.0348, -0.0131, ...,  0.1844, -0.0406,  0.1185],\n",
       "       [ 0.1273, -0.1795,  0.2076, ...,  0.1616, -0.0369,  0.0505],\n",
       "       [ 0.0515, -0.0238, -0.0343, ...,  0.153 ,  0.137 , -0.0806],\n",
       "       [-0.1741, -0.1503, -0.0835, ...,  0.3085, -0.1184,  0.0525]],\n",
       "      dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y=encoder.transform(y)\n",
    "y=np_utils.to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input shape\n",
    "\n",
    "3D tensor with shape (batch_size, timesteps, input_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=50, return_sequences=True, input_shape=(None, 300...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_dim=300))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(new_input, y_train, epochs = 25, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = Input(shape=(None, 300)) # unknown timespan, fixed feature size\n",
    "lstm = LSTM(20)\n",
    "dense=Dense(units = 7)\n",
    "dense_out=dense(I)\n",
    "f = K.function(inputs=[I], outputs=[lstm(I)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input=list(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(input=[I], output=[dense_out])\n",
    "model.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'mean_squared_error')\n",
    "model.fit(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 300)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input=list(X_train)\n",
    "new_input=[np.reshape(arr, (1, arr.shape[0], 300)) for arr in new_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00189861, -0.02905425,  0.01028993,  0.05693588,  0.12599891,\n",
       "         -0.07233123, -0.15039034, -0.03753458,  0.0746619 ,  0.03368369,\n",
       "         -0.01838236, -0.05823456, -0.0047072 , -0.00749667, -0.03643454,\n",
       "          0.03162642,  0.02647312,  0.02489861,  0.06106438,  0.00751286]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window\n",
    "X_1 = []\n",
    "    for index in range(0, len(numbers_series)):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(3, len(doc)):\n",
    "            X_1.append(doc[i-3:i])\n",
    "    return np.array(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Second model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=100, return_sequences=True, input_shape=(None, 300...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_dim=300))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = adam, metrics=['accuracy'], loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "42768/42768 [==============================] - 32s 748us/step - loss: 0.8835 - acc: 0.6842\n",
      "Epoch 2/25\n",
      "42768/42768 [==============================] - 32s 737us/step - loss: 0.8675 - acc: 0.6885\n",
      "Epoch 3/25\n",
      "42768/42768 [==============================] - 32s 743us/step - loss: 0.8545 - acc: 0.6947\n",
      "Epoch 4/25\n",
      "42768/42768 [==============================] - 31s 735us/step - loss: 0.8418 - acc: 0.7000\n",
      "Epoch 5/25\n",
      "42768/42768 [==============================] - 31s 732us/step - loss: 0.8274 - acc: 0.7037\n",
      "Epoch 6/25\n",
      "42768/42768 [==============================] - 31s 732us/step - loss: 0.8193 - acc: 0.7093\n",
      "Epoch 7/25\n",
      "42768/42768 [==============================] - 32s 750us/step - loss: 0.8059 - acc: 0.71211s - l\n",
      "Epoch 8/25\n",
      "42768/42768 [==============================] - 32s 747us/step - loss: 0.8002 - acc: 0.7122\n",
      "Epoch 9/25\n",
      "42768/42768 [==============================] - 32s 757us/step - loss: 0.7883 - acc: 0.7179\n",
      "Epoch 10/25\n",
      "42768/42768 [==============================] - 33s 765us/step - loss: 0.7722 - acc: 0.7258\n",
      "Epoch 11/25\n",
      "42768/42768 [==============================] - 32s 758us/step - loss: 0.7584 - acc: 0.7316\n",
      "Epoch 12/25\n",
      "42768/42768 [==============================] - 32s 751us/step - loss: 0.7558 - acc: 0.7318\n",
      "Epoch 13/25\n",
      "42768/42768 [==============================] - 32s 751us/step - loss: 0.7494 - acc: 0.7315\n",
      "Epoch 14/25\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7293 - acc: 0.7412\n",
      "Epoch 15/25\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.7254 - acc: 0.7419\n",
      "Epoch 16/25\n",
      "42768/42768 [==============================] - 32s 747us/step - loss: 0.7128 - acc: 0.7475\n",
      "Epoch 17/25\n",
      "42768/42768 [==============================] - 32s 745us/step - loss: 0.7022 - acc: 0.7511\n",
      "Epoch 18/25\n",
      "42768/42768 [==============================] - 32s 748us/step - loss: 0.6936 - acc: 0.7562\n",
      "Epoch 19/25\n",
      "42768/42768 [==============================] - 32s 748us/step - loss: 0.6867 - acc: 0.7577\n",
      "Epoch 20/25\n",
      "42768/42768 [==============================] - 32s 749us/step - loss: 0.6770 - acc: 0.7605\n",
      "Epoch 21/25\n",
      "42768/42768 [==============================] - 32s 756us/step - loss: 0.6689 - acc: 0.7668\n",
      "Epoch 22/25\n",
      "42768/42768 [==============================] - 34s 792us/step - loss: 0.6641 - acc: 0.7670\n",
      "Epoch 23/25\n",
      "42768/42768 [==============================] - 33s 771us/step - loss: 0.6543 - acc: 0.7704\n",
      "Epoch 24/25\n",
      "42768/42768 [==============================] - 32s 746us/step - loss: 0.6436 - acc: 0.7735\n",
      "Epoch 25/25\n",
      "42768/42768 [==============================] - 34s 804us/step - loss: 0.6421 - acc: 0.7752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe51b8940>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train, y_train, epochs = 25, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10692/10692 [==============================] - 2s 215us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.791913282456489, 0.48054620276842497]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = regressor.to_json()\n",
    "with open(\"emotions_regressor_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "regressor.save_weights(\"emotions_regressor_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Third model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=100, return_sequences=True, input_shape=(None, 300...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_dim=300))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=Adam(lr=0.0007, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = adam, metrics=['accuracy'], loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42768 samples, validate on 10692 samples\n",
      "Epoch 1/100\n",
      "42768/42768 [==============================] - 33s 783us/step - loss: 0.5579 - acc: 0.8134 - val_loss: 2.3434 - val_acc: 0.4560\n",
      "Epoch 2/100\n",
      " 3776/42768 [=>............................] - ETA: 28s - loss: 0.5140 - acc: 0.8284"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-2bf9d2c89e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist=regressor.fit(X_train, y_train, epochs = 100, batch_size = 32, validation_data=(X_test, y_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10692/10692 [==============================] - 2s 215us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.214059761745226, 0.4675458286569398]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(hist.history['loss'][500:])\n",
    "pyplot.plot(hist.history['val_loss'][500:])\n",
    "pyplot.title('model train vs validation loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fourth model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=100, return_sequences=True, input_shape=(None, 300...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_dim=300))\n",
    "regressor.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Adding a LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 100, return_sequences = True))\n",
    "regressor.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Adding a LSTM layer\n",
    "regressor.add(LSTM(units = 100))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42768 samples, validate on 10692 samples\n",
      "Epoch 1/100\n",
      "42768/42768 [==============================] - 43s 1000us/step - loss: 1.8514 - acc: 0.2293 - val_loss: 1.7700 - val_acc: 0.2788\n",
      "Epoch 2/100\n",
      "42768/42768 [==============================] - 34s 789us/step - loss: 1.7607 - acc: 0.2812 - val_loss: 1.7198 - val_acc: 0.3098\n",
      "Epoch 3/100\n",
      "42768/42768 [==============================] - 34s 789us/step - loss: 1.7203 - acc: 0.3059 - val_loss: 1.6828 - val_acc: 0.3389\n",
      "Epoch 4/100\n",
      "42768/42768 [==============================] - 34s 791us/step - loss: 1.6804 - acc: 0.3327 - val_loss: 1.6587 - val_acc: 0.3534\n",
      "Epoch 5/100\n",
      "42768/42768 [==============================] - 34s 795us/step - loss: 1.6542 - acc: 0.3474 - val_loss: 1.6329 - val_acc: 0.3690\n",
      "Epoch 6/100\n",
      "42768/42768 [==============================] - 34s 789us/step - loss: 1.6295 - acc: 0.3625 - val_loss: 1.6260 - val_acc: 0.3715\n",
      "Epoch 7/100\n",
      "42768/42768 [==============================] - 34s 792us/step - loss: 1.6074 - acc: 0.3700 - val_loss: 1.6168 - val_acc: 0.3734\n",
      "Epoch 8/100\n",
      "42768/42768 [==============================] - 35s 824us/step - loss: 1.5885 - acc: 0.3795 - val_loss: 1.6055 - val_acc: 0.3838\n",
      "Epoch 9/100\n",
      "42768/42768 [==============================] - 34s 791us/step - loss: 1.5681 - acc: 0.3909 - val_loss: 1.5949 - val_acc: 0.3889\n",
      "Epoch 10/100\n",
      "42768/42768 [==============================] - 34s 790us/step - loss: 1.5457 - acc: 0.4021 - val_loss: 1.5926 - val_acc: 0.3923\n",
      "Epoch 11/100\n",
      "42768/42768 [==============================] - 34s 795us/step - loss: 1.5269 - acc: 0.4078 - val_loss: 1.5851 - val_acc: 0.3937\n",
      "Epoch 12/100\n",
      "42768/42768 [==============================] - 34s 795us/step - loss: 1.5076 - acc: 0.4171 - val_loss: 1.5806 - val_acc: 0.4001\n",
      "Epoch 13/100\n",
      "42768/42768 [==============================] - 35s 810us/step - loss: 1.4895 - acc: 0.4273 - val_loss: 1.5758 - val_acc: 0.4022\n",
      "Epoch 14/100\n",
      "42768/42768 [==============================] - 35s 807us/step - loss: 1.4701 - acc: 0.4357 - val_loss: 1.5702 - val_acc: 0.4065\n",
      "Epoch 15/100\n",
      "42768/42768 [==============================] - 35s 809us/step - loss: 1.4512 - acc: 0.4448 - val_loss: 1.5654 - val_acc: 0.4126\n",
      "Epoch 16/100\n",
      "42768/42768 [==============================] - 34s 797us/step - loss: 1.4336 - acc: 0.4531 - val_loss: 1.5682 - val_acc: 0.4111\n",
      "Epoch 17/100\n",
      "42768/42768 [==============================] - 34s 798us/step - loss: 1.4111 - acc: 0.4609 - val_loss: 1.5681 - val_acc: 0.4151\n",
      "Epoch 18/100\n",
      "42768/42768 [==============================] - 35s 824us/step - loss: 1.3965 - acc: 0.4684 - val_loss: 1.5747 - val_acc: 0.4115\n",
      "Epoch 19/100\n",
      "42768/42768 [==============================] - 34s 797us/step - loss: 1.3815 - acc: 0.4737 - val_loss: 1.5693 - val_acc: 0.4152\n",
      "Epoch 20/100\n",
      "42768/42768 [==============================] - 34s 796us/step - loss: 1.3642 - acc: 0.4814 - val_loss: 1.5850 - val_acc: 0.4150\n",
      "Epoch 21/100\n",
      "42768/42768 [==============================] - 34s 796us/step - loss: 1.3480 - acc: 0.4892 - val_loss: 1.5762 - val_acc: 0.4215\n",
      "Epoch 22/100\n",
      "42768/42768 [==============================] - 34s 801us/step - loss: 1.3335 - acc: 0.4940 - val_loss: 1.5699 - val_acc: 0.4179\n",
      "Epoch 23/100\n",
      "42768/42768 [==============================] - 34s 798us/step - loss: 1.3175 - acc: 0.5024 - val_loss: 1.5783 - val_acc: 0.4199\n",
      "Epoch 24/100\n",
      "42768/42768 [==============================] - 34s 798us/step - loss: 1.3008 - acc: 0.5097 - val_loss: 1.5713 - val_acc: 0.4220\n",
      "Epoch 25/100\n",
      "42768/42768 [==============================] - 34s 801us/step - loss: 1.2875 - acc: 0.5126 - val_loss: 1.5693 - val_acc: 0.4212\n",
      "Epoch 26/100\n",
      "42768/42768 [==============================] - 34s 801us/step - loss: 1.2721 - acc: 0.5207 - val_loss: 1.5945 - val_acc: 0.4198\n",
      "Epoch 27/100\n",
      "42768/42768 [==============================] - 34s 801us/step - loss: 1.2606 - acc: 0.5260 - val_loss: 1.5812 - val_acc: 0.4229\n",
      "Epoch 28/100\n",
      "42768/42768 [==============================] - 34s 802us/step - loss: 1.2416 - acc: 0.5303 - val_loss: 1.5680 - val_acc: 0.4298\n",
      "Epoch 29/100\n",
      "42768/42768 [==============================] - 35s 807us/step - loss: 1.2321 - acc: 0.5362 - val_loss: 1.5716 - val_acc: 0.4314\n",
      "Epoch 30/100\n",
      "42768/42768 [==============================] - 34s 805us/step - loss: 1.2163 - acc: 0.5418 - val_loss: 1.5965 - val_acc: 0.4321\n",
      "Epoch 31/100\n",
      "42768/42768 [==============================] - 34s 804us/step - loss: 1.2094 - acc: 0.5447 - val_loss: 1.6050 - val_acc: 0.4239\n",
      "Epoch 32/100\n",
      "42768/42768 [==============================] - 35s 807us/step - loss: 1.1934 - acc: 0.5533 - val_loss: 1.6068 - val_acc: 0.4275\n",
      "Epoch 33/100\n",
      "42768/42768 [==============================] - 35s 810us/step - loss: 1.1802 - acc: 0.5576 - val_loss: 1.5984 - val_acc: 0.4238\n",
      "Epoch 34/100\n",
      "42768/42768 [==============================] - 34s 805us/step - loss: 1.1694 - acc: 0.5632 - val_loss: 1.5965 - val_acc: 0.4297\n",
      "Epoch 35/100\n",
      "42768/42768 [==============================] - 34s 806us/step - loss: 1.1566 - acc: 0.5704 - val_loss: 1.6290 - val_acc: 0.4228\n",
      "Epoch 36/100\n",
      "42768/42768 [==============================] - 35s 809us/step - loss: 1.1451 - acc: 0.5739 - val_loss: 1.6159 - val_acc: 0.4234\n",
      "Epoch 37/100\n",
      "42768/42768 [==============================] - 35s 812us/step - loss: 1.1327 - acc: 0.5764 - val_loss: 1.6349 - val_acc: 0.4266\n",
      "Epoch 38/100\n",
      "42768/42768 [==============================] - 35s 811us/step - loss: 1.1229 - acc: 0.5822 - val_loss: 1.6340 - val_acc: 0.4305\n",
      "Epoch 39/100\n",
      "42768/42768 [==============================] - 36s 851us/step - loss: 1.1099 - acc: 0.5889 - val_loss: 1.6357 - val_acc: 0.4316\n",
      "Epoch 40/100\n",
      "42768/42768 [==============================] - 38s 881us/step - loss: 1.1009 - acc: 0.5920 - val_loss: 1.6566 - val_acc: 0.4288\n",
      "Epoch 41/100\n",
      "42768/42768 [==============================] - 38s 883us/step - loss: 1.0905 - acc: 0.5983 - val_loss: 1.6533 - val_acc: 0.4309\n",
      "Epoch 42/100\n",
      "42768/42768 [==============================] - 35s 828us/step - loss: 1.0816 - acc: 0.5994 - val_loss: 1.6633 - val_acc: 0.4313\n",
      "Epoch 43/100\n",
      "42768/42768 [==============================] - 37s 862us/step - loss: 1.0719 - acc: 0.6044 - val_loss: 1.6786 - val_acc: 0.4314\n",
      "Epoch 44/100\n",
      "42768/42768 [==============================] - 39s 923us/step - loss: 1.0565 - acc: 0.6099 - val_loss: 1.6864 - val_acc: 0.4361\n",
      "Epoch 45/100\n",
      "42768/42768 [==============================] - 36s 838us/step - loss: 1.0485 - acc: 0.6152 - val_loss: 1.6755 - val_acc: 0.4356\n",
      "Epoch 46/100\n",
      "42768/42768 [==============================] - 37s 863us/step - loss: 1.0411 - acc: 0.6156 - val_loss: 1.6822 - val_acc: 0.4369\n",
      "Epoch 47/100\n",
      "42768/42768 [==============================] - 36s 834us/step - loss: 1.0383 - acc: 0.6189 - val_loss: 1.6861 - val_acc: 0.4335\n",
      "Epoch 48/100\n",
      "42768/42768 [==============================] - 36s 832us/step - loss: 1.0215 - acc: 0.6267 - val_loss: 1.7073 - val_acc: 0.4366\n",
      "Epoch 49/100\n",
      "42768/42768 [==============================] - 38s 891us/step - loss: 1.0156 - acc: 0.6290 - val_loss: 1.7236 - val_acc: 0.4357\n",
      "Epoch 50/100\n",
      "42768/42768 [==============================] - 37s 869us/step - loss: 1.0002 - acc: 0.6333 - val_loss: 1.7242 - val_acc: 0.4332\n",
      "Epoch 51/100\n",
      "42768/42768 [==============================] - 34s 802us/step - loss: 0.9981 - acc: 0.6357 - val_loss: 1.7567 - val_acc: 0.4338\n",
      "Epoch 52/100\n",
      "42768/42768 [==============================] - 32s 758us/step - loss: 0.9872 - acc: 0.6387 - val_loss: 1.7581 - val_acc: 0.4358\n",
      "Epoch 53/100\n",
      "42768/42768 [==============================] - 33s 762us/step - loss: 0.9731 - acc: 0.6479 - val_loss: 1.7419 - val_acc: 0.4379\n",
      "Epoch 54/100\n",
      "42768/42768 [==============================] - 32s 756us/step - loss: 0.9679 - acc: 0.6469 - val_loss: 1.7363 - val_acc: 0.4397\n",
      "Epoch 55/100\n",
      "42768/42768 [==============================] - 32s 755us/step - loss: 0.9615 - acc: 0.6505 - val_loss: 1.7338 - val_acc: 0.4406\n",
      "Epoch 56/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.9517 - acc: 0.6560 - val_loss: 1.7362 - val_acc: 0.4427\n",
      "Epoch 57/100\n",
      "42768/42768 [==============================] - 32s 756us/step - loss: 0.9428 - acc: 0.6591 - val_loss: 1.7801 - val_acc: 0.4376\n",
      "Epoch 58/100\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.9382 - acc: 0.6615 - val_loss: 1.7783 - val_acc: 0.4426\n",
      "Epoch 59/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.9305 - acc: 0.6656 - val_loss: 1.7981 - val_acc: 0.4419\n",
      "Epoch 60/100\n",
      "42768/42768 [==============================] - 32s 755us/step - loss: 0.9241 - acc: 0.6659 - val_loss: 1.8278 - val_acc: 0.4425\n",
      "Epoch 61/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.9144 - acc: 0.6686 - val_loss: 1.8162 - val_acc: 0.4393\n",
      "Epoch 62/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.9015 - acc: 0.6767 - val_loss: 1.7962 - val_acc: 0.4432\n",
      "Epoch 63/100\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.8953 - acc: 0.6770 - val_loss: 1.8196 - val_acc: 0.4388\n",
      "Epoch 64/100\n",
      "42768/42768 [==============================] - 32s 755us/step - loss: 0.8948 - acc: 0.6779 - val_loss: 1.8400 - val_acc: 0.4416\n",
      "Epoch 65/100\n",
      "42768/42768 [==============================] - 33s 764us/step - loss: 0.8881 - acc: 0.6859 - val_loss: 1.8124 - val_acc: 0.4378\n",
      "Epoch 66/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.8762 - acc: 0.6853 - val_loss: 1.8170 - val_acc: 0.4442\n",
      "Epoch 67/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.8703 - acc: 0.6900 - val_loss: 1.8558 - val_acc: 0.4466\n",
      "Epoch 68/100\n",
      "42768/42768 [==============================] - 32s 755us/step - loss: 0.8638 - acc: 0.6872 - val_loss: 1.8627 - val_acc: 0.4441\n",
      "Epoch 69/100\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.8595 - acc: 0.6946 - val_loss: 1.8735 - val_acc: 0.4395\n",
      "Epoch 70/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.8512 - acc: 0.6961 - val_loss: 1.8819 - val_acc: 0.4430\n",
      "Epoch 71/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.8451 - acc: 0.6983 - val_loss: 1.8719 - val_acc: 0.4418\n",
      "Epoch 72/100\n",
      "42768/42768 [==============================] - 33s 769us/step - loss: 0.8416 - acc: 0.6983 - val_loss: 1.8967 - val_acc: 0.4416\n",
      "Epoch 73/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.8273 - acc: 0.7044 - val_loss: 1.8871 - val_acc: 0.4418\n",
      "Epoch 74/100\n",
      "42768/42768 [==============================] - 32s 751us/step - loss: 0.8301 - acc: 0.7060 - val_loss: 1.8671 - val_acc: 0.4438\n",
      "Epoch 75/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.8278 - acc: 0.7054 - val_loss: 1.8937 - val_acc: 0.4441\n",
      "Epoch 76/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.8173 - acc: 0.7111 - val_loss: 1.9048 - val_acc: 0.4480\n",
      "Epoch 77/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.8072 - acc: 0.7157 - val_loss: 1.9409 - val_acc: 0.4394\n",
      "Epoch 78/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.8039 - acc: 0.7150 - val_loss: 1.9232 - val_acc: 0.4453\n",
      "Epoch 79/100\n",
      "42768/42768 [==============================] - 32s 755us/step - loss: 0.7995 - acc: 0.7165 - val_loss: 1.9192 - val_acc: 0.4406\n",
      "Epoch 80/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7964 - acc: 0.7184 - val_loss: 1.9511 - val_acc: 0.4361\n",
      "Epoch 81/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.7933 - acc: 0.7203 - val_loss: 1.9428 - val_acc: 0.4459\n",
      "Epoch 82/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7881 - acc: 0.7207 - val_loss: 1.9577 - val_acc: 0.4406\n",
      "Epoch 83/100\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.7822 - acc: 0.7225 - val_loss: 1.9676 - val_acc: 0.4409\n",
      "Epoch 84/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7721 - acc: 0.7270 - val_loss: 1.9585 - val_acc: 0.4397\n",
      "Epoch 85/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7625 - acc: 0.7309 - val_loss: 1.9854 - val_acc: 0.4397\n",
      "Epoch 86/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7665 - acc: 0.7301 - val_loss: 1.9704 - val_acc: 0.4424\n",
      "Epoch 87/100\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.7587 - acc: 0.7342 - val_loss: 1.9297 - val_acc: 0.4431\n",
      "Epoch 88/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7546 - acc: 0.7350 - val_loss: 1.9790 - val_acc: 0.4428\n",
      "Epoch 89/100\n",
      "42768/42768 [==============================] - 32s 751us/step - loss: 0.7480 - acc: 0.7384 - val_loss: 1.9934 - val_acc: 0.4399\n",
      "Epoch 90/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7479 - acc: 0.7365 - val_loss: 2.0000 - val_acc: 0.4472\n",
      "Epoch 91/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.7403 - acc: 0.7413 - val_loss: 1.9670 - val_acc: 0.4413\n",
      "Epoch 92/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7324 - acc: 0.7437 - val_loss: 2.0213 - val_acc: 0.4395\n",
      "Epoch 93/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.7268 - acc: 0.7483 - val_loss: 2.0332 - val_acc: 0.4440\n",
      "Epoch 94/100\n",
      "42768/42768 [==============================] - 32s 754us/step - loss: 0.7279 - acc: 0.7470 - val_loss: 1.9813 - val_acc: 0.4448\n",
      "Epoch 95/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7275 - acc: 0.7451 - val_loss: 2.0423 - val_acc: 0.4447\n",
      "Epoch 96/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.7179 - acc: 0.7498 - val_loss: 2.0421 - val_acc: 0.4429\n",
      "Epoch 97/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7180 - acc: 0.7481 - val_loss: 2.0301 - val_acc: 0.4418\n",
      "Epoch 98/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7121 - acc: 0.7531 - val_loss: 2.0658 - val_acc: 0.4461\n",
      "Epoch 99/100\n",
      "42768/42768 [==============================] - 32s 753us/step - loss: 0.7062 - acc: 0.7553 - val_loss: 2.0796 - val_acc: 0.4424\n",
      "Epoch 100/100\n",
      "42768/42768 [==============================] - 32s 752us/step - loss: 0.7071 - acc: 0.7512 - val_loss: 2.0429 - val_acc: 0.4408\n"
     ]
    }
   ],
   "source": [
    "model_4_hist=regressor.fit(X_train, y_train, epochs = 100, batch_size = 32, validation_data=(X_test, y_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model 5: Stateful learning on size 1 elements</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=isear[1]\n",
    "X=X.apply(clean)\n",
    "y=isear[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    num_words=len(numbers_series)\n",
    "    X_1 = []\n",
    "    y_1=  []\n",
    "    for index in range(0, num_words):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(len(doc)):\n",
    "            X_1.append(doc[i])\n",
    "            y_1.append(y.iloc[index])\n",
    "    return X_1, y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, y_1=transform_train(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(y_1)\n",
    "y_1=encoder.transform(y_1)\n",
    "y_1=np_utils.to_categorical(y_1)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=np.reshape(X_1, (X_1.shape[0], 1, X_1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.2, random_state = 0)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58205, 1, 300)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 25, return_sequences = True, stateful=True,  batch_input_shape=(35, 1, 300)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 25, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 25, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 25))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "58205/58205 [==============================] - 11s 187us/step - loss: 0.1149 - acc: 0.2824\n",
      "Epoch 2/25\n",
      "58205/58205 [==============================] - 8s 140us/step - loss: 0.1150 - acc: 0.2831\n",
      "Epoch 3/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1152 - acc: 0.2803\n",
      "Epoch 4/25\n",
      "58205/58205 [==============================] - 8s 140us/step - loss: 0.1150 - acc: 0.2807\n",
      "Epoch 5/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1151 - acc: 0.2816\n",
      "Epoch 6/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1151 - acc: 0.2788\n",
      "Epoch 7/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1153 - acc: 0.2792\n",
      "Epoch 8/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1153 - acc: 0.2784\n",
      "Epoch 9/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1153 - acc: 0.2774\n",
      "Epoch 10/25\n",
      "58205/58205 [==============================] - 8s 143us/step - loss: 0.1154 - acc: 0.2774\n",
      "Epoch 11/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1154 - acc: 0.2757\n",
      "Epoch 12/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1156 - acc: 0.2738\n",
      "Epoch 13/25\n",
      "58205/58205 [==============================] - 8s 143us/step - loss: 0.1155 - acc: 0.2745\n",
      "Epoch 14/25\n",
      "58205/58205 [==============================] - 8s 143us/step - loss: 0.1157 - acc: 0.2736\n",
      "Epoch 15/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1157 - acc: 0.2722\n",
      "Epoch 16/25\n",
      "58205/58205 [==============================] - 8s 140us/step - loss: 0.1159 - acc: 0.2675\n",
      "Epoch 17/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1162 - acc: 0.2638\n",
      "Epoch 18/25\n",
      "58205/58205 [==============================] - 8s 141us/step - loss: 0.1164 - acc: 0.2621\n",
      "Epoch 19/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1166 - acc: 0.2568\n",
      "Epoch 20/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1168 - acc: 0.2548\n",
      "Epoch 21/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1167 - acc: 0.2520\n",
      "Epoch 22/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1168 - acc: 0.2507\n",
      "Epoch 23/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1168 - acc: 0.2489\n",
      "Epoch 24/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1168 - acc: 0.2497\n",
      "Epoch 25/25\n",
      "58205/58205 [==============================] - 8s 142us/step - loss: 0.1167 - acc: 0.2517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xdad397f0>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'mean_squared_error')\n",
    "#Fitting to training set\n",
    "regressor.fit(X_train, y_train, epochs = 25, batch_size = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Only run below once </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test= X_test[:13475], y_test[:13475]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13475/13475 [==============================] - 2s 113us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11635366178952254, 0.2500185603106564]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.evaluate(X_test, y_test, batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Notes on size 1: Not as good as size 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model 6: timestep size 6 (double of first model)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=isear[1]\n",
    "X=X.apply(clean)\n",
    "y=isear[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_t6_train(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1 = []\n",
    "    y_1= []\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(6, len(doc)):\n",
    "            X_1.append(doc[i-6:i])\n",
    "            y_1.append(y.iloc[index])\n",
    "    return X_1, y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_t6_input(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    num_docs=len(numbers_series)\n",
    "    if numbers_series[0].size!=0:\n",
    "        for index, doc in enumerate(numbers_series):\n",
    "            while len(doc)<7:\n",
    "                orig_doc=doc.copy()\n",
    "                orig_doc=list(orig_doc)\n",
    "                doc=list(doc)\n",
    "                for word in orig_doc:\n",
    "                    doc.append(word)\n",
    "                modified=True\n",
    "            numbers_series.iloc[index]=np.array(doc)\n",
    "        X_1 = []\n",
    "        if num_docs>1:\n",
    "            for index in range(0, num_docs):\n",
    "                doc=numbers_series.iloc[index]\n",
    "                for i in range(6, len(doc)):\n",
    "                    X_1.append(doc[i-6:i])\n",
    "        else:\n",
    "            doc=numbers_series.iloc[0]\n",
    "            for i in range(6, len(doc)):\n",
    "                    X_1.append(doc[i-6:i])\n",
    "        return np.array(X_1)\n",
    "    else:\n",
    "        return 'not_found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie=transform_t6_input('I like to eat pie a lot it is yummy and very delicious')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, y_1=transform_t6_train(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32468, 6, 300)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1=np.array(X_1)\n",
    "X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(y_1)\n",
    "y_1=encoder.transform(y_1)\n",
    "y_1=np_utils.to_categorical(y_1)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.2, random_state = 0)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(num_emotions):\n",
    "    # Initialising the RNN\n",
    "    regressor = Sequential()\n",
    "\n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    regressor.add(LSTM(units = 25, return_sequences = True, input_dim=300))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a second LSTM layer and some Dropout regularisation\n",
    "    regressor.add(LSTM(units = 25, return_sequences = True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a third LSTM layer and some Dropout regularisation\n",
    "    regressor.add(LSTM(units = 25, return_sequences = True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a fourth LSTM layer and some Dropout regularisation\n",
    "    regressor.add(LSTM(units = 25))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding the output layer\n",
    "    regressor.add(Dense(units = num_emotions))\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25974/25974 [==============================] - 17s 637us/step - loss: 0.0240 - acc: 0.9157\n",
      "Epoch 2/25\n",
      "25974/25974 [==============================] - 12s 470us/step - loss: 0.0236 - acc: 0.9177\n",
      "Epoch 3/25\n",
      "25974/25974 [==============================] - 12s 462us/step - loss: 0.0239 - acc: 0.9175\n",
      "Epoch 4/25\n",
      "25974/25974 [==============================] - 12s 470us/step - loss: 0.0238 - acc: 0.9171\n",
      "Epoch 5/25\n",
      "25974/25974 [==============================] - 12s 468us/step - loss: 0.0233 - acc: 0.9191\n",
      "Epoch 6/25\n",
      "25974/25974 [==============================] - 12s 467us/step - loss: 0.0237 - acc: 0.9173\n",
      "Epoch 7/25\n",
      "25974/25974 [==============================] - 12s 472us/step - loss: 0.0232 - acc: 0.9202\n",
      "Epoch 8/25\n",
      "25974/25974 [==============================] - 12s 468us/step - loss: 0.0232 - acc: 0.9205\n",
      "Epoch 9/25\n",
      "25974/25974 [==============================] - 12s 473us/step - loss: 0.0232 - acc: 0.9193\n",
      "Epoch 10/25\n",
      "25974/25974 [==============================] - 12s 469us/step - loss: 0.0231 - acc: 0.9212\n",
      "Epoch 11/25\n",
      "25974/25974 [==============================] - 12s 473us/step - loss: 0.0235 - acc: 0.9196\n",
      "Epoch 12/25\n",
      "25974/25974 [==============================] - 12s 473us/step - loss: 0.0227 - acc: 0.9234\n",
      "Epoch 13/25\n",
      "25974/25974 [==============================] - 12s 471us/step - loss: 0.0230 - acc: 0.9216\n",
      "Epoch 14/25\n",
      "25974/25974 [==============================] - 12s 469us/step - loss: 0.0233 - acc: 0.9204\n",
      "Epoch 15/25\n",
      "25974/25974 [==============================] - 12s 469us/step - loss: 0.0230 - acc: 0.9221\n",
      "Epoch 16/25\n",
      "25974/25974 [==============================] - 12s 473us/step - loss: 0.0229 - acc: 0.9223\n",
      "Epoch 17/25\n",
      "25974/25974 [==============================] - 13s 491us/step - loss: 0.0230 - acc: 0.9215\n",
      "Epoch 18/25\n",
      "25974/25974 [==============================] - 12s 471us/step - loss: 0.0223 - acc: 0.9243\n",
      "Epoch 19/25\n",
      "25974/25974 [==============================] - 12s 474us/step - loss: 0.0224 - acc: 0.9244\n",
      "Epoch 20/25\n",
      "25974/25974 [==============================] - 12s 478us/step - loss: 0.0224 - acc: 0.9245\n",
      "Epoch 21/25\n",
      "25974/25974 [==============================] - 12s 475us/step - loss: 0.0227 - acc: 0.9233\n",
      "Epoch 22/25\n",
      "25974/25974 [==============================] - 12s 472us/step - loss: 0.0219 - acc: 0.9258\n",
      "Epoch 23/25\n",
      "25974/25974 [==============================] - 12s 475us/step - loss: 0.0227 - acc: 0.9237\n",
      "Epoch 24/25\n",
      "25974/25974 [==============================] - 12s 475us/step - loss: 0.0219 - acc: 0.9268\n",
      "Epoch 25/25\n",
      "25974/25974 [==============================] - 12s 471us/step - loss: 0.0219 - acc: 0.9275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xdac10b70>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'mean_squared_error')\n",
    "#Fitting to training set\n",
    "regressor.fit(X_train, y_train, epochs = 25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6494/6494 [==============================] - 2s 297us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05276456107346735, 0.7685555896466792]"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text, model):\n",
    "    vec_text=transform_t6_input(text)\n",
    "    pred=model.predict(vec_text)\n",
    "    pred=np.mean(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_duplicator(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits\n",
    "    num_docs=len(numbers_series)\n",
    "    for index, doc in enumerate(numbers_series):\n",
    "        print(len(doc))\n",
    "        while len(doc)<7:\n",
    "            orig_doc=doc.copy()\n",
    "            orig_doc=list(orig_doc)\n",
    "            doc=list(doc)\n",
    "            for word in orig_doc:\n",
    "                doc.append(word)\n",
    "                #doc=np.insert(doc,(len(doc)),word, axis=0)\n",
    "                #doc=np.append(doc, word, axis=1)\n",
    "            modified=True\n",
    "        numbers_series.iloc[index]=np.array(doc)\n",
    "    X_1 = []\n",
    "    if num_docs>1:\n",
    "        for index in range(0, num_docs):\n",
    "            doc=numbers_series.iloc[index]\n",
    "            for i in range(6, len(doc)):\n",
    "                X_1.append(doc[i-6:i])\n",
    "    else:\n",
    "        doc=numbers_series.iloc[0]\n",
    "        print(doc.shape)\n",
    "        for i in range(6, len(doc)):\n",
    "                X_1.append(doc[i-6:i])\n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['love', 'everybody', 'love', 'everybody', 'love', 'everybody'],\n",
       "       dtype='<U9'),\n",
       " array(['everybody', 'love', 'everybody', 'love', 'everybody', 'love'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_duplicator('I love everybody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(8, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.0217679e-03,  6.1481744e-03,  1.6123056e-03, -7.1944296e-04,\n",
       "         2.5430262e-02,  9.4908869e-01,  7.6540709e-03],\n",
       "       [ 6.0217679e-03,  6.1481744e-03,  1.6123056e-03, -7.1944296e-04,\n",
       "         2.5430262e-02,  9.4908869e-01,  7.6540709e-03]], dtype=float32)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing amazon reviews sample</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=pd.read_csv(\"amazon_reviews_utf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I bought this umbrella a few months ago and unfortunately, it broke while it was windy out'"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['Review Text'].iloc[1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>Verifed Review</th>\n",
       "      <th>Helpful Vote Count</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Author Link</th>\n",
       "      <th>Review Link</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So far, so good (and dry)</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>25</td>\n",
       "      <td>November 9, 2016</td>\n",
       "      <td>S. R. Southard</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3V8GI...</td>\n",
       "      <td>I had my first opportunity to use the umbrella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not really a lifetime warranty...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>July 10, 2018</td>\n",
       "      <td>Rico</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1ZA3B...</td>\n",
       "      <td>I bought this umbrella a few months ago and un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"THIS IS THE ONLY UMBRELLA YOU SHOULD EVER PUR...</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>January 22, 2018</td>\n",
       "      <td>On The Fly</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RG5V0S...</td>\n",
       "      <td>First of all: I have no financial interest in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Most good quality umbrellas can't stand up to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>April 25, 2018</td>\n",
       "      <td>Linann</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2EYVB...</td>\n",
       "      <td>I work in New Haven CT, and in the fall, and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decent umbrella compromised by poor-quality ri...</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>3</td>\n",
       "      <td>October 15, 2017</td>\n",
       "      <td>Edward Ripley-Duggan</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1L9TU...</td>\n",
       "      <td>The umbrella appears to be robust; I used it r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it is still like new. Great construction</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 14, 2016</td>\n",
       "      <td>MC Oddslice</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2W29X...</td>\n",
       "      <td>Update: Too small! If there is wind, you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>For a tiny person</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>May 25, 2018</td>\n",
       "      <td>AR</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3FPDT...</td>\n",
       "      <td>I was excited to try out this umbrella based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decent, a bit pricey?</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>September 2, 2017</td>\n",
       "      <td>Alex Wang</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R12Q4A...</td>\n",
       "      <td>This umbrella is pretty good. never had a umbr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Not consistent with reviews or description.</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>3</td>\n",
       "      <td>June 20, 2017</td>\n",
       "      <td>Hannah</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1UX1K...</td>\n",
       "      <td>I must have received a bad item or something b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Seems well-built and well-designed.</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>Terende</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3C9IS...</td>\n",
       "      <td>I bought two of these for my and my spouse's u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Good Quality but Nearly Useless - Umbrella for...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>July 3, 2018</td>\n",
       "      <td>Robert S. Blackie</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R9Y91F...</td>\n",
       "      <td>I cannot argue with the fact that this umbrell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>... have to say this one is by far my favorite...</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>IJ</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3TDZY...</td>\n",
       "      <td>I've owned at least 50 umbrellas (some bought ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>classy cute umbrella</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>March 25, 2018</td>\n",
       "      <td>lawAbidingCitizen</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2PYPT...</td>\n",
       "      <td>I have benkii before(less1 year) but i lost it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Well made product, excellent vendor</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>April 7, 2018</td>\n",
       "      <td>sherm624</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2VU5A...</td>\n",
       "      <td>Change from 2 to 5 stars.  After a not-so-grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This one goes to 9...</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>January 29, 2017</td>\n",
       "      <td>Mergatroid</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1C6RZ...</td>\n",
       "      <td>Nicely built. The metal shaft is black however...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>support</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>January 23, 2018</td>\n",
       "      <td>Robert J. Moser</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2IXZT...</td>\n",
       "      <td>I am a 67 year old banker, been banking for ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Excellent umbrella- would definitely recommend</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>October 18, 2017</td>\n",
       "      <td>G Tan</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R38B28...</td>\n",
       "      <td>I like bullet points, thus: + Excellent build ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Umbrella has lots of good features</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>December 2, 2017</td>\n",
       "      <td>Lori C.</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2MWM3...</td>\n",
       "      <td>3rd party seller very involved, customer servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NOT \"super light\" or lightweight. Misleading p...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>July 20, 2017</td>\n",
       "      <td>JulieT</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R32MI0...</td>\n",
       "      <td>I was looking for a lightweight umbrella to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nope nope nope</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>June 15, 2018</td>\n",
       "      <td>BillyP</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3DZFN...</td>\n",
       "      <td>I moved to San Francisco and needed an umbrell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Not what I had hoped....</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>Krysrox413</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1TAZB...</td>\n",
       "      <td>For all the money I paid for this umbrella, I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nice while it lasted</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>January 9, 2017</td>\n",
       "      <td>Ben Perkins</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RFQF9R...</td>\n",
       "      <td>It's ok, the build quality seemed nice at firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stands up to tough weather</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 10, 2017</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RS0OF5...</td>\n",
       "      <td>I live in Chicago suburbia so I have very litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Great size.</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>June 10, 2018</td>\n",
       "      <td>Seaside Sarah</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/ROM02O...</td>\n",
       "      <td>For a compact umbrella, it's a \"big\" umbrella ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Can withstand powerful wind</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>May 6, 2018</td>\n",
       "      <td>Zavage</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3DKA9...</td>\n",
       "      <td>Being a bachelor, I never really felt the need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Great product and customer service</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>January 3, 2017</td>\n",
       "      <td>M.G.M.</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2N49P...</td>\n",
       "      <td>The umbrella feels sturdy. Haven't experienced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Has potential - top piece unreliable</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>June 19, 2017</td>\n",
       "      <td>Kevin Lee</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R111SP...</td>\n",
       "      <td>**Update: I'm updating my review from 2 to 4 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>well-made umbrella</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>February 5, 2018</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2TZXS...</td>\n",
       "      <td>I ordered this during a rainstorm and since it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Good value!</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>January 25, 2017</td>\n",
       "      <td>Clifford Carroll</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RY5UFB...</td>\n",
       "      <td>This umbrella is of better apparent quality th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sturdy and Well-Constructed</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 17, 2017</td>\n",
       "      <td>Justin Paul</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3VOD7...</td>\n",
       "      <td>Feels sturdy and a well constructed umbrella. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>December 15, 2017</td>\n",
       "      <td>jensb</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1WOIR...</td>\n",
       "      <td>Great product and fast shipping, thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>I recommend it.</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 18, 2016</td>\n",
       "      <td>Keith McCormick</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R32LUL...</td>\n",
       "      <td>It's an expensive umbrella but i assure you th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 19, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2QU4V...</td>\n",
       "      <td>Great quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 18, 2017</td>\n",
       "      <td>Jenny Larkin</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R39NX4...</td>\n",
       "      <td>Great product!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>One Star</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>November 26, 2017</td>\n",
       "      <td>Jennifer Franzone</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2QKJ0...</td>\n",
       "      <td>It was very heavy! Not what I expected.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 30, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1ZJUE...</td>\n",
       "      <td>Super product.... excellent value..buy 2!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>This is a very good umbrella and functions wel...</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 29, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R36FW6...</td>\n",
       "      <td>This is a very good umbrella and functions wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 19, 2017</td>\n",
       "      <td>MattSanders</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R204XQ...</td>\n",
       "      <td>Amazing, easy to use umbrella!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>April 16, 2017</td>\n",
       "      <td>Nicholas Rodriguez</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RVUQ68...</td>\n",
       "      <td>Sturdy and built to last. These umbrellas are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 22, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RQ2D6B...</td>\n",
       "      <td>Feels very sturdy and is a decent size - bigge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>May 5, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2TGWQ...</td>\n",
       "      <td>Nice quality umbrella. Arrived earlier than ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>Great product from a company that stands behin...</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 9, 2016</td>\n",
       "      <td>CH</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R31W5S...</td>\n",
       "      <td>Holds up well in the wind. Great product from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>September 30, 2016</td>\n",
       "      <td>berta</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1UFPQ...</td>\n",
       "      <td>5 stars for this umbrella. Well made with high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>March 18, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RGAJTW...</td>\n",
       "      <td>So love this umbrella. Very easy to use. I wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>honest review</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 11, 2016</td>\n",
       "      <td>Richard villa olea</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3EQ9X...</td>\n",
       "      <td>cool product.i recieved for a reduced discount.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 21, 2017</td>\n",
       "      <td>Zhitun Yang</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R51QEV...</td>\n",
       "      <td>Five Star.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 9, 2016</td>\n",
       "      <td>Heather</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RJ8AM2...</td>\n",
       "      <td>Fits in my bag and keeps my hair and clothes d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>May 3, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3SUPI...</td>\n",
       "      <td>A wonderful umbrella at an excellent price!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 19, 2017</td>\n",
       "      <td>D A Sleeter</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1O2BJ...</td>\n",
       "      <td>solid construction.  haven't had rain yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>October 31, 2016</td>\n",
       "      <td>ZIX</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RQO1A8...</td>\n",
       "      <td>It is a well designed product and worth your m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 11, 2017</td>\n",
       "      <td>hudsonview</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2V8WY...</td>\n",
       "      <td>Great umbrella!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 25, 2016</td>\n",
       "      <td>kim formby</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3K1FY...</td>\n",
       "      <td>I received this Umbrella free and LOVE it. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 16, 2016</td>\n",
       "      <td>caro</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1UUCR...</td>\n",
       "      <td>The Dupont Teflon feels sturdy and keeps the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>Four Stars</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>April 5, 2016</td>\n",
       "      <td>Patricia</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R395TG...</td>\n",
       "      <td>love a good walk in the rain, its so relaxing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>very pleased.</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>December 11, 2016</td>\n",
       "      <td>KB</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R169R9...</td>\n",
       "      <td>These umbrellas are everything they say they a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>As Advertised</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 23, 2017</td>\n",
       "      <td>Corrie Kristina Luebke</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RFGLZK...</td>\n",
       "      <td>Works well so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>April 8, 2017</td>\n",
       "      <td>Rene</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RDCU3C...</td>\n",
       "      <td>Great umbrella!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>GREAT!</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 11, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3PODB...</td>\n",
       "      <td>Great product! Would recommend to anyone!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>May 18, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R10KGM...</td>\n",
       "      <td>Easy and well made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Four Stars</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 4, 2017</td>\n",
       "      <td>Ethan</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R11KOI...</td>\n",
       "      <td>The umbrella is easy to open and close.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Name  Star Rating  \\\n",
       "0                             So far, so good (and dry)            5   \n",
       "1                     Not really a lifetime warranty...            2   \n",
       "2     \"THIS IS THE ONLY UMBRELLA YOU SHOULD EVER PUR...            5   \n",
       "3     Most good quality umbrellas can't stand up to ...            3   \n",
       "4     Decent umbrella compromised by poor-quality ri...            3   \n",
       "5              it is still like new. Great construction            4   \n",
       "6                                     For a tiny person            2   \n",
       "7                                 Decent, a bit pricey?            4   \n",
       "8           Not consistent with reviews or description.            2   \n",
       "9                   Seems well-built and well-designed.            5   \n",
       "10    Good Quality but Nearly Useless - Umbrella for...            2   \n",
       "11    ... have to say this one is by far my favorite...            5   \n",
       "12                                 classy cute umbrella            5   \n",
       "13                  Well made product, excellent vendor            5   \n",
       "14                                This one goes to 9...            5   \n",
       "15                                              support            4   \n",
       "16       Excellent umbrella- would definitely recommend            5   \n",
       "17                   Umbrella has lots of good features            4   \n",
       "18    NOT \"super light\" or lightweight. Misleading p...            2   \n",
       "19                                       Nope nope nope            1   \n",
       "20                             Not what I had hoped....            2   \n",
       "21                                 Nice while it lasted            3   \n",
       "22                           Stands up to tough weather            5   \n",
       "23                                          Great size.            4   \n",
       "24                          Can withstand powerful wind            5   \n",
       "25                   Great product and customer service            5   \n",
       "26                 Has potential - top piece unreliable            4   \n",
       "27                                   well-made umbrella            5   \n",
       "28                                          Good value!            5   \n",
       "29                          Sturdy and Well-Constructed            4   \n",
       "...                                                 ...          ...   \n",
       "2370                                         Five Stars            5   \n",
       "2371                                    I recommend it.            5   \n",
       "2372                                         Five Stars            5   \n",
       "2373                                         Five Stars            5   \n",
       "2374                                           One Star            1   \n",
       "2375                                         Five Stars            5   \n",
       "2376  This is a very good umbrella and functions wel...            4   \n",
       "2377                                         Five Stars            5   \n",
       "2378                                         Five Stars            5   \n",
       "2379                                         Five Stars            5   \n",
       "2380                                         Five Stars            5   \n",
       "2381  Great product from a company that stands behin...            5   \n",
       "2382                                            5 stars            5   \n",
       "2383                                         Five Stars            5   \n",
       "2384                                      honest review            4   \n",
       "2385                                         Five Stars            5   \n",
       "2386                                         Five Stars            5   \n",
       "2387                                         Five Stars            5   \n",
       "2388                                         Five Stars            5   \n",
       "2389                                         Five Stars            5   \n",
       "2390                                         Five Stars            5   \n",
       "2391                                         Five Stars            5   \n",
       "2392                                            awesome            5   \n",
       "2393                                         Four Stars            4   \n",
       "2394                                      very pleased.            5   \n",
       "2395                                      As Advertised            5   \n",
       "2396                                         Five Stars            5   \n",
       "2397                                             GREAT!            5   \n",
       "2398                                         Five Stars            5   \n",
       "2399                                         Four Stars            4   \n",
       "\n",
       "     Verifed Review Helpful Vote Count                 Date  \\\n",
       "0               YES                 25     November 9, 2016   \n",
       "1               YES                 On        July 10, 2018   \n",
       "2               YES                 On     January 22, 2018   \n",
       "3               YES                  2       April 25, 2018   \n",
       "4               YES                  3     October 15, 2017   \n",
       "5               YES                 On      August 14, 2016   \n",
       "6               YES                 On         May 25, 2018   \n",
       "7               YES                  2    September 2, 2017   \n",
       "8               YES                  3        June 20, 2017   \n",
       "9               YES                  0     January 26, 2018   \n",
       "10              YES                  0         July 3, 2018   \n",
       "11              YES                  0    February 27, 2018   \n",
       "12              YES                  0       March 25, 2018   \n",
       "13              YES                  0        April 7, 2018   \n",
       "14              YES                 On     January 29, 2017   \n",
       "15              YES                 On     January 23, 2018   \n",
       "16              YES                  0     October 18, 2017   \n",
       "17              YES                 On     December 2, 2017   \n",
       "18              YES                 On        July 20, 2017   \n",
       "19              YES                 On        June 15, 2018   \n",
       "20              YES                  0          May 2, 2018   \n",
       "21              YES                  2      January 9, 2017   \n",
       "22              YES                 On      August 10, 2017   \n",
       "23              YES                  0        June 10, 2018   \n",
       "24              YES                  0          May 6, 2018   \n",
       "25              YES                  0      January 3, 2017   \n",
       "26              YES                 On        June 19, 2017   \n",
       "27              YES                  0     February 5, 2018   \n",
       "28              YES                  0     January 25, 2017   \n",
       "29              YES                 On      August 17, 2017   \n",
       "...             ...                ...                  ...   \n",
       "2370             NO                  0    December 15, 2017   \n",
       "2371             NO                  0        June 18, 2016   \n",
       "2372             NO                  0      August 19, 2017   \n",
       "2373             NO                  0        June 18, 2017   \n",
       "2374             NO                  0    November 26, 2017   \n",
       "2375             NO                  0      August 30, 2017   \n",
       "2376            YES                 On      August 29, 2017   \n",
       "2377             NO                  0        July 19, 2017   \n",
       "2378             NO                  0       April 16, 2017   \n",
       "2379             NO                  0    February 22, 2017   \n",
       "2380             NO                  0          May 5, 2017   \n",
       "2381             NO                  0         July 9, 2016   \n",
       "2382             NO                  0   September 30, 2016   \n",
       "2383             NO                  0       March 18, 2017   \n",
       "2384             NO                  0        July 11, 2016   \n",
       "2385             NO                  0      August 21, 2017   \n",
       "2386             NO                  0         June 9, 2016   \n",
       "2387             NO                  0          May 3, 2017   \n",
       "2388             NO                  0    February 19, 2017   \n",
       "2389             NO                  0     October 31, 2016   \n",
       "2390             NO                  0        June 11, 2017   \n",
       "2391             NO                  0      August 25, 2016   \n",
       "2392             NO                  0        July 16, 2016   \n",
       "2393             NO                  0        April 5, 2016   \n",
       "2394             NO                  0    December 11, 2016   \n",
       "2395             NO                  0        June 23, 2017   \n",
       "2396             NO                  0        April 8, 2017   \n",
       "2397             NO                  0    February 11, 2017   \n",
       "2398             NO                  0         May 18, 2017   \n",
       "2399             NO                  0     February 4, 2017   \n",
       "\n",
       "                      Author  \\\n",
       "0             S. R. Southard   \n",
       "1                       Rico   \n",
       "2                 On The Fly   \n",
       "3                     Linann   \n",
       "4       Edward Ripley-Duggan   \n",
       "5                MC Oddslice   \n",
       "6                         AR   \n",
       "7                  Alex Wang   \n",
       "8                     Hannah   \n",
       "9                    Terende   \n",
       "10         Robert S. Blackie   \n",
       "11                        IJ   \n",
       "12         lawAbidingCitizen   \n",
       "13                  sherm624   \n",
       "14                Mergatroid   \n",
       "15           Robert J. Moser   \n",
       "16                     G Tan   \n",
       "17                   Lori C.   \n",
       "18                    JulieT   \n",
       "19                    BillyP   \n",
       "20                Krysrox413   \n",
       "21               Ben Perkins   \n",
       "22                Alexandria   \n",
       "23             Seaside Sarah   \n",
       "24                    Zavage   \n",
       "25                    M.G.M.   \n",
       "26                 Kevin Lee   \n",
       "27                     Mitch   \n",
       "28          Clifford Carroll   \n",
       "29               Justin Paul   \n",
       "...                      ...   \n",
       "2370                   jensb   \n",
       "2371         Keith McCormick   \n",
       "2372         Amazon Customer   \n",
       "2373            Jenny Larkin   \n",
       "2374       Jennifer Franzone   \n",
       "2375         Amazon Customer   \n",
       "2376         Amazon Customer   \n",
       "2377             MattSanders   \n",
       "2378      Nicholas Rodriguez   \n",
       "2379         Amazon Customer   \n",
       "2380         Amazon Customer   \n",
       "2381                      CH   \n",
       "2382                   berta   \n",
       "2383         Amazon Customer   \n",
       "2384      Richard villa olea   \n",
       "2385             Zhitun Yang   \n",
       "2386                 Heather   \n",
       "2387         Amazon Customer   \n",
       "2388             D A Sleeter   \n",
       "2389                     ZIX   \n",
       "2390              hudsonview   \n",
       "2391              kim formby   \n",
       "2392                    caro   \n",
       "2393                Patricia   \n",
       "2394                      KB   \n",
       "2395  Corrie Kristina Luebke   \n",
       "2396                    Rene   \n",
       "2397         Amazon Customer   \n",
       "2398         Amazon Customer   \n",
       "2399                   Ethan   \n",
       "\n",
       "                                            Author Link  \\\n",
       "0     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "1     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "3     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "4     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "5     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "6     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "7     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "8     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "9     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "10    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "11    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "12    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "13    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "14    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "15    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "16    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "17    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "18    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "19    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "20    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "21    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "22    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "23    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "24    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "25    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "26    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "27    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "28    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "29    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "...                                                 ...   \n",
       "2370  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2371  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2372  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2373  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2374  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2375  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2376  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2377  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2378  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2379  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2380  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2381  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2382  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2383  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2384  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2385  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2386  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2387  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2388  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2389  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2390  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2391  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2392  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2393  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2394  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2395  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2396  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2397  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2398  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2399  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "\n",
       "                                            Review Link  \\\n",
       "0     https://amazon.com//gp/customer-reviews/R3V8GI...   \n",
       "1     https://amazon.com//gp/customer-reviews/R1ZA3B...   \n",
       "2     https://amazon.com//gp/customer-reviews/RG5V0S...   \n",
       "3     https://amazon.com//gp/customer-reviews/R2EYVB...   \n",
       "4     https://amazon.com//gp/customer-reviews/R1L9TU...   \n",
       "5     https://amazon.com//gp/customer-reviews/R2W29X...   \n",
       "6     https://amazon.com//gp/customer-reviews/R3FPDT...   \n",
       "7     https://amazon.com//gp/customer-reviews/R12Q4A...   \n",
       "8     https://amazon.com//gp/customer-reviews/R1UX1K...   \n",
       "9     https://amazon.com//gp/customer-reviews/R3C9IS...   \n",
       "10    https://amazon.com//gp/customer-reviews/R9Y91F...   \n",
       "11    https://amazon.com//gp/customer-reviews/R3TDZY...   \n",
       "12    https://amazon.com//gp/customer-reviews/R2PYPT...   \n",
       "13    https://amazon.com//gp/customer-reviews/R2VU5A...   \n",
       "14    https://amazon.com//gp/customer-reviews/R1C6RZ...   \n",
       "15    https://amazon.com//gp/customer-reviews/R2IXZT...   \n",
       "16    https://amazon.com//gp/customer-reviews/R38B28...   \n",
       "17    https://amazon.com//gp/customer-reviews/R2MWM3...   \n",
       "18    https://amazon.com//gp/customer-reviews/R32MI0...   \n",
       "19    https://amazon.com//gp/customer-reviews/R3DZFN...   \n",
       "20    https://amazon.com//gp/customer-reviews/R1TAZB...   \n",
       "21    https://amazon.com//gp/customer-reviews/RFQF9R...   \n",
       "22    https://amazon.com//gp/customer-reviews/RS0OF5...   \n",
       "23    https://amazon.com//gp/customer-reviews/ROM02O...   \n",
       "24    https://amazon.com//gp/customer-reviews/R3DKA9...   \n",
       "25    https://amazon.com//gp/customer-reviews/R2N49P...   \n",
       "26    https://amazon.com//gp/customer-reviews/R111SP...   \n",
       "27    https://amazon.com//gp/customer-reviews/R2TZXS...   \n",
       "28    https://amazon.com//gp/customer-reviews/RY5UFB...   \n",
       "29    https://amazon.com//gp/customer-reviews/R3VOD7...   \n",
       "...                                                 ...   \n",
       "2370  https://amazon.com//gp/customer-reviews/R1WOIR...   \n",
       "2371  https://amazon.com//gp/customer-reviews/R32LUL...   \n",
       "2372  https://amazon.com//gp/customer-reviews/R2QU4V...   \n",
       "2373  https://amazon.com//gp/customer-reviews/R39NX4...   \n",
       "2374  https://amazon.com//gp/customer-reviews/R2QKJ0...   \n",
       "2375  https://amazon.com//gp/customer-reviews/R1ZJUE...   \n",
       "2376  https://amazon.com//gp/customer-reviews/R36FW6...   \n",
       "2377  https://amazon.com//gp/customer-reviews/R204XQ...   \n",
       "2378  https://amazon.com//gp/customer-reviews/RVUQ68...   \n",
       "2379  https://amazon.com//gp/customer-reviews/RQ2D6B...   \n",
       "2380  https://amazon.com//gp/customer-reviews/R2TGWQ...   \n",
       "2381  https://amazon.com//gp/customer-reviews/R31W5S...   \n",
       "2382  https://amazon.com//gp/customer-reviews/R1UFPQ...   \n",
       "2383  https://amazon.com//gp/customer-reviews/RGAJTW...   \n",
       "2384  https://amazon.com//gp/customer-reviews/R3EQ9X...   \n",
       "2385  https://amazon.com//gp/customer-reviews/R51QEV...   \n",
       "2386  https://amazon.com//gp/customer-reviews/RJ8AM2...   \n",
       "2387  https://amazon.com//gp/customer-reviews/R3SUPI...   \n",
       "2388  https://amazon.com//gp/customer-reviews/R1O2BJ...   \n",
       "2389  https://amazon.com//gp/customer-reviews/RQO1A8...   \n",
       "2390  https://amazon.com//gp/customer-reviews/R2V8WY...   \n",
       "2391  https://amazon.com//gp/customer-reviews/R3K1FY...   \n",
       "2392  https://amazon.com//gp/customer-reviews/R1UUCR...   \n",
       "2393  https://amazon.com//gp/customer-reviews/R395TG...   \n",
       "2394  https://amazon.com//gp/customer-reviews/R169R9...   \n",
       "2395  https://amazon.com//gp/customer-reviews/RFGLZK...   \n",
       "2396  https://amazon.com//gp/customer-reviews/RDCU3C...   \n",
       "2397  https://amazon.com//gp/customer-reviews/R3PODB...   \n",
       "2398  https://amazon.com//gp/customer-reviews/R10KGM...   \n",
       "2399  https://amazon.com//gp/customer-reviews/R11KOI...   \n",
       "\n",
       "                                            Review Text  \n",
       "0     I had my first opportunity to use the umbrella...  \n",
       "1     I bought this umbrella a few months ago and un...  \n",
       "2     First of all: I have no financial interest in ...  \n",
       "3     I work in New Haven CT, and in the fall, and w...  \n",
       "4     The umbrella appears to be robust; I used it r...  \n",
       "5     Update: Too small! If there is wind, you will ...  \n",
       "6     I was excited to try out this umbrella based o...  \n",
       "7     This umbrella is pretty good. never had a umbr...  \n",
       "8     I must have received a bad item or something b...  \n",
       "9     I bought two of these for my and my spouse's u...  \n",
       "10    I cannot argue with the fact that this umbrell...  \n",
       "11    I've owned at least 50 umbrellas (some bought ...  \n",
       "12    I have benkii before(less1 year) but i lost it...  \n",
       "13    Change from 2 to 5 stars.  After a not-so-grea...  \n",
       "14    Nicely built. The metal shaft is black however...  \n",
       "15    I am a 67 year old banker, been banking for ov...  \n",
       "16    I like bullet points, thus: + Excellent build ...  \n",
       "17    3rd party seller very involved, customer servi...  \n",
       "18    I was looking for a lightweight umbrella to re...  \n",
       "19    I moved to San Francisco and needed an umbrell...  \n",
       "20    For all the money I paid for this umbrella, I'...  \n",
       "21    It's ok, the build quality seemed nice at firs...  \n",
       "22    I live in Chicago suburbia so I have very litt...  \n",
       "23    For a compact umbrella, it's a \"big\" umbrella ...  \n",
       "24    Being a bachelor, I never really felt the need...  \n",
       "25    The umbrella feels sturdy. Haven't experienced...  \n",
       "26    **Update: I'm updating my review from 2 to 4 s...  \n",
       "27    I ordered this during a rainstorm and since it...  \n",
       "28    This umbrella is of better apparent quality th...  \n",
       "29    Feels sturdy and a well constructed umbrella. ...  \n",
       "...                                                 ...  \n",
       "2370         Great product and fast shipping, thank you  \n",
       "2371  It's an expensive umbrella but i assure you th...  \n",
       "2372                                      Great quality  \n",
       "2373                                     Great product!  \n",
       "2374            It was very heavy! Not what I expected.  \n",
       "2375          Super product.... excellent value..buy 2!  \n",
       "2376  This is a very good umbrella and functions wel...  \n",
       "2377                     Amazing, easy to use umbrella!  \n",
       "2378  Sturdy and built to last. These umbrellas are ...  \n",
       "2379  Feels very sturdy and is a decent size - bigge...  \n",
       "2380  Nice quality umbrella. Arrived earlier than ex...  \n",
       "2381  Holds up well in the wind. Great product from ...  \n",
       "2382  5 stars for this umbrella. Well made with high...  \n",
       "2383  So love this umbrella. Very easy to use. I wou...  \n",
       "2384    cool product.i recieved for a reduced discount.  \n",
       "2385                                         Five Star.  \n",
       "2386  Fits in my bag and keeps my hair and clothes d...  \n",
       "2387        A wonderful umbrella at an excellent price!  \n",
       "2388          solid construction.  haven't had rain yet  \n",
       "2389  It is a well designed product and worth your m...  \n",
       "2390                                    Great umbrella!  \n",
       "2391  I received this Umbrella free and LOVE it. It ...  \n",
       "2392  The Dupont Teflon feels sturdy and keeps the r...  \n",
       "2393  love a good walk in the rain, its so relaxing ...  \n",
       "2394  These umbrellas are everything they say they a...  \n",
       "2395                                  Works well so far  \n",
       "2396                                    Great umbrella!  \n",
       "2397          Great product! Would recommend to anyone!  \n",
       "2398                                 Easy and well made  \n",
       "2399            The umbrella is easy to open and close.  \n",
       "\n",
       "[2400 rows x 9 columns]"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(7, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.05101114, 0.04537321, 0.715312  , 0.04185817, 0.03482933,\n",
       "       0.05562435, 0.05663225], dtype=float32)"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(reviews['Review Text'].iloc[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing DepecheMood</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "depeche=pd.read_csv('depeche_word_isolation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lemma#PoS', 'AFRAID', 'AMUSED', 'ANGRY', 'ANNOYED', 'DONT_CARE',\n",
       "       'HAPPY', 'INSPIRED', 'SAD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depeche.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger='ANGRY'+'ANNOYED'\n",
    "disgust='ANGRY'\n",
    "fear='AFRAID'\n",
    "guilt='SAD'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing fb-reactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions=pd.read_json('data/fb_reactions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions['avg_wow']=fb_reactions['fb_wow']/fb_reactions['fb_like']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions['message'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions.dropna(subset=['message'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions=fb_reactions.sort_values('avg_wow', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions=fb_reactions.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reactions['emotion']='surprise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['database', 'date_only', 'day_name', 'description', 'external_picture',\n",
       "       'fanpagelink', 'fb_angry', 'fb_haha', 'fb_id', 'fb_like', 'fb_love',\n",
       "       'fb_sad', 'fb_thankful', 'fb_total_reactions', 'fb_wow',\n",
       "       'highest_reaction', 'highest_reaction_extended', 'id', 'link',\n",
       "       'message', 'name', 'num_comments', 'page_id', 'preprocessed_name',\n",
       "       'preprocessed_stem_stop', 'shares', 'time_created', 'time_only', 'type',\n",
       "       'avg_wow', 'emotion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_reactions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise=fb_reactions.iloc[:1080]\n",
    "surprise_X=surprise['message']\n",
    "surprise_y=surprise['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_df=pd.DataFrame()\n",
    "surprise_df[0]=surprise_y\n",
    "surprise_df[1]=surprise_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27823</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Terrifying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27382</th>\n",
       "      <td>surprise</td>\n",
       "      <td>This is terrifying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28303</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Yikes!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33513</th>\n",
       "      <td>surprise</td>\n",
       "      <td>For some reason this is a little hard to belie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49643</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Passengers aboard Delta flight 762 experienced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28081</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The 33-year-old officer managed to avoid serio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33611</th>\n",
       "      <td>surprise</td>\n",
       "      <td>DEVELOPING...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27464</th>\n",
       "      <td>surprise</td>\n",
       "      <td>An industrial-sized barbecue grill broke free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7140</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Oh my dear god.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27269</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Awful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12968</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Snow in July? Not quite. But this storm produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28410</th>\n",
       "      <td>surprise</td>\n",
       "      <td>\"I'm just glad that my son wasn't wearing them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35154</th>\n",
       "      <td>surprise</td>\n",
       "      <td>What the?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27894</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Warn your friends!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33714</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Each WNBA team was fined $5,000 and its player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34614</th>\n",
       "      <td>surprise</td>\n",
       "      <td>An alligator dragged a 2-year-old into the wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34599</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The child’s body was found completely intact “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15072</th>\n",
       "      <td>surprise</td>\n",
       "      <td>\"This could happen at any store that sells tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47543</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Noooooo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25631</th>\n",
       "      <td>surprise</td>\n",
       "      <td>His unborn twin had hair, legs and genitals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28101</th>\n",
       "      <td>surprise</td>\n",
       "      <td>It’s their only way to get home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13434</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Colleen Burns, 35, was hiking along the trails...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8174</th>\n",
       "      <td>surprise</td>\n",
       "      <td>“No mother should have to go [through] what I’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7230</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The actor was most famous for appearing as Che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34460</th>\n",
       "      <td>surprise</td>\n",
       "      <td>\"I refuse to ruin the lives of two young men w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12957</th>\n",
       "      <td>surprise</td>\n",
       "      <td>A deer ate the thawed remains of an infected r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46672</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Bid farewell to your childhood. ???? ???? ????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34409</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Unbelievable that this is happening.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28227</th>\n",
       "      <td>surprise</td>\n",
       "      <td>BREAKING NEWS: In a tragic start to Preakness ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27610</th>\n",
       "      <td>surprise</td>\n",
       "      <td>KTLA reporter Steve Kuzj and cameraman Victor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63826</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Is it possible to finish a marathon in under 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50729</th>\n",
       "      <td>surprise</td>\n",
       "      <td>\"To stop boys getting ideas and create a good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>surprise</td>\n",
       "      <td>How high can we build in the future?\\nhttp://9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67278</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Researchers investigating the 2,500-year-old m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15327</th>\n",
       "      <td>surprise</td>\n",
       "      <td>It's one of the most famous ships in history -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19891</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Scientists searching for a solution to ever-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65021</th>\n",
       "      <td>surprise</td>\n",
       "      <td>People here said they had known that a network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6158</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Seriously? SERIOUSLY?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20816</th>\n",
       "      <td>surprise</td>\n",
       "      <td>An Oregon man has been charged with abusive se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36055</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Television footage showed fires, power outages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41758</th>\n",
       "      <td>surprise</td>\n",
       "      <td>A new theory suggests a cataclysmic disaster b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29108</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Stewardesses were prepared to wear headscarves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The standoff is \"between the goodhearted moms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25830</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Her dream was ripped away by a urinary tract i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26021</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Such a sad story. RIP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20967</th>\n",
       "      <td>surprise</td>\n",
       "      <td>A Dutch woman woke up in an unfamiliar locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48513</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Talk about a baaaaaaaaaaaddddd situation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Nope, it's not Star Trek or The Hunger Games.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25199</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The fake taxi rapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50327</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Eerie yet beautiful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28559</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The monument will be moved to another location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25569</th>\n",
       "      <td>surprise</td>\n",
       "      <td>So sad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15007</th>\n",
       "      <td>surprise</td>\n",
       "      <td>BREAKING: Distress signal detected in vicinity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65349</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Will Smith, a former defensive end for the New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56446</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Our video of the day: It might be a featherwei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28133</th>\n",
       "      <td>surprise</td>\n",
       "      <td>San Francisco officials upheld the city's stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33474</th>\n",
       "      <td>surprise</td>\n",
       "      <td>This is a truly shameful response, Donald J. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25904</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The 'Babes in the Wood', aged 10 and nine, wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34004</th>\n",
       "      <td>surprise</td>\n",
       "      <td>There's a solution!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50148</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Naturally, they're marketing this water as Pur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                                                  1\n",
       "27823  surprise                                        Terrifying.\n",
       "27382  surprise                                This is terrifying.\n",
       "28303  surprise                                             Yikes!\n",
       "33513  surprise  For some reason this is a little hard to belie...\n",
       "49643  surprise  Passengers aboard Delta flight 762 experienced...\n",
       "28081  surprise  The 33-year-old officer managed to avoid serio...\n",
       "33611  surprise                                      DEVELOPING...\n",
       "27464  surprise  An industrial-sized barbecue grill broke free ...\n",
       "7140   surprise                                    Oh my dear god.\n",
       "27269  surprise                                             Awful.\n",
       "12968  surprise  Snow in July? Not quite. But this storm produc...\n",
       "28410  surprise  \"I'm just glad that my son wasn't wearing them...\n",
       "35154  surprise                                         What the?!\n",
       "27894  surprise                                 Warn your friends!\n",
       "33714  surprise  Each WNBA team was fined $5,000 and its player...\n",
       "34614  surprise  An alligator dragged a 2-year-old into the wat...\n",
       "34599  surprise  The child’s body was found completely intact “...\n",
       "15072  surprise  \"This could happen at any store that sells tre...\n",
       "47543  surprise                                           Noooooo!\n",
       "25631  surprise        His unborn twin had hair, legs and genitals\n",
       "28101  surprise                 It’s their only way to get home...\n",
       "13434  surprise  Colleen Burns, 35, was hiking along the trails...\n",
       "8174   surprise  “No mother should have to go [through] what I’...\n",
       "7230   surprise  The actor was most famous for appearing as Che...\n",
       "34460  surprise  \"I refuse to ruin the lives of two young men w...\n",
       "12957  surprise  A deer ate the thawed remains of an infected r...\n",
       "46672  surprise     Bid farewell to your childhood. ???? ???? ????\n",
       "34409  surprise               Unbelievable that this is happening.\n",
       "28227  surprise  BREAKING NEWS: In a tragic start to Preakness ...\n",
       "27610  surprise  KTLA reporter Steve Kuzj and cameraman Victor ...\n",
       "...         ...                                                ...\n",
       "63826  surprise  Is it possible to finish a marathon in under 2...\n",
       "50729  surprise  \"To stop boys getting ideas and create a good ...\n",
       "1179   surprise  How high can we build in the future?\\nhttp://9...\n",
       "67278  surprise  Researchers investigating the 2,500-year-old m...\n",
       "15327  surprise  It's one of the most famous ships in history -...\n",
       "19891  surprise  Scientists searching for a solution to ever-re...\n",
       "65021  surprise  People here said they had known that a network...\n",
       "6158   surprise                              Seriously? SERIOUSLY?\n",
       "20816  surprise  An Oregon man has been charged with abusive se...\n",
       "36055  surprise  Television footage showed fires, power outages...\n",
       "41758  surprise  A new theory suggests a cataclysmic disaster b...\n",
       "29108  surprise  Stewardesses were prepared to wear headscarves...\n",
       "28868  surprise  The standoff is \"between the goodhearted moms ...\n",
       "25830  surprise  Her dream was ripped away by a urinary tract i...\n",
       "26021  surprise                             Such a sad story. RIP.\n",
       "20967  surprise  A Dutch woman woke up in an unfamiliar locatio...\n",
       "48513  surprise          Talk about a baaaaaaaaaaaddddd situation.\n",
       "22489  surprise      Nope, it's not Star Trek or The Hunger Games.\n",
       "25199  surprise                               The fake taxi rapist\n",
       "50327  surprise                               Eerie yet beautiful.\n",
       "28559  surprise  The monument will be moved to another location...\n",
       "25569  surprise                                            So sad.\n",
       "15007  surprise  BREAKING: Distress signal detected in vicinity...\n",
       "65349  surprise  Will Smith, a former defensive end for the New...\n",
       "56446  surprise  Our video of the day: It might be a featherwei...\n",
       "28133  surprise  San Francisco officials upheld the city's stri...\n",
       "33474  surprise  This is a truly shameful response, Donald J. T...\n",
       "25904  surprise  The 'Babes in the Wood', aged 10 and nine, wer...\n",
       "34004  surprise                                There's a solution!\n",
       "50148  surprise  Naturally, they're marketing this water as Pur...\n",
       "\n",
       "[1080 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear[0].replace(['shame', 'disgust', 'guilt'], np.nan, inplace=True)\n",
    "isear.dropna(subset=[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "indico_emotions=isear.append(surprise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>joy</td>\n",
       "      <td>After my girlfriend had taken her exam we went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fear</td>\n",
       "      <td>When, for the first time I realized the meanin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anger</td>\n",
       "      <td>When a car is overtaking another and I am forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I recently thought about the hard work it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I pass an examination which I did not thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fear</td>\n",
       "      <td>When one has arranged to meet someone and that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>anger</td>\n",
       "      <td>When one is unjustly accused of something one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When one's studies seem hopelessly difficult a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>joy</td>\n",
       "      <td>Passing an exam I did not expect to pass.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I climbed up a tree to pick apples.  The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I had my children.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fear</td>\n",
       "      <td>When my 2 year old son climbed up and sat on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>anger</td>\n",
       "      <td>When my partner was attacked and lost three te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I see children on T.V from areas devastat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>joy</td>\n",
       "      <td>When my child was born.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fear</td>\n",
       "      <td>It was spring and the ice was melting.  I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>anger</td>\n",
       "      <td>Unjust accusations directed at me and my way o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Failing an examination.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I saw a person I had not seen for a long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>fear</td>\n",
       "      <td>When, as a child, I was nearly knocked down by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I heard on the radio that the football ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I feel lonely, perhaps because I have to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I was accepted for a course on finance an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>fear</td>\n",
       "      <td>A bus drove over my right leg.  The event itse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>anger</td>\n",
       "      <td>At my Summer job, nobody looked after me in pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I was not accepted as a student in financ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63826</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Is it possible to finish a marathon in under 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50729</th>\n",
       "      <td>surprise</td>\n",
       "      <td>\"To stop boys getting ideas and create a good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>surprise</td>\n",
       "      <td>How high can we build in the future?\\nhttp://9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67278</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Researchers investigating the 2,500-year-old m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15327</th>\n",
       "      <td>surprise</td>\n",
       "      <td>It's one of the most famous ships in history -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19891</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Scientists searching for a solution to ever-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65021</th>\n",
       "      <td>surprise</td>\n",
       "      <td>People here said they had known that a network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6158</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Seriously? SERIOUSLY?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20816</th>\n",
       "      <td>surprise</td>\n",
       "      <td>An Oregon man has been charged with abusive se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36055</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Television footage showed fires, power outages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41758</th>\n",
       "      <td>surprise</td>\n",
       "      <td>A new theory suggests a cataclysmic disaster b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29108</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Stewardesses were prepared to wear headscarves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The standoff is \"between the goodhearted moms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25830</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Her dream was ripped away by a urinary tract i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26021</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Such a sad story. RIP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20967</th>\n",
       "      <td>surprise</td>\n",
       "      <td>A Dutch woman woke up in an unfamiliar locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48513</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Talk about a baaaaaaaaaaaddddd situation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Nope, it's not Star Trek or The Hunger Games.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25199</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The fake taxi rapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50327</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Eerie yet beautiful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28559</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The monument will be moved to another location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25569</th>\n",
       "      <td>surprise</td>\n",
       "      <td>So sad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15007</th>\n",
       "      <td>surprise</td>\n",
       "      <td>BREAKING: Distress signal detected in vicinity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65349</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Will Smith, a former defensive end for the New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56446</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Our video of the day: It might be a featherwei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28133</th>\n",
       "      <td>surprise</td>\n",
       "      <td>San Francisco officials upheld the city's stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33474</th>\n",
       "      <td>surprise</td>\n",
       "      <td>This is a truly shameful response, Donald J. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25904</th>\n",
       "      <td>surprise</td>\n",
       "      <td>The 'Babes in the Wood', aged 10 and nine, wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34004</th>\n",
       "      <td>surprise</td>\n",
       "      <td>There's a solution!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50148</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Naturally, they're marketing this water as Pur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                                                  1\n",
       "0           joy  On days when I feel close to my partner and ot...\n",
       "1          fear  Every time I imagine that someone I love or I ...\n",
       "2         anger  When I had been obviously unjustly treated and...\n",
       "3       sadness  When I think about the short time that we live...\n",
       "7           joy  After my girlfriend had taken her exam we went...\n",
       "8          fear  When, for the first time I realized the meanin...\n",
       "9         anger  When a car is overtaking another and I am forc...\n",
       "10      sadness  When I recently thought about the hard work it...\n",
       "14          joy  When I pass an examination which I did not thi...\n",
       "15         fear  When one has arranged to meet someone and that...\n",
       "16        anger  When one is unjustly accused of something one ...\n",
       "17      sadness  When one's studies seem hopelessly difficult a...\n",
       "21          joy          Passing an exam I did not expect to pass.\n",
       "22         fear  When I climbed up a tree to pick apples.  The ...\n",
       "24          joy                            When I had my children.\n",
       "25         fear  When my 2 year old son climbed up and sat on t...\n",
       "26        anger  When my partner was attacked and lost three te...\n",
       "27      sadness  When I see children on T.V from areas devastat...\n",
       "31          joy                            When my child was born.\n",
       "32         fear  It was spring and the ice was melting.  I was ...\n",
       "33        anger  Unjust accusations directed at me and my way o...\n",
       "34      sadness                            Failing an examination.\n",
       "38          joy  When I saw a person I had not seen for a long ...\n",
       "39         fear  When, as a child, I was nearly knocked down by...\n",
       "40        anger  When I heard on the radio that the football ma...\n",
       "41      sadness  When I feel lonely, perhaps because I have to ...\n",
       "45          joy  When I was accepted for a course on finance an...\n",
       "46         fear  A bus drove over my right leg.  The event itse...\n",
       "47        anger  At my Summer job, nobody looked after me in pa...\n",
       "48      sadness  When I was not accepted as a student in financ...\n",
       "...         ...                                                ...\n",
       "63826  surprise  Is it possible to finish a marathon in under 2...\n",
       "50729  surprise  \"To stop boys getting ideas and create a good ...\n",
       "1179   surprise  How high can we build in the future?\\nhttp://9...\n",
       "67278  surprise  Researchers investigating the 2,500-year-old m...\n",
       "15327  surprise  It's one of the most famous ships in history -...\n",
       "19891  surprise  Scientists searching for a solution to ever-re...\n",
       "65021  surprise  People here said they had known that a network...\n",
       "6158   surprise                              Seriously? SERIOUSLY?\n",
       "20816  surprise  An Oregon man has been charged with abusive se...\n",
       "36055  surprise  Television footage showed fires, power outages...\n",
       "41758  surprise  A new theory suggests a cataclysmic disaster b...\n",
       "29108  surprise  Stewardesses were prepared to wear headscarves...\n",
       "28868  surprise  The standoff is \"between the goodhearted moms ...\n",
       "25830  surprise  Her dream was ripped away by a urinary tract i...\n",
       "26021  surprise                             Such a sad story. RIP.\n",
       "20967  surprise  A Dutch woman woke up in an unfamiliar locatio...\n",
       "48513  surprise          Talk about a baaaaaaaaaaaddddd situation.\n",
       "22489  surprise      Nope, it's not Star Trek or The Hunger Games.\n",
       "25199  surprise                               The fake taxi rapist\n",
       "50327  surprise                               Eerie yet beautiful.\n",
       "28559  surprise  The monument will be moved to another location...\n",
       "25569  surprise                                            So sad.\n",
       "15007  surprise  BREAKING: Distress signal detected in vicinity...\n",
       "65349  surprise  Will Smith, a former defensive end for the New...\n",
       "56446  surprise  Our video of the day: It might be a featherwei...\n",
       "28133  surprise  San Francisco officials upheld the city's stri...\n",
       "33474  surprise  This is a truly shameful response, Donald J. T...\n",
       "25904  surprise  The 'Babes in the Wood', aged 10 and nine, wer...\n",
       "34004  surprise                                There's a solution!\n",
       "50148  surprise  Naturally, they're marketing this water as Pur...\n",
       "\n",
       "[5409 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indico_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Indico validation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import indicoio\n",
    "indicoio.config.api_key='397e82142cfdfe487c61dca634280b49'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Naturally, they're marketing this water as Purple Rain.\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indico_emotions[1].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_1=indicoio.emotion(indico_emotions[1].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23341253399848938"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_1['surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    On days when I feel close to my partner and ot...\n",
       "1    Every time I imagine that someone I love or I ...\n",
       "2    When I had been obviously unjustly treated and...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indico_emotions[1].iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_mult=indicoio.emotion(list(indico_emotions[1].iloc[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'anger': 0.11947168409824371,\n",
       "  'fear': 0.29070523381233215,\n",
       "  'joy': 0.22359329462051392,\n",
       "  'sadness': 0.33788686990737915,\n",
       "  'surprise': 0.02834296226501465},\n",
       " {'anger': 0.3306489884853363,\n",
       "  'fear': 0.10204938054084778,\n",
       "  'joy': 0.035173580050468445,\n",
       "  'sadness': 0.5207858085632324,\n",
       "  'surprise': 0.011342236772179604},\n",
       " {'anger': 0.271008163690567,\n",
       "  'fear': 0.07341042160987854,\n",
       "  'joy': 0.03585120290517807,\n",
       "  'sadness': 0.5347471237182617,\n",
       "  'surprise': 0.08498305082321167}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02834296226501465 0\n",
      "0.011342236772179604 1\n",
      "0.08498305082321167 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.02834296226501465, 1: 0.011342236772179604, 2: 0.08498305082321167}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_dict={}\n",
    "for index, pred in enumerate(test_pred_mult):\n",
    "    print(pred['surprise'], index)\n",
    "    surprise_dict[index]=pred['surprise']\n",
    "surprise_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.011342236772179604), (0, 0.02834296226501465), (2, 0.08498305082321167)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "sorted_surprise_dict=sorted(surprise_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_surprise_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On days when I feel close to my partner and other friends.   \n",
      "When I feel at peace with myself and also experience a close  \n",
      "contact with people whom I regard greatly.\n",
      "Every time I imagine that someone I love or I could contact a  \n",
      "serious illness, even death.\n",
      "When I had been obviously unjustly treated and had no possibility  \n",
      "of elucidating this.\n"
     ]
    }
   ],
   "source": [
    "for index in surprise_dict:\n",
    "    print(indico_emotions[1].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluating many facebook entries in the message column</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise=fb_reactions.iloc[:4000]\n",
    "surprise_X=surprise['message']\n",
    "len(surprise_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Do NOT run this next step again, it counts against the quota</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Indico_surprise_predictions=indicoio.emotion(list(surprise_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(Indico_surprise_predictions, open('Indico_surprise_predictions.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_surprise(indico_list):\n",
    "    surprise_dict={}\n",
    "    for index, pred in enumerate(indico_list):\n",
    "        surprise_dict[index]=pred['surprise']\n",
    "    sorted_surprise_dict=sorted(surprise_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_surprise_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_surprise=sort_surprise(Indico_surprise_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3683, 0.9539512395858765),\n",
       " (3883, 0.9527742862701416),\n",
       " (829, 0.949898898601532),\n",
       " (447, 0.9370132684707642),\n",
       " (2890, 0.9296550750732422),\n",
       " (2145, 0.9246888756752014),\n",
       " (3447, 0.9224801063537598),\n",
       " (2809, 0.9215435981750488),\n",
       " (2163, 0.9202695488929749),\n",
       " (151, 0.9162212610244751),\n",
       " (1989, 0.9157223701477051),\n",
       " (3298, 0.9139634966850281),\n",
       " (1070, 0.901566743850708),\n",
       " (800, 0.897817850112915),\n",
       " (749, 0.8887519836425781),\n",
       " (1754, 0.887529730796814),\n",
       " (3439, 0.8798812031745911),\n",
       " (548, 0.8695153594017029),\n",
       " (1268, 0.8664361238479614),\n",
       " (2858, 0.8638198375701904),\n",
       " (1512, 0.8535745143890381),\n",
       " (3850, 0.8436046838760376),\n",
       " (861, 0.8278483152389526),\n",
       " (1348, 0.8221002817153931),\n",
       " (1773, 0.8197835683822632),\n",
       " (2017, 0.8197430372238159),\n",
       " (1732, 0.8158684968948364),\n",
       " (3125, 0.811977744102478),\n",
       " (1006, 0.8094233274459839),\n",
       " (394, 0.808263897895813),\n",
       " (1647, 0.8046984076499939),\n",
       " (2420, 0.8036953210830688),\n",
       " (3146, 0.7918825149536133),\n",
       " (103, 0.7886497378349304),\n",
       " (2585, 0.7864409685134888),\n",
       " (3996, 0.7849421501159668),\n",
       " (1530, 0.7834092378616333),\n",
       " (1691, 0.7792932391166687),\n",
       " (1513, 0.7707408666610718),\n",
       " (572, 0.7673758864402771),\n",
       " (2938, 0.7626558542251587),\n",
       " (1722, 0.7566217184066772),\n",
       " (2824, 0.756122887134552),\n",
       " (887, 0.7557228803634644),\n",
       " (192, 0.7500152587890625),\n",
       " (786, 0.7499655485153198),\n",
       " (2576, 0.7498582601547241),\n",
       " (1895, 0.7461179494857788),\n",
       " (2942, 0.7418396472930908),\n",
       " (3440, 0.7418286800384521),\n",
       " (3849, 0.7362621426582336),\n",
       " (3591, 0.728298544883728),\n",
       " (2207, 0.7264888286590576),\n",
       " (1606, 0.725237250328064),\n",
       " (1096, 0.7176775932312012),\n",
       " (2767, 0.7149225473403931),\n",
       " (516, 0.7125385403633118),\n",
       " (2066, 0.7080731391906738),\n",
       " (1586, 0.7059670686721802),\n",
       " (3035, 0.7029675841331482),\n",
       " (3782, 0.7019439935684204),\n",
       " (3777, 0.6969588994979858),\n",
       " (1262, 0.6956998705863953),\n",
       " (1036, 0.6948436498641968),\n",
       " (1832, 0.6914515495300293),\n",
       " (3602, 0.6896507740020752),\n",
       " (273, 0.6892974376678467),\n",
       " (2030, 0.6878498792648315),\n",
       " (2739, 0.6860780119895935),\n",
       " (3179, 0.6845444440841675),\n",
       " (704, 0.6835403442382812),\n",
       " (1323, 0.6788219213485718),\n",
       " (3097, 0.6780945658683777),\n",
       " (3232, 0.6772827506065369),\n",
       " (241, 0.6770901083946228),\n",
       " (285, 0.6763696074485779),\n",
       " (2802, 0.6743148565292358),\n",
       " (2896, 0.6688090562820435),\n",
       " (1626, 0.6683692336082458),\n",
       " (1271, 0.6611875295639038),\n",
       " (3356, 0.6607730388641357),\n",
       " (2338, 0.6545461416244507),\n",
       " (3383, 0.6533165574073792),\n",
       " (264, 0.6489347219467163),\n",
       " (2154, 0.641842246055603),\n",
       " (530, 0.6357525587081909),\n",
       " (1280, 0.6321629285812378),\n",
       " (3055, 0.6289348602294922),\n",
       " (2490, 0.6277894377708435),\n",
       " (3122, 0.6269257068634033),\n",
       " (2453, 0.6264384984970093),\n",
       " (2085, 0.6213452816009521),\n",
       " (2074, 0.6211046576499939),\n",
       " (3071, 0.6204652190208435),\n",
       " (2184, 0.6200705170631409),\n",
       " (3056, 0.6161024570465088),\n",
       " (370, 0.6160140037536621),\n",
       " (567, 0.6158285140991211),\n",
       " (3715, 0.6135971546173096),\n",
       " (2758, 0.608470618724823),\n",
       " (639, 0.6080875396728516),\n",
       " (3091, 0.6051163673400879),\n",
       " (2140, 0.6023010015487671),\n",
       " (3413, 0.6011525392532349),\n",
       " (2855, 0.5974069833755493),\n",
       " (3369, 0.5965532064437866),\n",
       " (1284, 0.5871944427490234),\n",
       " (1328, 0.5791987180709839),\n",
       " (3067, 0.5780333280563354),\n",
       " (3101, 0.5779296159744263),\n",
       " (2320, 0.5764357447624207),\n",
       " (3590, 0.5758888721466064),\n",
       " (874, 0.5754921436309814),\n",
       " (628, 0.5715537667274475),\n",
       " (1413, 0.5674400329589844),\n",
       " (3684, 0.5659320950508118),\n",
       " (3537, 0.5654478669166565),\n",
       " (2692, 0.5647611021995544),\n",
       " (226, 0.5643975734710693),\n",
       " (634, 0.5632918477058411),\n",
       " (3713, 0.5616342425346375),\n",
       " (3147, 0.5597267746925354),\n",
       " (1215, 0.5580368041992188),\n",
       " (2107, 0.5573498010635376),\n",
       " (2883, 0.5573203563690186),\n",
       " (2340, 0.5549601316452026),\n",
       " (1082, 0.5548690557479858),\n",
       " (13, 0.5528103113174438),\n",
       " (3034, 0.5526117086410522),\n",
       " (710, 0.5524241328239441),\n",
       " (1646, 0.5511788129806519),\n",
       " (1884, 0.5507338047027588),\n",
       " (1802, 0.5498151183128357),\n",
       " (2456, 0.5484938621520996),\n",
       " (2669, 0.5472447872161865),\n",
       " (3415, 0.5426992774009705),\n",
       " (2438, 0.5413762331008911),\n",
       " (1420, 0.5411970019340515),\n",
       " (3944, 0.5371140241622925),\n",
       " (3524, 0.5369701385498047),\n",
       " (3965, 0.5353381633758545),\n",
       " (890, 0.5342118740081787),\n",
       " (3004, 0.532106876373291),\n",
       " (3105, 0.5318514108657837),\n",
       " (2083, 0.5242485404014587),\n",
       " (3992, 0.5225255489349365),\n",
       " (1453, 0.5211979150772095),\n",
       " (1569, 0.5196333527565002),\n",
       " (2854, 0.5165546536445618),\n",
       " (761, 0.5162725448608398),\n",
       " (2294, 0.514093816280365),\n",
       " (3658, 0.5127812623977661),\n",
       " (2160, 0.5115731358528137),\n",
       " (798, 0.5111028552055359),\n",
       " (2776, 0.49878740310668945),\n",
       " (664, 0.4978370666503906),\n",
       " (1399, 0.49441957473754883),\n",
       " (3978, 0.4929388165473938),\n",
       " (647, 0.4927877187728882),\n",
       " (1668, 0.48839807510375977),\n",
       " (2277, 0.4880051612854004),\n",
       " (2024, 0.4864565134048462),\n",
       " (1196, 0.48507779836654663),\n",
       " (3174, 0.4842117130756378),\n",
       " (999, 0.4827393889427185),\n",
       " (3660, 0.4803796112537384),\n",
       " (3364, 0.4791892170906067),\n",
       " (2703, 0.47575217485427856),\n",
       " (3881, 0.47331735491752625),\n",
       " (3410, 0.4729739725589752),\n",
       " (560, 0.472384512424469),\n",
       " (539, 0.47150060534477234),\n",
       " (1613, 0.47117018699645996),\n",
       " (2052, 0.47111791372299194),\n",
       " (2127, 0.4688704013824463),\n",
       " (2659, 0.4667859673500061),\n",
       " (2782, 0.4662914574146271),\n",
       " (3075, 0.4654635190963745),\n",
       " (3597, 0.4641178548336029),\n",
       " (1259, 0.4621790647506714),\n",
       " (1797, 0.46015381813049316),\n",
       " (2599, 0.4590962529182434),\n",
       " (2878, 0.4579334259033203),\n",
       " (2699, 0.454700231552124),\n",
       " (1293, 0.4543749690055847),\n",
       " (733, 0.4532191753387451),\n",
       " (1644, 0.4526705741882324),\n",
       " (272, 0.45257556438446045),\n",
       " (3800, 0.44934821128845215),\n",
       " (3481, 0.4480440020561218),\n",
       " (2528, 0.44791561365127563),\n",
       " (3162, 0.44732505083084106),\n",
       " (821, 0.44729337096214294),\n",
       " (1313, 0.44716575741767883),\n",
       " (148, 0.4464128315448761),\n",
       " (3957, 0.44491153955459595),\n",
       " (862, 0.44222983717918396),\n",
       " (1315, 0.44123637676239014),\n",
       " (3616, 0.44084444642066956),\n",
       " (2673, 0.4406173825263977),\n",
       " (2255, 0.43996334075927734),\n",
       " (2442, 0.43992945551872253),\n",
       " (526, 0.4390034079551697),\n",
       " (2032, 0.436635285615921),\n",
       " (165, 0.43612363934516907),\n",
       " (751, 0.4352225065231323),\n",
       " (2352, 0.43305325508117676),\n",
       " (1904, 0.43241825699806213),\n",
       " (3319, 0.43142932653427124),\n",
       " (881, 0.4305722117424011),\n",
       " (544, 0.4288754463195801),\n",
       " (2494, 0.42807984352111816),\n",
       " (622, 0.4269675016403198),\n",
       " (2973, 0.42644262313842773),\n",
       " (564, 0.42528796195983887),\n",
       " (3362, 0.42520782351493835),\n",
       " (3788, 0.4238157570362091),\n",
       " (1379, 0.4227464199066162),\n",
       " (1921, 0.4227268397808075),\n",
       " (385, 0.4217231571674347),\n",
       " (2305, 0.421054482460022),\n",
       " (3780, 0.42003563046455383),\n",
       " (1454, 0.4194789528846741),\n",
       " (1455, 0.41886210441589355),\n",
       " (3531, 0.41694778203964233),\n",
       " (2801, 0.41684556007385254),\n",
       " (3693, 0.4161037504673004),\n",
       " (256, 0.41554874181747437),\n",
       " (2822, 0.41367262601852417),\n",
       " (3346, 0.41329655051231384),\n",
       " (889, 0.4130614697933197),\n",
       " (2601, 0.4129021167755127),\n",
       " (3506, 0.41181468963623047),\n",
       " (2431, 0.4115191698074341),\n",
       " (1841, 0.41140586137771606),\n",
       " (469, 0.4110809564590454),\n",
       " (2488, 0.41104114055633545),\n",
       " (2229, 0.4093852639198303),\n",
       " (2261, 0.4093806743621826),\n",
       " (3486, 0.40916240215301514),\n",
       " (1003, 0.40861237049102783),\n",
       " (39, 0.4084694981575012),\n",
       " (2701, 0.4069797396659851),\n",
       " (3335, 0.40662261843681335),\n",
       " (416, 0.4066184461116791),\n",
       " (906, 0.4060567021369934),\n",
       " (487, 0.40586113929748535),\n",
       " (883, 0.40569132566452026),\n",
       " (3976, 0.4050067067146301),\n",
       " (421, 0.40407630801200867),\n",
       " (3492, 0.40353935956954956),\n",
       " (1023, 0.40148329734802246),\n",
       " (1877, 0.40147268772125244),\n",
       " (3005, 0.40055108070373535),\n",
       " (2175, 0.4001080393791199),\n",
       " (2778, 0.3991221487522125),\n",
       " (2978, 0.3987691402435303),\n",
       " (1341, 0.3987211585044861),\n",
       " (2788, 0.3984144628047943),\n",
       " (2003, 0.39800703525543213),\n",
       " (1776, 0.39788198471069336),\n",
       " (2688, 0.3972257673740387),\n",
       " (3211, 0.3969632387161255),\n",
       " (1539, 0.3956940770149231),\n",
       " (2116, 0.39543694257736206),\n",
       " (1375, 0.39536941051483154),\n",
       " (3831, 0.3949659764766693),\n",
       " (2075, 0.39456111192703247),\n",
       " (838, 0.3942706286907196),\n",
       " (3271, 0.3941180109977722),\n",
       " (3163, 0.39358872175216675),\n",
       " (2913, 0.3933582603931427),\n",
       " (2156, 0.39317893981933594),\n",
       " (3919, 0.3914426565170288),\n",
       " (696, 0.39098691940307617),\n",
       " (849, 0.3906329274177551),\n",
       " (177, 0.38993072509765625),\n",
       " (1793, 0.38808923959732056),\n",
       " (853, 0.38804319500923157),\n",
       " (3085, 0.3871527314186096),\n",
       " (836, 0.38700515031814575),\n",
       " (942, 0.3866040110588074),\n",
       " (1135, 0.3866040110588074),\n",
       " (1517, 0.3851558268070221),\n",
       " (2479, 0.38475412130355835),\n",
       " (843, 0.38420870900154114),\n",
       " (1472, 0.38414451479911804),\n",
       " (2227, 0.38375866413116455),\n",
       " (679, 0.3836725354194641),\n",
       " (2806, 0.38319799304008484),\n",
       " (2640, 0.3831276297569275),\n",
       " (3012, 0.382705420255661),\n",
       " (2398, 0.38246530294418335),\n",
       " (1855, 0.38169562816619873),\n",
       " (2684, 0.38151347637176514),\n",
       " (3604, 0.3799746334552765),\n",
       " (927, 0.3787726163864136),\n",
       " (1763, 0.3777272403240204),\n",
       " (1055, 0.3774121403694153),\n",
       " (3425, 0.3770756125450134),\n",
       " (2976, 0.3765456974506378),\n",
       " (3728, 0.3765132427215576),\n",
       " (1630, 0.3750544786453247),\n",
       " (931, 0.3745741546154022),\n",
       " (3737, 0.37323397397994995),\n",
       " (1654, 0.3718761205673218),\n",
       " (3016, 0.3708936274051666),\n",
       " (1509, 0.37027353048324585),\n",
       " (2001, 0.3698061406612396),\n",
       " (3305, 0.36952120065689087),\n",
       " (2946, 0.3690947890281677),\n",
       " (1150, 0.36839422583580017),\n",
       " (662, 0.36817944049835205),\n",
       " (2299, 0.3675001859664917),\n",
       " (961, 0.367178738117218),\n",
       " (855, 0.36686310172080994),\n",
       " (2478, 0.3648177981376648),\n",
       " (2369, 0.3646719455718994),\n",
       " (3568, 0.3646324574947357),\n",
       " (1121, 0.3643595278263092),\n",
       " (3802, 0.364174485206604),\n",
       " (99, 0.3638477325439453),\n",
       " (2041, 0.3634878098964691),\n",
       " (3279, 0.36309918761253357),\n",
       " (2355, 0.3630604147911072),\n",
       " (3835, 0.3629316985607147),\n",
       " (2096, 0.3626558780670166),\n",
       " (2849, 0.3624790906906128),\n",
       " (2744, 0.362138956785202),\n",
       " (1521, 0.3606952428817749),\n",
       " (980, 0.36041831970214844),\n",
       " (896, 0.3597778081893921),\n",
       " (685, 0.3594666123390198),\n",
       " (2284, 0.3594522476196289),\n",
       " (3775, 0.3589380383491516),\n",
       " (1187, 0.3587842881679535),\n",
       " (111, 0.3581158518791199),\n",
       " (1587, 0.35621312260627747),\n",
       " (2469, 0.3559219539165497),\n",
       " (2190, 0.35583820939064026),\n",
       " (2004, 0.35389065742492676),\n",
       " (3119, 0.3532417118549347),\n",
       " (1605, 0.35277360677719116),\n",
       " (229, 0.35255852341651917),\n",
       " (895, 0.35255852341651917),\n",
       " (1093, 0.35255852341651917),\n",
       " (1416, 0.3521067500114441),\n",
       " (311, 0.3521062135696411),\n",
       " (459, 0.35151398181915283),\n",
       " (2468, 0.3502436876296997),\n",
       " (96, 0.3497498333454132),\n",
       " (3559, 0.34923818707466125),\n",
       " (2359, 0.34818562865257263),\n",
       " (2986, 0.3480384349822998),\n",
       " (3618, 0.3476945757865906),\n",
       " (3230, 0.347682923078537),\n",
       " (1330, 0.34586116671562195),\n",
       " (1445, 0.34574148058891296),\n",
       " (3399, 0.3454345762729645),\n",
       " (1532, 0.34507104754447937),\n",
       " (2540, 0.34457409381866455),\n",
       " (1019, 0.3421906530857086),\n",
       " (2250, 0.34189265966415405),\n",
       " (2062, 0.3416859805583954),\n",
       " (1123, 0.340844988822937),\n",
       " (3494, 0.3404013216495514),\n",
       " (3879, 0.3401225209236145),\n",
       " (3134, 0.3400684893131256),\n",
       " (1110, 0.33985936641693115),\n",
       " (2371, 0.33922693133354187),\n",
       " (1427, 0.33888471126556396),\n",
       " (2397, 0.33848702907562256),\n",
       " (30, 0.33775922656059265),\n",
       " (1485, 0.33755993843078613),\n",
       " (3681, 0.3375456631183624),\n",
       " (3082, 0.3373045325279236),\n",
       " (3773, 0.33644795417785645),\n",
       " (2111, 0.3353213965892792),\n",
       " (3461, 0.33386534452438354),\n",
       " (3700, 0.33332422375679016),\n",
       " (1331, 0.3323189318180084),\n",
       " (2549, 0.3319617509841919),\n",
       " (2663, 0.3318999707698822),\n",
       " (1380, 0.3313656747341156),\n",
       " (2546, 0.3293386399745941),\n",
       " (3354, 0.3292721211910248),\n",
       " (3208, 0.3291938602924347),\n",
       " (1719, 0.32914236187934875),\n",
       " (930, 0.32824572920799255),\n",
       " (200, 0.32799142599105835),\n",
       " (1575, 0.3270933926105499),\n",
       " (3482, 0.3257937729358673),\n",
       " (3621, 0.32537397742271423),\n",
       " (1939, 0.3248876631259918),\n",
       " (1965, 0.32219570875167847),\n",
       " (11, 0.320464551448822),\n",
       " (342, 0.32031962275505066),\n",
       " (1376, 0.3200361132621765),\n",
       " (2541, 0.31916409730911255),\n",
       " (2996, 0.31900930404663086),\n",
       " (1312, 0.31874191761016846),\n",
       " (1211, 0.31861579418182373),\n",
       " (553, 0.31802037358283997),\n",
       " (3434, 0.31788313388824463),\n",
       " (409, 0.3178093433380127),\n",
       " (2844, 0.31652742624282837),\n",
       " (750, 0.31603291630744934),\n",
       " (1931, 0.31506508588790894),\n",
       " (2087, 0.31434762477874756),\n",
       " (2547, 0.3138909339904785),\n",
       " (1671, 0.3138778507709503),\n",
       " (102, 0.31257855892181396),\n",
       " (1467, 0.3120661973953247),\n",
       " (2498, 0.3118119239807129),\n",
       " (3634, 0.3117539882659912),\n",
       " (3223, 0.3116926848888397),\n",
       " (3872, 0.30963361263275146),\n",
       " (2049, 0.3077661693096161),\n",
       " (2864, 0.30758270621299744),\n",
       " (2587, 0.30744025111198425),\n",
       " (2916, 0.30738914012908936),\n",
       " (3010, 0.3072717487812042),\n",
       " (3790, 0.3072300851345062),\n",
       " (617, 0.30678075551986694),\n",
       " (3738, 0.30617964267730713),\n",
       " (1386, 0.30592605471611023),\n",
       " (1430, 0.30588677525520325),\n",
       " (2422, 0.3050526976585388),\n",
       " (3615, 0.305001437664032),\n",
       " (3421, 0.3049098551273346),\n",
       " (2581, 0.3034414052963257),\n",
       " (3925, 0.3022941052913666),\n",
       " (3109, 0.30187204480171204),\n",
       " (213, 0.30182206630706787),\n",
       " (3983, 0.30175599455833435),\n",
       " (2172, 0.30020320415496826),\n",
       " (2502, 0.30010369420051575),\n",
       " (1973, 0.2998601496219635),\n",
       " (2682, 0.29947781562805176),\n",
       " (129, 0.298440158367157),\n",
       " (62, 0.29838213324546814),\n",
       " (2124, 0.29707780480384827),\n",
       " (3558, 0.29641133546829224),\n",
       " (1403, 0.2962872087955475),\n",
       " (481, 0.2955213785171509),\n",
       " (3333, 0.2951567769050598),\n",
       " (3426, 0.2943405508995056),\n",
       " (2893, 0.29246872663497925),\n",
       " (2621, 0.29229575395584106),\n",
       " (1220, 0.2916554808616638),\n",
       " (2204, 0.29116666316986084),\n",
       " (3675, 0.2911625802516937),\n",
       " (3398, 0.2904926836490631),\n",
       " (3270, 0.2896234393119812),\n",
       " (1842, 0.28846338391304016),\n",
       " (74, 0.2882283926010132),\n",
       " (2573, 0.2879635989665985),\n",
       " (678, 0.28781312704086304),\n",
       " (2637, 0.28769034147262573),\n",
       " (2911, 0.28743112087249756),\n",
       " (352, 0.2870643138885498),\n",
       " (3198, 0.2869901657104492),\n",
       " (3191, 0.2865247130393982),\n",
       " (3345, 0.2857098877429962),\n",
       " (3750, 0.285620778799057),\n",
       " (1326, 0.28473928570747375),\n",
       " (474, 0.28467875719070435),\n",
       " (2159, 0.2843537926673889),\n",
       " (2520, 0.28419309854507446),\n",
       " (2029, 0.28396838903427124),\n",
       " (2451, 0.2837388813495636),\n",
       " (1958, 0.28369593620300293),\n",
       " (2622, 0.28327327966690063),\n",
       " (3623, 0.2822340726852417),\n",
       " (2417, 0.28222599625587463),\n",
       " (1049, 0.28155526518821716),\n",
       " (1615, 0.2810133099555969),\n",
       " (2222, 0.28082075715065),\n",
       " (687, 0.2806878089904785),\n",
       " (356, 0.2802111506462097),\n",
       " (3292, 0.2801598310470581),\n",
       " (987, 0.2799553871154785),\n",
       " (3318, 0.2797749638557434),\n",
       " (3365, 0.27965307235717773),\n",
       " (2671, 0.2789098918437958),\n",
       " (2649, 0.2788345515727997),\n",
       " (2034, 0.278778612613678),\n",
       " (1004, 0.27866506576538086),\n",
       " (2035, 0.2785761058330536),\n",
       " (3237, 0.27784010767936707),\n",
       " (1550, 0.2773084342479706),\n",
       " (1025, 0.2770382761955261),\n",
       " (633, 0.27607211470603943),\n",
       " (1498, 0.27572429180145264),\n",
       " (782, 0.2757066488265991),\n",
       " (2212, 0.27560070157051086),\n",
       " (3341, 0.2754546105861664),\n",
       " (2306, 0.2751534581184387),\n",
       " (2677, 0.27462902665138245),\n",
       " (91, 0.27418726682662964),\n",
       " (3290, 0.2739034593105316),\n",
       " (48, 0.27311792969703674),\n",
       " (768, 0.2727974057197571),\n",
       " (253, 0.27234745025634766),\n",
       " (2984, 0.2719661593437195),\n",
       " (727, 0.2717928886413574),\n",
       " (3514, 0.27176037430763245),\n",
       " (1505, 0.2712273597717285),\n",
       " (1774, 0.2711520493030548),\n",
       " (349, 0.2708459198474884),\n",
       " (953, 0.2707862854003906),\n",
       " (2589, 0.27056455612182617),\n",
       " (1449, 0.26933276653289795),\n",
       " (521, 0.269275426864624),\n",
       " (2726, 0.26894107460975647),\n",
       " (2990, 0.26809072494506836),\n",
       " (943, 0.2680196166038513),\n",
       " (2256, 0.26762545108795166),\n",
       " (1360, 0.2675042450428009),\n",
       " (2716, 0.2674659788608551),\n",
       " (366, 0.2674393653869629),\n",
       " (655, 0.2674393653869629),\n",
       " (659, 0.2674393653869629),\n",
       " (1483, 0.2664840817451477),\n",
       " (933, 0.2660446763038635),\n",
       " (411, 0.26515549421310425),\n",
       " (3718, 0.26508602499961853),\n",
       " (1589, 0.26494765281677246),\n",
       " (275, 0.26483482122421265),\n",
       " (3808, 0.2645435631275177),\n",
       " (1192, 0.2637741267681122),\n",
       " (2481, 0.2636783719062805),\n",
       " (2061, 0.2636413872241974),\n",
       " (1084, 0.2632198929786682),\n",
       " (3526, 0.26313310861587524),\n",
       " (3941, 0.26242077350616455),\n",
       " (1173, 0.26193857192993164),\n",
       " (3844, 0.26150935888290405),\n",
       " (1060, 0.26150786876678467),\n",
       " (3267, 0.2609104812145233),\n",
       " (1735, 0.26067638397216797),\n",
       " (3441, 0.2606542706489563),\n",
       " (236, 0.2601310610771179),\n",
       " (1464, 0.26004093885421753),\n",
       " (1069, 0.25992822647094727),\n",
       " (672, 0.2592594623565674),\n",
       " (3766, 0.2592189311981201),\n",
       " (1695, 0.2590630054473877),\n",
       " (3062, 0.25893089175224304),\n",
       " (2584, 0.25873029232025146),\n",
       " (1356, 0.25814419984817505),\n",
       " (3116, 0.25765275955200195),\n",
       " (2455, 0.25722435116767883),\n",
       " (1893, 0.25714942812919617),\n",
       " (1426, 0.25706028938293457),\n",
       " (3509, 0.2569000720977783),\n",
       " (1886, 0.2564515769481659),\n",
       " (1089, 0.2563985586166382),\n",
       " (2508, 0.2554837465286255),\n",
       " (1588, 0.25514593720436096),\n",
       " (2482, 0.25465965270996094),\n",
       " (3159, 0.2544972002506256),\n",
       " (3367, 0.25446733832359314),\n",
       " (3132, 0.25408485531806946),\n",
       " (2607, 0.2539248764514923),\n",
       " (2376, 0.2538304626941681),\n",
       " (75, 0.2535874843597412),\n",
       " (313, 0.2535874843597412),\n",
       " (602, 0.2535874843597412),\n",
       " (1081, 0.2535874843597412),\n",
       " (2393, 0.2535874843597412),\n",
       " (2424, 0.2535874843597412),\n",
       " (3652, 0.2518194913864136),\n",
       " (831, 0.2518148124217987),\n",
       " (2719, 0.25172334909439087),\n",
       " (3138, 0.25168389081954956),\n",
       " (866, 0.25130027532577515),\n",
       " (2208, 0.2512374222278595),\n",
       " (1648, 0.2508852183818817),\n",
       " (2013, 0.2508668899536133),\n",
       " (3382, 0.2501348853111267),\n",
       " (1817, 0.25001364946365356),\n",
       " (2348, 0.2499937117099762),\n",
       " (1883, 0.2499869465827942),\n",
       " (2755, 0.2497839629650116),\n",
       " (3950, 0.24956969916820526),\n",
       " (2136, 0.24919697642326355),\n",
       " (484, 0.24916356801986694),\n",
       " (1040, 0.24908605217933655),\n",
       " (2967, 0.24904105067253113),\n",
       " (1839, 0.24870653450489044),\n",
       " (3846, 0.24828317761421204),\n",
       " (3269, 0.24792136251926422),\n",
       " (3359, 0.24718521535396576),\n",
       " (2619, 0.2458137571811676),\n",
       " (3108, 0.2457445114850998),\n",
       " (3890, 0.24524830281734467),\n",
       " (2835, 0.2449996918439865),\n",
       " (3343, 0.2448684126138687),\n",
       " (1974, 0.24424636363983154),\n",
       " (3702, 0.24396878480911255),\n",
       " (2139, 0.24357236921787262),\n",
       " (3259, 0.24353143572807312),\n",
       " (308, 0.2433285117149353),\n",
       " (537, 0.24305561184883118),\n",
       " (3311, 0.241876482963562),\n",
       " (3058, 0.24174898862838745),\n",
       " (3543, 0.24151082336902618),\n",
       " (456, 0.241313636302948),\n",
       " (413, 0.24033653736114502),\n",
       " (3454, 0.2400859296321869),\n",
       " (950, 0.24007722735404968),\n",
       " (3751, 0.24005399644374847),\n",
       " (1640, 0.23931878805160522),\n",
       " (3164, 0.23930281400680542),\n",
       " (3898, 0.23926027119159698),\n",
       " (690, 0.23859752714633942),\n",
       " (1902, 0.23805159330368042),\n",
       " (3154, 0.2370111644268036),\n",
       " (2817, 0.23687607049942017),\n",
       " (2193, 0.2367793619632721),\n",
       " (724, 0.23612546920776367),\n",
       " (1470, 0.2359272837638855),\n",
       " (1738, 0.23551973700523376),\n",
       " (533, 0.23548948764801025),\n",
       " (1638, 0.23520486056804657),\n",
       " (2093, 0.23495715856552124),\n",
       " (2271, 0.23483861982822418),\n",
       " (2225, 0.23464012145996094),\n",
       " (2210, 0.2338702380657196),\n",
       " (3582, 0.23314684629440308),\n",
       " (3430, 0.23308895528316498),\n",
       " (2343, 0.23301896452903748),\n",
       " (1397, 0.23284517228603363),\n",
       " (3257, 0.2328421026468277),\n",
       " (1034, 0.23255126178264618),\n",
       " (3397, 0.23245282471179962),\n",
       " (3572, 0.23244012892246246),\n",
       " (857, 0.23238015174865723),\n",
       " (1736, 0.23180469870567322),\n",
       " (1705, 0.2316213995218277),\n",
       " (1257, 0.2315102368593216),\n",
       " (3262, 0.23132647573947906),\n",
       " (575, 0.23113298416137695),\n",
       " (2535, 0.23112785816192627),\n",
       " (1800, 0.23087556660175323),\n",
       " (621, 0.23044556379318237),\n",
       " (2943, 0.2302333414554596),\n",
       " (1167, 0.23013833165168762),\n",
       " (3732, 0.22994273900985718),\n",
       " (3073, 0.22986051440238953),\n",
       " (2290, 0.22979426383972168),\n",
       " (160, 0.22952690720558167),\n",
       " (3901, 0.22951418161392212),\n",
       " (3547, 0.22916960716247559),\n",
       " (1775, 0.22901754081249237),\n",
       " (159, 0.22901341319084167),\n",
       " (1663, 0.22848355770111084),\n",
       " (3842, 0.2281816452741623),\n",
       " (1172, 0.2279939204454422),\n",
       " (2203, 0.22775810956954956),\n",
       " (1900, 0.22713983058929443),\n",
       " (2177, 0.22669294476509094),\n",
       " (2728, 0.22651731967926025),\n",
       " (3975, 0.22639212012290955),\n",
       " (703, 0.2257600873708725),\n",
       " (2596, 0.22558799386024475),\n",
       " (418, 0.2255159616470337),\n",
       " (2474, 0.22548410296440125),\n",
       " (1185, 0.22548064589500427),\n",
       " (133, 0.2251981645822525),\n",
       " (1547, 0.22482538223266602),\n",
       " (2020, 0.22478538751602173),\n",
       " (2314, 0.22462990880012512),\n",
       " (1634, 0.22437229752540588),\n",
       " (1617, 0.2232489138841629),\n",
       " (2416, 0.2231529951095581),\n",
       " (663, 0.22271835803985596),\n",
       " (1751, 0.22266338765621185),\n",
       " (3585, 0.2223040610551834),\n",
       " (3953, 0.22226113080978394),\n",
       " (189, 0.22181126475334167),\n",
       " (2295, 0.2216913402080536),\n",
       " (3669, 0.22151580452919006),\n",
       " (2036, 0.22117364406585693),\n",
       " (1270, 0.220921590924263),\n",
       " (3261, 0.22089916467666626),\n",
       " (1740, 0.22025121748447418),\n",
       " (3497, 0.21954327821731567),\n",
       " (2765, 0.21940447390079498),\n",
       " (3124, 0.2190392017364502),\n",
       " (2466, 0.21893076598644257),\n",
       " (3862, 0.21892955899238586),\n",
       " (1812, 0.218686044216156),\n",
       " (238, 0.21854494512081146),\n",
       " (2303, 0.2182200402021408),\n",
       " (2791, 0.2181483954191208),\n",
       " (3676, 0.21786196529865265),\n",
       " (1210, 0.21775372326374054),\n",
       " (581, 0.21743062138557434),\n",
       " (391, 0.21701186895370483),\n",
       " (1595, 0.2163715809583664),\n",
       " (3226, 0.21628278493881226),\n",
       " (1718, 0.21626630425453186),\n",
       " (2617, 0.21619723737239838),\n",
       " (3739, 0.21595367789268494),\n",
       " (848, 0.21489517390727997),\n",
       " (3655, 0.21478477120399475),\n",
       " (2590, 0.2142978012561798),\n",
       " (2897, 0.21427813172340393),\n",
       " (3661, 0.21425941586494446),\n",
       " (1250, 0.21413567662239075),\n",
       " (3851, 0.2137650102376938),\n",
       " (3969, 0.21336330473423004),\n",
       " (3920, 0.21292473375797272),\n",
       " (3632, 0.21256376802921295),\n",
       " (2187, 0.2119205892086029),\n",
       " (288, 0.21136091649532318),\n",
       " (3175, 0.21114686131477356),\n",
       " (2679, 0.21101880073547363),\n",
       " (2704, 0.2107679545879364),\n",
       " (2236, 0.21056899428367615),\n",
       " (2524, 0.21050438284873962),\n",
       " (2611, 0.2102401703596115),\n",
       " (706, 0.20999455451965332),\n",
       " (1849, 0.2099582850933075),\n",
       " (3839, 0.20972374081611633),\n",
       " (3937, 0.20958396792411804),\n",
       " (2583, 0.20911012589931488),\n",
       " (629, 0.20906880497932434),\n",
       " (196, 0.20889189839363098),\n",
       " (1624, 0.2088504433631897),\n",
       " (2050, 0.20857417583465576),\n",
       " (1079, 0.20855817198753357),\n",
       " (323, 0.20846068859100342),\n",
       " (449, 0.2081509828567505),\n",
       " (686, 0.20794659852981567),\n",
       " (2326, 0.2078905552625656),\n",
       " (2789, 0.20786383748054504),\n",
       " (2233, 0.2077859342098236),\n",
       " (3923, 0.20728003978729248),\n",
       " (1574, 0.2069999724626541),\n",
       " (3880, 0.20682457089424133),\n",
       " (3344, 0.2067261040210724),\n",
       " (1867, 0.2067108005285263),\n",
       " (1878, 0.20669712126255035),\n",
       " (98, 0.2057695835828781),\n",
       " (1016, 0.20575465261936188),\n",
       " (2356, 0.2056627869606018),\n",
       " (1190, 0.20562197268009186),\n",
       " (1418, 0.20548619329929352),\n",
       " (3947, 0.2051849663257599),\n",
       " (976, 0.20514154434204102),\n",
       " (3011, 0.2051171064376831),\n",
       " (1419, 0.20511594414710999),\n",
       " (1692, 0.20485955476760864),\n",
       " (1522, 0.20478372275829315),\n",
       " (2028, 0.20478372275829315),\n",
       " (2231, 0.20478372275829315),\n",
       " (2388, 0.20478372275829315),\n",
       " (2707, 0.20478372275829315),\n",
       " (3192, 0.20478372275829315),\n",
       " (3214, 0.20478372275829315),\n",
       " (3868, 0.20467278361320496),\n",
       " (3803, 0.20435434579849243),\n",
       " (2337, 0.20393507182598114),\n",
       " (1628, 0.20341217517852783),\n",
       " (2080, 0.2032041698694229),\n",
       " (3052, 0.2028815746307373),\n",
       " (164, 0.20277139544487),\n",
       " (403, 0.2027224600315094),\n",
       " (3408, 0.20262013375759125),\n",
       " (1100, 0.20249013602733612),\n",
       " (3556, 0.20245975255966187),\n",
       " (693, 0.20201271772384644),\n",
       " (1131, 0.20184054970741272),\n",
       " (946, 0.20182809233665466),\n",
       " (157, 0.2018168419599533),\n",
       " (1970, 0.20177555084228516),\n",
       " (283, 0.20165881514549255),\n",
       " (2876, 0.2009645700454712),\n",
       " (3945, 0.200792595744133),\n",
       " (725, 0.20070502161979675),\n",
       " (3337, 0.20009508728981018),\n",
       " (960, 0.19987882673740387),\n",
       " (694, 0.19943398237228394),\n",
       " (23, 0.1992921680212021),\n",
       " (3549, 0.19911935925483704),\n",
       " (2825, 0.19892428815364838),\n",
       " (1651, 0.19889292120933533),\n",
       " (3942, 0.19875183701515198),\n",
       " (2040, 0.19759738445281982),\n",
       " (3065, 0.19754987955093384),\n",
       " (1639, 0.19727465510368347),\n",
       " (1677, 0.19720888137817383),\n",
       " (1601, 0.19715481996536255),\n",
       " (2784, 0.19712959229946136),\n",
       " (774, 0.19685852527618408),\n",
       " (220, 0.19660720229148865),\n",
       " (1283, 0.19659793376922607),\n",
       " (1662, 0.19659793376922607),\n",
       " (2413, 0.19659793376922607),\n",
       " (260, 0.19643890857696533),\n",
       " (1184, 0.19643434882164001),\n",
       " (675, 0.19638952612876892),\n",
       " (1860, 0.19607487320899963),\n",
       " (3845, 0.19597819447517395),\n",
       " (2009, 0.19561827182769775),\n",
       " (716, 0.19542554020881653),\n",
       " (1495, 0.19532105326652527),\n",
       " (1988, 0.1953044831752777),\n",
       " (2901, 0.19518589973449707),\n",
       " (3536, 0.1950003206729889),\n",
       " (2033, 0.1948697417974472),\n",
       " (1333, 0.19430312514305115),\n",
       " (3720, 0.19404272735118866),\n",
       " (405, 0.19356100261211395),\n",
       " (2105, 0.19328303635120392),\n",
       " (3570, 0.1932394653558731),\n",
       " (2722, 0.19315247237682343),\n",
       " (758, 0.1925896257162094),\n",
       " (2559, 0.19227787852287292),\n",
       " (803, 0.1922699213027954),\n",
       " (2382, 0.19191932678222656),\n",
       " (2770, 0.19191057980060577),\n",
       " (2307, 0.19173769652843475),\n",
       " (1234, 0.1915695071220398),\n",
       " (597, 0.19151288270950317),\n",
       " (1943, 0.19144681096076965),\n",
       " (1309, 0.19084720313549042),\n",
       " (3612, 0.19067949056625366),\n",
       " (2626, 0.19054466485977173),\n",
       " (582, 0.19021384418010712),\n",
       " (3070, 0.1900774985551834),\n",
       " (551, 0.18992596864700317),\n",
       " (1209, 0.18988075852394104),\n",
       " (401, 0.1898048371076584),\n",
       " (333, 0.18975268304347992),\n",
       " (2763, 0.18945464491844177),\n",
       " (545, 0.18940988183021545),\n",
       " (3431, 0.18924842774868011),\n",
       " (3393, 0.18919974565505981),\n",
       " (3096, 0.1891721487045288),\n",
       " (1660, 0.18888327479362488),\n",
       " (3287, 0.18881107866764069),\n",
       " (2006, 0.1886298507452011),\n",
       " (2894, 0.18853455781936646),\n",
       " (414, 0.18853367865085602),\n",
       " (2059, 0.18850460648536682),\n",
       " (365, 0.18828299641609192),\n",
       " (1725, 0.1881100833415985),\n",
       " (583, 0.18785929679870605),\n",
       " (3313, 0.18780860304832458),\n",
       " (2614, 0.18734335899353027),\n",
       " (2363, 0.1873359978199005),\n",
       " (3489, 0.1873355209827423),\n",
       " (1122, 0.1865965574979782),\n",
       " (3358, 0.18648895621299744),\n",
       " (2871, 0.18644988536834717),\n",
       " (1251, 0.18622031807899475),\n",
       " (2042, 0.1860814392566681),\n",
       " (1564, 0.18588793277740479),\n",
       " (2418, 0.18551763892173767),\n",
       " (712, 0.18546535074710846),\n",
       " (3106, 0.18526890873908997),\n",
       " (3076, 0.18515394628047943),\n",
       " (2595, 0.18502411246299744),\n",
       " (1674, 0.18482013046741486),\n",
       " (1116, 0.18475820124149323),\n",
       " (3491, 0.18442197144031525),\n",
       " (1604, 0.18439050018787384),\n",
       " (1310, 0.18422532081604004),\n",
       " (1002, 0.18398018181324005),\n",
       " (3695, 0.18383634090423584),\n",
       " (3395, 0.18358927965164185),\n",
       " (178, 0.1834927350282669),\n",
       " (3886, 0.1830819547176361),\n",
       " (318, 0.18268701434135437),\n",
       " (541, 0.1823580414056778),\n",
       " (2530, 0.18230314552783966),\n",
       " (259, 0.1820564717054367),\n",
       " (919, 0.18197789788246155),\n",
       " (2319, 0.1818629503250122),\n",
       " (2795, 0.18110889196395874),\n",
       " (2553, 0.1808476597070694),\n",
       " (1597, 0.18080100417137146),\n",
       " (375, 0.1806512027978897),\n",
       " (528, 0.18045367300510406),\n",
       " (258, 0.18040242791175842),\n",
       " (2987, 0.18034079670906067),\n",
       " (3921, 0.180323988199234),\n",
       " (3663, 0.1801685094833374),\n",
       " (2863, 0.1800926923751831),\n",
       " (2088, 0.17997166514396667),\n",
       " (3664, 0.17983375489711761),\n",
       " (3435, 0.17979054152965546),\n",
       " (3610, 0.17970509827136993),\n",
       " (2489, 0.17952841520309448),\n",
       " (127, 0.17909055948257446),\n",
       " (364, 0.1788385510444641),\n",
       " (3922, 0.17877119779586792),\n",
       " (2189, 0.17833730578422546),\n",
       " (2991, 0.17831844091415405),\n",
       " (1913, 0.17824769020080566),\n",
       " (1834, 0.17789016664028168),\n",
       " (1409, 0.17784331738948822),\n",
       " (3840, 0.17759035527706146),\n",
       " (327, 0.1775466799736023),\n",
       " (4, 0.1772681623697281),\n",
       " (1227, 0.17718607187271118),\n",
       " (1371, 0.17714114487171173),\n",
       " (2969, 0.17703121900558472),\n",
       " (1926, 0.17699813842773438),\n",
       " (507, 0.1769929826259613),\n",
       " (2138, 0.17690619826316833),\n",
       " (2148, 0.17626827955245972),\n",
       " (2900, 0.17601598799228668),\n",
       " (523, 0.17596301436424255),\n",
       " (3885, 0.1755954623222351),\n",
       " (3966, 0.17548322677612305),\n",
       " (132, 0.17523355782032013),\n",
       " (626, 0.17496037483215332),\n",
       " (1941, 0.17395922541618347),\n",
       " (2056, 0.1739114224910736),\n",
       " (3552, 0.17364083230495453),\n",
       " (3117, 0.17360202968120575),\n",
       " (3635, 0.17359042167663574),\n",
       " (1298, 0.17358407378196716),\n",
       " (3757, 0.1735670566558838),\n",
       " (3891, 0.17337700724601746),\n",
       " (2332, 0.17324985563755035),\n",
       " (390, 0.17293643951416016),\n",
       " (2568, 0.1728687584400177),\n",
       " (3112, 0.17267996072769165),\n",
       " (3455, 0.17265962064266205),\n",
       " (3013, 0.17251692712306976),\n",
       " (3794, 0.1724703162908554),\n",
       " (1901, 0.17245471477508545),\n",
       " (1225, 0.1721612811088562),\n",
       " (3546, 0.17207422852516174),\n",
       " (3302, 0.172011137008667),\n",
       " (1058, 0.17112770676612854),\n",
       " (3182, 0.17102967202663422),\n",
       " (3438, 0.1709812432527542),\n",
       " (3312, 0.170729860663414),\n",
       " (2446, 0.1706664115190506),\n",
       " (3554, 0.17063474655151367),\n",
       " (255, 0.17028069496154785),\n",
       " (212, 0.16961108148097992),\n",
       " (3986, 0.16960100829601288),\n",
       " (977, 0.16956552863121033),\n",
       " (918, 0.1695641577243805),\n",
       " (1750, 0.169464573264122),\n",
       " (2738, 0.1693461388349533),\n",
       " (1263, 0.1688932478427887),\n",
       " (2531, 0.1687946617603302),\n",
       " (1632, 0.16878144443035126),\n",
       " (498, 0.1681745946407318),\n",
       " (2067, 0.1681598722934723),\n",
       " (2367, 0.1681157350540161),\n",
       " (2350, 0.16791822016239166),\n",
       " (949, 0.167844757437706),\n",
       " (3667, 0.1678086519241333),\n",
       " (1307, 0.1675720512866974),\n",
       " (509, 0.1675523966550827),\n",
       " (230, 0.16750724613666534),\n",
       " (442, 0.1673375964164734),\n",
       " (2533, 0.16728883981704712),\n",
       " (2323, 0.16721394658088684),\n",
       " (2196, 0.16698575019836426),\n",
       " (2846, 0.16695541143417358),\n",
       " (2270, 0.16687577962875366),\n",
       " (2764, 0.1668735295534134),\n",
       " (2048, 0.16654270887374878),\n",
       " (2702, 0.1663438379764557),\n",
       " (3801, 0.16625992953777313),\n",
       " (559, 0.16618718206882477),\n",
       " (2831, 0.16596485674381256),\n",
       " (2351, 0.16574151813983917),\n",
       " (1879, 0.16563621163368225),\n",
       " (1880, 0.16551753878593445),\n",
       " (201, 0.1650940477848053),\n",
       " (2954, 0.1649969071149826),\n",
       " (3613, 0.16462242603302002),\n",
       " (2209, 0.16453884541988373),\n",
       " (1583, 0.16427265107631683),\n",
       " (1396, 0.16415879130363464),\n",
       " (278, 0.16399794816970825),\n",
       " (1698, 0.1638568788766861),\n",
       " (711, 0.1636597365140915),\n",
       " (554, 0.1635603904724121),\n",
       " (3255, 0.16324380040168762),\n",
       " (902, 0.16324014961719513),\n",
       " (372, 0.1631798893213272),\n",
       " (3641, 0.16309690475463867),\n",
       " (3074, 0.1626889556646347),\n",
       " (3633, 0.16248975694179535),\n",
       " (939, 0.16227316856384277),\n",
       " (658, 0.16225190460681915),\n",
       " (1721, 0.1619875133037567),\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_surprise={}\n",
    "for key, value in sorted_surprise:\n",
    "    if value>0.5:\n",
    "        high_surprise[key]=value\n",
    "len(high_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Think of these lesser-known places before you book your next vacation.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_reactions.iloc[3872]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Once you cook salmon like this, you'll never want it any other way.\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_reactions.iloc[3255]['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Semeval</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_emotions=pd.read_csv('data/semeval_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_emotions.rename({'0': 0, '1': 1}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_plus_semeval=isear.append(semeval_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>joy</td>\n",
       "      <td>After my girlfriend had taken her exam we went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fear</td>\n",
       "      <td>When, for the first time I realized the meanin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anger</td>\n",
       "      <td>When a car is overtaking another and I am forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I recently thought about the hard work it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I pass an examination which I did not thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fear</td>\n",
       "      <td>When one has arranged to meet someone and that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>anger</td>\n",
       "      <td>When one is unjustly accused of something one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When one's studies seem hopelessly difficult a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>joy</td>\n",
       "      <td>Passing an exam I did not expect to pass.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I climbed up a tree to pick apples.  The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I had my children.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fear</td>\n",
       "      <td>When my 2 year old son climbed up and sat on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>anger</td>\n",
       "      <td>When my partner was attacked and lost three te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I see children on T.V from areas devastat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>joy</td>\n",
       "      <td>When my child was born.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fear</td>\n",
       "      <td>It was spring and the ice was melting.  I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>anger</td>\n",
       "      <td>Unjust accusations directed at me and my way o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Failing an examination.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I saw a person I had not seen for a long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>fear</td>\n",
       "      <td>When, as a child, I was nearly knocked down by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I heard on the radio that the football ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I feel lonely, perhaps because I have to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>joy</td>\n",
       "      <td>When I was accepted for a course on finance an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>fear</td>\n",
       "      <td>A bus drove over my right leg.  The event itse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>anger</td>\n",
       "      <td>At my Summer job, nobody looked after me in pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I was not accepted as a student in financ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>sadness</td>\n",
       "      <td>i got his bitch depress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Really planned on making videos this week. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Regret for the things we did can be tempered b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>sadness</td>\n",
       "      <td>im having the worst week ever and i cant even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>sadness</td>\n",
       "      <td>we never taste happiness in perfection, our mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Whenever I'm feeling sad I will listen to mons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Another grim &amp;amp; compelling news report by @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Absolutely hate the Apple Watch iOS 10 update....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@TNFryed Jesus, you just made think that of al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Never ever been this unhappy before in my life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@GRIPLIKEAVICE_ I wouldn't mind if it didn't y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@flybe Doesn't explain the ability to land at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@FoxNews Very thought provoking &amp;amp; leads on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I miss when social media was a place to get la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>sadness</td>\n",
       "      <td>My encouragement today is my dog while my head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@pureleine though lately with how bad my depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>sadness</td>\n",
       "      <td>now im all alone and my joy's turned to moping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I hate when it's gloomy outside because it alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>sadness</td>\n",
       "      <td>So unbelievably discouraged with music as of l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@charles_gaba No, I am probably the person mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>sadness</td>\n",
       "      <td>the sad moment when u hand in an exam knowing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@keyshamackie it's fucking dreadful for live f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@narcissusheiyan  maybe it'd have been differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>sadness</td>\n",
       "      <td>It's sad when your man leaves work a little bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>sadness</td>\n",
       "      <td>My friends tell me I'm pretty. Trigger tells m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@CrystiCaro yeah agree - I think it was a fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@wabermes The @RavalliRepublic had a good one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>sadness</td>\n",
       "      <td>If Troyler will die, I'm gonna die with them\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Overwhelming sadness.  This too shall pass. #l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Idk why people be glorifying depression. I wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7057 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                                  1\n",
       "0         joy  On days when I feel close to my partner and ot...\n",
       "1        fear  Every time I imagine that someone I love or I ...\n",
       "2       anger  When I had been obviously unjustly treated and...\n",
       "3     sadness  When I think about the short time that we live...\n",
       "7         joy  After my girlfriend had taken her exam we went...\n",
       "8        fear  When, for the first time I realized the meanin...\n",
       "9       anger  When a car is overtaking another and I am forc...\n",
       "10    sadness  When I recently thought about the hard work it...\n",
       "14        joy  When I pass an examination which I did not thi...\n",
       "15       fear  When one has arranged to meet someone and that...\n",
       "16      anger  When one is unjustly accused of something one ...\n",
       "17    sadness  When one's studies seem hopelessly difficult a...\n",
       "21        joy          Passing an exam I did not expect to pass.\n",
       "22       fear  When I climbed up a tree to pick apples.  The ...\n",
       "24        joy                            When I had my children.\n",
       "25       fear  When my 2 year old son climbed up and sat on t...\n",
       "26      anger  When my partner was attacked and lost three te...\n",
       "27    sadness  When I see children on T.V from areas devastat...\n",
       "31        joy                            When my child was born.\n",
       "32       fear  It was spring and the ice was melting.  I was ...\n",
       "33      anger  Unjust accusations directed at me and my way o...\n",
       "34    sadness                            Failing an examination.\n",
       "38        joy  When I saw a person I had not seen for a long ...\n",
       "39       fear  When, as a child, I was nearly knocked down by...\n",
       "40      anger  When I heard on the radio that the football ma...\n",
       "41    sadness  When I feel lonely, perhaps because I have to ...\n",
       "45        joy  When I was accepted for a course on finance an...\n",
       "46       fear  A bus drove over my right leg.  The event itse...\n",
       "47      anger  At my Summer job, nobody looked after me in pa...\n",
       "48    sadness  When I was not accepted as a student in financ...\n",
       "...       ...                                                ...\n",
       "2698  sadness                            i got his bitch depress\n",
       "2699  sadness  Really planned on making videos this week. The...\n",
       "2700  sadness  Regret for the things we did can be tempered b...\n",
       "2701  sadness  im having the worst week ever and i cant even ...\n",
       "2702  sadness  we never taste happiness in perfection, our mo...\n",
       "2703  sadness  Whenever I'm feeling sad I will listen to mons...\n",
       "2704  sadness  Another grim &amp; compelling news report by @...\n",
       "2705  sadness  Absolutely hate the Apple Watch iOS 10 update....\n",
       "2706  sadness  @TNFryed Jesus, you just made think that of al...\n",
       "2707  sadness  Never ever been this unhappy before in my life...\n",
       "2708  sadness  @GRIPLIKEAVICE_ I wouldn't mind if it didn't y...\n",
       "2709  sadness  @flybe Doesn't explain the ability to land at ...\n",
       "2710  sadness  @FoxNews Very thought provoking &amp; leads on...\n",
       "2711  sadness  I miss when social media was a place to get la...\n",
       "2712  sadness  My encouragement today is my dog while my head...\n",
       "2713  sadness  @pureleine though lately with how bad my depre...\n",
       "2714  sadness     now im all alone and my joy's turned to moping\n",
       "2715  sadness  I hate when it's gloomy outside because it alw...\n",
       "2716  sadness  So unbelievably discouraged with music as of l...\n",
       "2717  sadness  @charles_gaba No, I am probably the person mos...\n",
       "2718  sadness  the sad moment when u hand in an exam knowing ...\n",
       "2719  sadness  @keyshamackie it's fucking dreadful for live f...\n",
       "2720  sadness  @narcissusheiyan  maybe it'd have been differe...\n",
       "2721  sadness  It's sad when your man leaves work a little bi...\n",
       "2722  sadness  My friends tell me I'm pretty. Trigger tells m...\n",
       "2723  sadness  @CrystiCaro yeah agree - I think it was a fami...\n",
       "2724  sadness  @wabermes The @RavalliRepublic had a good one ...\n",
       "2725  sadness  If Troyler will die, I'm gonna die with them\\n...\n",
       "2726  sadness  Overwhelming sadness.  This too shall pass. #l...\n",
       "2727  sadness  Idk why people be glorifying depression. I wou...\n",
       "\n",
       "[7057 rows x 2 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isear_plus_semeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_duplicator_train(text):\n",
    "    splits=text.split(' ')\n",
    "    while len(splits)<7:\n",
    "        orig_doc=splits.copy()\n",
    "        for word in orig_doc:\n",
    "            splits.append(word)\n",
    "    return ' '.join(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=isear_plus_semeval[1].apply(clean).apply(input_duplicator_train)\n",
    "y=isear_plus_semeval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       day feel close partner friend feel peace also ...\n",
       "1       every time imagine someone love could contact ...\n",
       "2       obviously unjustly treated possibility elucida...\n",
       "3       think short time live relate period life think...\n",
       "7       girlfriend taken exam went parent place girlfr...\n",
       "8       when first time realized meaning death when fi...\n",
       "9       car overtaking another forced drive road car o...\n",
       "10      recently thought hard work take study one want...\n",
       "14      pas examination think well pas examination thi...\n",
       "15      one arranged meet someone person arrives late ...\n",
       "16      one unjustly accused something one done one un...\n",
       "17      one study seem hopelessly difficult uninterest...\n",
       "21        passing exam expect pas passing exam expect pas\n",
       "22      climbed tree pick apple angle ladder enable ge...\n",
       "24        child child child child child child child child\n",
       "25      2 year old son climbed sat 7th floor balcony l...\n",
       "26      partner attacked lost three teeth partner atta...\n",
       "27               see child tv area devastated drought war\n",
       "31            child born child born child born child born\n",
       "32      spring ice melting far jetty poked ice long br...\n",
       "33      unjust accusation directed way acting someone ...\n",
       "34      failing examination failing examination failin...\n",
       "38      saw person seen long time saw person seen long...\n",
       "39      when child nearly knocked car when child nearl...\n",
       "40      heard radio football match belgium ended catas...\n",
       "41      feel lonely perhaps study lot shut appartment ...\n",
       "45      accepted course finance accounting accepted co...\n",
       "46      bus drove right leg event frightening wait eme...\n",
       "47          summer job nobody looked particular learn own\n",
       "48      accepted student finance accounting accepted s...\n",
       "                              ...                        \n",
       "2698    got bitch depress got bitch depress got bitch ...\n",
       "2699    really planned making video week then tv died ...\n",
       "2700    regret thing tempered time regret thing incons...\n",
       "2701        im worst week ever cant even go home sulk bed\n",
       "2702    never taste happiness perfection fortunate suc...\n",
       "2703    whenever im feeling sad listen monsta x hug te...\n",
       "2704    another grim amp compelling news report nawalf...\n",
       "2705    absolutely hate apple watch io 10 update compl...\n",
       "2706    tnfryed jesus made think vols v fl game ive se...\n",
       "2707    never ever unhappy life lmao never ever unhapp...\n",
       "2708    griplikeavice mind know threaten job disagree ...\n",
       "2709    flybe explain ability land manchester bradford...\n",
       "2710    foxnews thought provoking amp lead one questio...\n",
       "2711    miss social medium place get laugh jump dm lol...\n",
       "2712    encouragement today dog head fog epicfail depr...\n",
       "2713    pureleine though lately bad depression feel li...\n",
       "2714    im alone joy turned moping im alone joy turned...\n",
       "2715       hate gloomy outside always get depressing mood\n",
       "2716    unbelievably discouraged music late incredibly...\n",
       "2717    charlesgaba no probably person likely complete...\n",
       "2718    sad moment u hand exam knowing u failed grieve...\n",
       "2719    keyshamackie fucking dreadful live footy match...\n",
       "2720    narcissusheiyan maybe itd different stayed six...\n",
       "2721    sad man leaf work little bit late worst fear o...\n",
       "2722    friend tell im pretty trigger tell im ugly fir...\n",
       "2723    crysticaro yeah agree think family member cove...\n",
       "2724    wabermes ravallirepublic good one reporter qui...\n",
       "2725    troyler die im gonna die themntroyler sadness ...\n",
       "2726    overwhelming sadness shall pas lost lonley sta...\n",
       "2727    idk people glorifying depression wish real dep...\n",
       "Name: 1, Length: 7057, dtype: object"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receiving postcard form sweetheart abroad vacation sure would write not',\n",
       " 'friend promised would definitely call tell visiting not',\n",
       " 'person died not really friend person died not really friend',\n",
       " 'person died not really friend person died not really friend',\n",
       " 'encountered father lack understanding also unwillingness understand parent far thing not he prof right concerned',\n",
       " 'anger men inability human not men anger men inability human not men',\n",
       " 'anger men inability human not men anger men inability human not men',\n",
       " 'sexually assaulted not rape passionate kissing friend father stopped driving home wanted lift refused 3 time felt itd rude refuse again really afraid might do',\n",
       " 'argument best friend thought right not argument best friend thought right not',\n",
       " 'argument best friend thought right not argument best friend thought right not',\n",
       " 'bought new bed apt squeak move it tried fix could not',\n",
       " 'get admission msc bsc felt really bad started journey bombay accepted uncle offer come bombay afraid wondering would succeed not',\n",
       " 'told everyone last lecture lecture english everybody come not english class',\n",
       " 'fearful realised relish overcooked simply keen enough checking whether still enough water relish not instead playing friend',\n",
       " 'finding ill not seriously finding ill not seriously',\n",
       " 'finding ill not seriously finding ill not seriously',\n",
       " 'told people class leader choose friend not true',\n",
       " 'iqam chosen lecture best friend not angry cannot help him',\n",
       " 'prejudice psychology student education student not allowing u go medical university lecture',\n",
       " 'someone told chosen english lecture class leader going not true',\n",
       " '……of course not quite dismayed ludicrous nature claim',\n",
       " 'bridgetjones joyous worried would disappointed definitely not chickflick giggle comethefuckonbridget',\n",
       " 'exhilarating feeling working hard feeling accomplished whether notice not feel fuking good 🤑',\n",
       " 'othrcarl ixsw purenift im sadly not im smutty',\n",
       " 'kinghis action really sadden todaythe word may not power did still cringe hear']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_count=0\n",
    "not_docs=[]\n",
    "for row in X:\n",
    "    split=row.split(' ')\n",
    "    for word in split:\n",
    "        if word=='not':\n",
    "            not_count+=1\n",
    "            not_docs.append(' '.join(split))\n",
    "not_count\n",
    "not_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, y_1=transform_t6_train(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(y_1)\n",
    "y_1=encoder.transform(y_1)\n",
    "y_1=np_utils.to_categorical(y_1)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.2, random_state = 0)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=25, return_sequences=True, input_shape=(None, 300...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "four_emotions=create_lstm(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "27013/27013 [==============================] - 16s 596us/step - loss: 0.0415 - acc: 0.9079\n",
      "Epoch 2/25\n",
      "27013/27013 [==============================] - 13s 465us/step - loss: 0.0402 - acc: 0.9114\n",
      "Epoch 3/25\n",
      "27013/27013 [==============================] - 13s 468us/step - loss: 0.0394 - acc: 0.9124\n",
      "Epoch 4/25\n",
      "27013/27013 [==============================] - 13s 467us/step - loss: 0.0379 - acc: 0.9176\n",
      "Epoch 5/25\n",
      "27013/27013 [==============================] - 13s 466us/step - loss: 0.0371 - acc: 0.9194\n",
      "Epoch 6/25\n",
      "27013/27013 [==============================] - 13s 468us/step - loss: 0.0373 - acc: 0.9202\n",
      "Epoch 7/25\n",
      "27013/27013 [==============================] - 14s 511us/step - loss: 0.0354 - acc: 0.9242\n",
      "Epoch 8/25\n",
      "27013/27013 [==============================] - 15s 544us/step - loss: 0.0346 - acc: 0.9269\n",
      "Epoch 9/25\n",
      "27013/27013 [==============================] - 15s 565us/step - loss: 0.0348 - acc: 0.9266\n",
      "Epoch 10/25\n",
      "27013/27013 [==============================] - 14s 526us/step - loss: 0.0336 - acc: 0.9304\n",
      "Epoch 11/25\n",
      "27013/27013 [==============================] - 14s 513us/step - loss: 0.0328 - acc: 0.9313\n",
      "Epoch 12/25\n",
      "27013/27013 [==============================] - 13s 498us/step - loss: 0.0327 - acc: 0.9325\n",
      "Epoch 13/25\n",
      "27013/27013 [==============================] - 12s 451us/step - loss: 0.0313 - acc: 0.9365\n",
      "Epoch 14/25\n",
      "27013/27013 [==============================] - 12s 446us/step - loss: 0.0312 - acc: 0.9365\n",
      "Epoch 15/25\n",
      "27013/27013 [==============================] - 13s 467us/step - loss: 0.0303 - acc: 0.9388\n",
      "Epoch 16/25\n",
      "27013/27013 [==============================] - 14s 506us/step - loss: 0.0294 - acc: 0.9408\n",
      "Epoch 17/25\n",
      "27013/27013 [==============================] - 14s 510us/step - loss: 0.0295 - acc: 0.9411\n",
      "Epoch 18/25\n",
      "27013/27013 [==============================] - 14s 533us/step - loss: 0.0291 - acc: 0.9419\n",
      "Epoch 19/25\n",
      "27013/27013 [==============================] - 13s 490us/step - loss: 0.0288 - acc: 0.9440\n",
      "Epoch 20/25\n",
      "27013/27013 [==============================] - 13s 469us/step - loss: 0.0280 - acc: 0.9445\n",
      "Epoch 21/25\n",
      "27013/27013 [==============================] - 13s 465us/step - loss: 0.0274 - acc: 0.9471\n",
      "Epoch 22/25\n",
      "27013/27013 [==============================] - 13s 467us/step - loss: 0.0277 - acc: 0.9454\n",
      "Epoch 23/25\n",
      "27013/27013 [==============================] - 12s 455us/step - loss: 0.0274 - acc: 0.9470\n",
      "Epoch 24/25\n",
      "27013/27013 [==============================] - 12s 456us/step - loss: 0.0268 - acc: 0.9485\n",
      "Epoch 25/25\n",
      "27013/27013 [==============================] - 13s 493us/step - loss: 0.0260 - acc: 0.9491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x537943c8>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_emotions.compile(optimizer = 'adam', metrics=['accuracy'], loss = 'mean_squared_error')\n",
    "four_emotions.fit(X_train, y_train, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test= np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6754/6754 [==============================] - 1s 215us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05646390239088301, 0.8606751554634291]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_emotions.evaluate(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_emotions.save('four_emotions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(8, 300)\n"
     ]
    }
   ],
   "source": [
    "i_eat=prediction('I eat', four_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Synsets from wordnet </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delight', 'joy', 'joyfulness', 'joyousness', 'pleasure'}"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets('joy', pos=wordnet.NOUN):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "synonyms=set(synonyms)\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn_finder(syns, part_of_speech):\n",
    "    new_syns=list(syns)\n",
    "    if len(new_syns)<100:\n",
    "        for syn in syns:\n",
    "            for synonym in wordnet.synsets(syn, pos=part_of_speech):\n",
    "                for l in synonym.lemmas():\n",
    "                    new_syns.append(l.name())\n",
    "        new_syns=list(set(new_syns))\n",
    "        return syn_finder(new_syns, part_of_speech)\n",
    "    else:\n",
    "        return new_syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=syn_finder(['joy'], wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amour',\n",
       " 'quality',\n",
       " 'chromosome_mapping',\n",
       " 'drug_abuse',\n",
       " 'engagement',\n",
       " 'betrothal',\n",
       " 'treatment',\n",
       " 'thing',\n",
       " 'workout',\n",
       " 'fiber',\n",
       " 'fibre',\n",
       " 'eccentric',\n",
       " 'joyfulness',\n",
       " 'study',\n",
       " 'white_plague',\n",
       " 'use',\n",
       " 'subroutine',\n",
       " 'section',\n",
       " 'conclusion',\n",
       " 'pattern',\n",
       " 'portion',\n",
       " 'spending',\n",
       " 'enjoyment',\n",
       " 'mesh',\n",
       " 'physical_exercise',\n",
       " 'type',\n",
       " 'case',\n",
       " 'position',\n",
       " 'purport',\n",
       " 'component_part',\n",
       " 'aim',\n",
       " 'ingestion',\n",
       " 'joy',\n",
       " 'booking',\n",
       " 'single-valued_function',\n",
       " 'use_of_goods_and_services',\n",
       " 'office',\n",
       " 'model',\n",
       " 'region',\n",
       " 'expending',\n",
       " 'inlet',\n",
       " 'act',\n",
       " 'conflict',\n",
       " 'share',\n",
       " 'occasion',\n",
       " 'wont',\n",
       " 'function',\n",
       " 'object',\n",
       " 'phthisis',\n",
       " 'bearing',\n",
       " 'deterrent_example',\n",
       " 'manipulation',\n",
       " 'affaire',\n",
       " 'object_lesson',\n",
       " 'breathing_in',\n",
       " 'excogitation',\n",
       " 'utilisation',\n",
       " 'juncture',\n",
       " 'usage',\n",
       " 'government_agency',\n",
       " 'finding',\n",
       " 'Mandrillus_leucophaeus',\n",
       " 'joyousness',\n",
       " 'inspiration',\n",
       " 'intention',\n",
       " 'post',\n",
       " 'habit',\n",
       " 'social_function',\n",
       " 'decision',\n",
       " 'recital',\n",
       " 'good_example',\n",
       " 'practice_session',\n",
       " 'component',\n",
       " 'power',\n",
       " 'reference',\n",
       " 'delectation',\n",
       " 'parting',\n",
       " 'inhalation',\n",
       " 'authority',\n",
       " 'designing',\n",
       " 'social_occasion',\n",
       " 'practice',\n",
       " 'fictional_character',\n",
       " 'exemplar',\n",
       " 'substance_abuse',\n",
       " 'berth',\n",
       " 'involvement',\n",
       " 'pleasance',\n",
       " 'business_office',\n",
       " 'map',\n",
       " 'riding_habit',\n",
       " 'blueprint',\n",
       " 'operation',\n",
       " 'involution',\n",
       " 'troth',\n",
       " 'battle',\n",
       " 'custom',\n",
       " 'plan',\n",
       " 'voice',\n",
       " 'usance',\n",
       " 'piece',\n",
       " 'exercise',\n",
       " 'recitation',\n",
       " 'conception',\n",
       " 'invention',\n",
       " 'class_period',\n",
       " 'workplace',\n",
       " 'turn',\n",
       " 'constituent',\n",
       " 'physical_exertion',\n",
       " 'mathematical_function',\n",
       " 'drug_addiction',\n",
       " 'situation',\n",
       " 'agency',\n",
       " 'character',\n",
       " 'intake',\n",
       " 'handling',\n",
       " 'fight',\n",
       " 'spot',\n",
       " 'number',\n",
       " 'customs_duty',\n",
       " 'lesson',\n",
       " 'drill',\n",
       " 'employ',\n",
       " 'billet',\n",
       " 'date',\n",
       " 'determination',\n",
       " 'place',\n",
       " 'figure',\n",
       " 'illustration',\n",
       " 'delight',\n",
       " 'purpose',\n",
       " 'part',\n",
       " 'modus_operandi',\n",
       " 'body_of_work',\n",
       " 'course_session',\n",
       " 'design',\n",
       " 'piece_of_work',\n",
       " 'exercising',\n",
       " 'federal_agency',\n",
       " 'subprogram',\n",
       " 'office_staff',\n",
       " 'bit',\n",
       " 'instance',\n",
       " 'oeuvre',\n",
       " 'meshing',\n",
       " 'process',\n",
       " 'aspiration',\n",
       " 'theatrical_role',\n",
       " 'example',\n",
       " 'wasting_disease',\n",
       " 'expenditure',\n",
       " 'grapheme',\n",
       " 'liaison',\n",
       " 'interlocking',\n",
       " 'graphic_symbol',\n",
       " 'matter',\n",
       " 'work',\n",
       " 'utilization',\n",
       " 'innovation',\n",
       " 'affair',\n",
       " 'lineament',\n",
       " 'appointment',\n",
       " 'uptake',\n",
       " 'intimacy',\n",
       " 'role',\n",
       " 'spirit',\n",
       " 'mapping',\n",
       " 'pleasure',\n",
       " 'division',\n",
       " 'outgo',\n",
       " 'participation',\n",
       " 'economic_consumption',\n",
       " 'praxis',\n",
       " 'persona',\n",
       " 'image',\n",
       " 'tradition',\n",
       " 'employment',\n",
       " 'consumption',\n",
       " 'heading',\n",
       " 'using_up',\n",
       " 'routine',\n",
       " 'pulmonary_tuberculosis',\n",
       " 'intent',\n",
       " 'fictitious_character',\n",
       " 'bureau',\n",
       " 'percentage',\n",
       " 'reading',\n",
       " 'objective',\n",
       " 'customs',\n",
       " 'procedure',\n",
       " 'impost',\n",
       " 'outlay',\n",
       " 'contribution',\n",
       " 'target',\n",
       " 'character_reference',\n",
       " 'representative']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DepecheMood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "depeche=pd.read_csv('data/depeche_word_isolation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma#PoS</th>\n",
       "      <th>AFRAID</th>\n",
       "      <th>AMUSED</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>ANNOYED</th>\n",
       "      <th>DONT_CARE</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>INSPIRED</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.087697</td>\n",
       "      <td>0.092489</td>\n",
       "      <td>0.154124</td>\n",
       "      <td>0.077188</td>\n",
       "      <td>0.136462</td>\n",
       "      <td>0.287111</td>\n",
       "      <td>0.084152</td>\n",
       "      <td>0.080778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.129418</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>0.114674</td>\n",
       "      <td>0.114568</td>\n",
       "      <td>0.136147</td>\n",
       "      <td>0.161294</td>\n",
       "      <td>0.114729</td>\n",
       "      <td>0.120718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.130558</td>\n",
       "      <td>0.109739</td>\n",
       "      <td>0.115716</td>\n",
       "      <td>0.109559</td>\n",
       "      <td>0.151769</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.111359</td>\n",
       "      <td>0.130342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.156990</td>\n",
       "      <td>0.126365</td>\n",
       "      <td>0.102937</td>\n",
       "      <td>0.087751</td>\n",
       "      <td>0.121395</td>\n",
       "      <td>0.109795</td>\n",
       "      <td>0.118774</td>\n",
       "      <td>0.175993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.133133</td>\n",
       "      <td>0.186631</td>\n",
       "      <td>0.096675</td>\n",
       "      <td>0.060371</td>\n",
       "      <td>0.051486</td>\n",
       "      <td>0.151350</td>\n",
       "      <td>0.120990</td>\n",
       "      <td>0.199363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.079639</td>\n",
       "      <td>0.114055</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.643330</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.075132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000th</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437545</td>\n",
       "      <td>0.271235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100th</td>\n",
       "      <td>0.308124</td>\n",
       "      <td>0.115666</td>\n",
       "      <td>0.100621</td>\n",
       "      <td>0.102881</td>\n",
       "      <td>0.037436</td>\n",
       "      <td>0.118386</td>\n",
       "      <td>0.150157</td>\n",
       "      <td>0.066728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>101st</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10th</td>\n",
       "      <td>0.092306</td>\n",
       "      <td>0.066443</td>\n",
       "      <td>0.122701</td>\n",
       "      <td>0.086613</td>\n",
       "      <td>0.109534</td>\n",
       "      <td>0.181413</td>\n",
       "      <td>0.101312</td>\n",
       "      <td>0.239679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.180347</td>\n",
       "      <td>0.094039</td>\n",
       "      <td>0.114482</td>\n",
       "      <td>0.101375</td>\n",
       "      <td>0.117258</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>0.163165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>110th</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857859</td>\n",
       "      <td>0.102437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>115th</td>\n",
       "      <td>0.095784</td>\n",
       "      <td>0.043246</td>\n",
       "      <td>0.080320</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.082486</td>\n",
       "      <td>0.549403</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>0.075256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11th</td>\n",
       "      <td>0.102418</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.090567</td>\n",
       "      <td>0.130365</td>\n",
       "      <td>0.167512</td>\n",
       "      <td>0.161856</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>0.107427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>0.130229</td>\n",
       "      <td>0.106832</td>\n",
       "      <td>0.124114</td>\n",
       "      <td>0.142247</td>\n",
       "      <td>0.122799</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>0.130273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>120</td>\n",
       "      <td>0.137943</td>\n",
       "      <td>0.059509</td>\n",
       "      <td>0.133294</td>\n",
       "      <td>0.156163</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>0.125719</td>\n",
       "      <td>0.099549</td>\n",
       "      <td>0.187316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120th</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172776</td>\n",
       "      <td>0.262831</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12th</td>\n",
       "      <td>0.081284</td>\n",
       "      <td>0.105454</td>\n",
       "      <td>0.105266</td>\n",
       "      <td>0.220674</td>\n",
       "      <td>0.119434</td>\n",
       "      <td>0.145781</td>\n",
       "      <td>0.103076</td>\n",
       "      <td>0.119031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>0.113830</td>\n",
       "      <td>0.113067</td>\n",
       "      <td>0.147606</td>\n",
       "      <td>0.139966</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>0.088118</td>\n",
       "      <td>0.134123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13th</td>\n",
       "      <td>0.084568</td>\n",
       "      <td>0.103624</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.142401</td>\n",
       "      <td>0.169072</td>\n",
       "      <td>0.211764</td>\n",
       "      <td>0.122687</td>\n",
       "      <td>0.111186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14</td>\n",
       "      <td>0.109144</td>\n",
       "      <td>0.113042</td>\n",
       "      <td>0.123924</td>\n",
       "      <td>0.120082</td>\n",
       "      <td>0.145046</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.104223</td>\n",
       "      <td>0.141839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>144</td>\n",
       "      <td>0.210856</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.045845</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.068370</td>\n",
       "      <td>0.093623</td>\n",
       "      <td>0.133566</td>\n",
       "      <td>0.200707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14th</td>\n",
       "      <td>0.070650</td>\n",
       "      <td>0.062961</td>\n",
       "      <td>0.139161</td>\n",
       "      <td>0.138515</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.189092</td>\n",
       "      <td>0.119890</td>\n",
       "      <td>0.141929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>0.141574</td>\n",
       "      <td>0.110872</td>\n",
       "      <td>0.133966</td>\n",
       "      <td>0.114720</td>\n",
       "      <td>0.119732</td>\n",
       "      <td>0.141042</td>\n",
       "      <td>0.089399</td>\n",
       "      <td>0.148696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150th</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.684236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15th</td>\n",
       "      <td>0.061601</td>\n",
       "      <td>0.116090</td>\n",
       "      <td>0.155957</td>\n",
       "      <td>0.137201</td>\n",
       "      <td>0.164420</td>\n",
       "      <td>0.147053</td>\n",
       "      <td>0.149305</td>\n",
       "      <td>0.068372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16</td>\n",
       "      <td>0.119421</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>0.127524</td>\n",
       "      <td>0.117573</td>\n",
       "      <td>0.170526</td>\n",
       "      <td>0.135229</td>\n",
       "      <td>0.090803</td>\n",
       "      <td>0.127738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16th</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.148314</td>\n",
       "      <td>0.097487</td>\n",
       "      <td>0.166863</td>\n",
       "      <td>0.132453</td>\n",
       "      <td>0.160826</td>\n",
       "      <td>0.115615</td>\n",
       "      <td>0.085370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17</td>\n",
       "      <td>0.135894</td>\n",
       "      <td>0.113842</td>\n",
       "      <td>0.144138</td>\n",
       "      <td>0.106563</td>\n",
       "      <td>0.105475</td>\n",
       "      <td>0.156021</td>\n",
       "      <td>0.098517</td>\n",
       "      <td>0.139551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1728</td>\n",
       "      <td>0.094016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188916</td>\n",
       "      <td>0.225235</td>\n",
       "      <td>0.276742</td>\n",
       "      <td>0.123113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37741</th>\n",
       "      <td>zionist</td>\n",
       "      <td>0.149224</td>\n",
       "      <td>0.067373</td>\n",
       "      <td>0.062567</td>\n",
       "      <td>0.528923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062997</td>\n",
       "      <td>0.128916</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37742</th>\n",
       "      <td>zip</td>\n",
       "      <td>0.063739</td>\n",
       "      <td>0.115184</td>\n",
       "      <td>0.024072</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.142529</td>\n",
       "      <td>0.130776</td>\n",
       "      <td>0.296190</td>\n",
       "      <td>0.132752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37743</th>\n",
       "      <td>zip</td>\n",
       "      <td>0.039454</td>\n",
       "      <td>0.254250</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.184242</td>\n",
       "      <td>0.172760</td>\n",
       "      <td>0.202852</td>\n",
       "      <td>0.004991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37744</th>\n",
       "      <td>zipper</td>\n",
       "      <td>0.090654</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.041031</td>\n",
       "      <td>0.391805</td>\n",
       "      <td>0.093548</td>\n",
       "      <td>0.181263</td>\n",
       "      <td>0.091721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37745</th>\n",
       "      <td>zippy</td>\n",
       "      <td>0.023420</td>\n",
       "      <td>0.383111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014986</td>\n",
       "      <td>0.436862</td>\n",
       "      <td>0.083081</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.042127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37746</th>\n",
       "      <td>zn</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204499</td>\n",
       "      <td>0.675232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37747</th>\n",
       "      <td>zodiac</td>\n",
       "      <td>0.261967</td>\n",
       "      <td>0.153140</td>\n",
       "      <td>0.159014</td>\n",
       "      <td>0.031348</td>\n",
       "      <td>0.060724</td>\n",
       "      <td>0.180759</td>\n",
       "      <td>0.108552</td>\n",
       "      <td>0.044497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37748</th>\n",
       "      <td>zodiacal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107396</td>\n",
       "      <td>0.157739</td>\n",
       "      <td>0.305251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37749</th>\n",
       "      <td>zola</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37750</th>\n",
       "      <td>zombie</td>\n",
       "      <td>0.080676</td>\n",
       "      <td>0.350053</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>0.057944</td>\n",
       "      <td>0.155826</td>\n",
       "      <td>0.117568</td>\n",
       "      <td>0.169591</td>\n",
       "      <td>0.049107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37751</th>\n",
       "      <td>zonal</td>\n",
       "      <td>0.072611</td>\n",
       "      <td>0.020090</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.217128</td>\n",
       "      <td>0.062171</td>\n",
       "      <td>0.098027</td>\n",
       "      <td>0.023069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37752</th>\n",
       "      <td>zone</td>\n",
       "      <td>0.179288</td>\n",
       "      <td>0.100752</td>\n",
       "      <td>0.141065</td>\n",
       "      <td>0.109328</td>\n",
       "      <td>0.126486</td>\n",
       "      <td>0.109752</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>0.127249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37753</th>\n",
       "      <td>zone</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.484714</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.049666</td>\n",
       "      <td>0.060091</td>\n",
       "      <td>0.087094</td>\n",
       "      <td>0.087863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37754</th>\n",
       "      <td>zoning</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.020992</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>0.127137</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>0.068175</td>\n",
       "      <td>0.011716</td>\n",
       "      <td>0.163682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37755</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.009278</td>\n",
       "      <td>0.165128</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.104209</td>\n",
       "      <td>0.029027</td>\n",
       "      <td>0.081399</td>\n",
       "      <td>0.023178</td>\n",
       "      <td>0.544599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37756</th>\n",
       "      <td>zoological</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37757</th>\n",
       "      <td>zoologist</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.612791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37758</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345957</td>\n",
       "      <td>0.165779</td>\n",
       "      <td>0.039704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391538</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.021665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37759</th>\n",
       "      <td>zoom</td>\n",
       "      <td>0.015118</td>\n",
       "      <td>0.080315</td>\n",
       "      <td>0.022272</td>\n",
       "      <td>0.030470</td>\n",
       "      <td>0.133275</td>\n",
       "      <td>0.173237</td>\n",
       "      <td>0.536272</td>\n",
       "      <td>0.009041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37760</th>\n",
       "      <td>zoom</td>\n",
       "      <td>0.101876</td>\n",
       "      <td>0.219795</td>\n",
       "      <td>0.053424</td>\n",
       "      <td>0.105393</td>\n",
       "      <td>0.084543</td>\n",
       "      <td>0.231974</td>\n",
       "      <td>0.114640</td>\n",
       "      <td>0.088355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37761</th>\n",
       "      <td>zoroastrian</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37762</th>\n",
       "      <td>zoster</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193803</td>\n",
       "      <td>0.224521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37763</th>\n",
       "      <td>zr</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>0.317017</td>\n",
       "      <td>0.084321</td>\n",
       "      <td>0.266152</td>\n",
       "      <td>0.123522</td>\n",
       "      <td>0.038465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37764</th>\n",
       "      <td>zu</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.029398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37765</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.726349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205299</td>\n",
       "      <td>0.068352</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37766</th>\n",
       "      <td>zulu</td>\n",
       "      <td>0.040057</td>\n",
       "      <td>0.195927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.186194</td>\n",
       "      <td>0.254680</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.015736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37767</th>\n",
       "      <td>zuni</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37768</th>\n",
       "      <td>zurich</td>\n",
       "      <td>0.052162</td>\n",
       "      <td>0.395481</td>\n",
       "      <td>0.061545</td>\n",
       "      <td>0.062029</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.138862</td>\n",
       "      <td>0.187169</td>\n",
       "      <td>0.063667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37769</th>\n",
       "      <td>zweig</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37770</th>\n",
       "      <td>zygote</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.032365</td>\n",
       "      <td>0.502798</td>\n",
       "      <td>0.098713</td>\n",
       "      <td>0.027961</td>\n",
       "      <td>0.023802</td>\n",
       "      <td>0.272262</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37771 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lemma#PoS    AFRAID    AMUSED     ANGRY   ANNOYED  DONT_CARE  \\\n",
       "0                0  0.087697  0.092489  0.154124  0.077188   0.136462   \n",
       "1                1  0.129418  0.108454  0.114674  0.114568   0.136147   \n",
       "2               10  0.130558  0.109739  0.115716  0.109559   0.151769   \n",
       "3              100  0.156990  0.126365  0.102937  0.087751   0.121395   \n",
       "4             1000  0.133133  0.186631  0.096675  0.060371   0.051486   \n",
       "5            10000  0.079639  0.114055  0.016001  0.005296   0.643330   \n",
       "6           1000th  0.000000  0.291220  0.000000  0.000000   0.000000   \n",
       "7            100th  0.308124  0.115666  0.100621  0.102881   0.037436   \n",
       "8            101st  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "9             10th  0.092306  0.066443  0.122701  0.086613   0.109534   \n",
       "10              11  0.180347  0.094039  0.114482  0.101375   0.117258   \n",
       "11           110th  0.000000  0.000000  0.000000  0.000000   0.857859   \n",
       "12           115th  0.095784  0.043246  0.080320  0.033227   0.082486   \n",
       "13            11th  0.102418  0.123678  0.090567  0.130365   0.167512   \n",
       "14              12  0.130229  0.106832  0.124114  0.142247   0.122799   \n",
       "15             120  0.137943  0.059509  0.133294  0.156163   0.100507   \n",
       "16           120th  0.000000  0.564393  0.000000  0.000000   0.000000   \n",
       "17            12th  0.081284  0.105454  0.105266  0.220674   0.119434   \n",
       "18              13  0.113830  0.113067  0.147606  0.139966   0.127601   \n",
       "19            13th  0.084568  0.103624  0.054698  0.142401   0.169072   \n",
       "20              14  0.109144  0.113042  0.123924  0.120082   0.145046   \n",
       "21             144  0.210856  0.034223  0.045845  0.212811   0.068370   \n",
       "22            14th  0.070650  0.062961  0.139161  0.138515   0.137800   \n",
       "23              15  0.141574  0.110872  0.133966  0.114720   0.119732   \n",
       "24           150th  0.000000  0.000000  0.684236  0.000000   0.000000   \n",
       "25            15th  0.061601  0.116090  0.155957  0.137201   0.164420   \n",
       "26              16  0.119421  0.111186  0.127524  0.117573   0.170526   \n",
       "27            16th  0.093073  0.148314  0.097487  0.166863   0.132453   \n",
       "28              17  0.135894  0.113842  0.144138  0.106563   0.105475   \n",
       "29            1728  0.094016  0.000000  0.091978  0.000000   0.188916   \n",
       "...            ...       ...       ...       ...       ...        ...   \n",
       "37741      zionist  0.149224  0.067373  0.062567  0.528923   0.000000   \n",
       "37742          zip  0.063739  0.115184  0.024072  0.094757   0.142529   \n",
       "37743          zip  0.039454  0.254250  0.007305  0.134146   0.184242   \n",
       "37744       zipper  0.090654  0.105729  0.004249  0.041031   0.391805   \n",
       "37745        zippy  0.023420  0.383111  0.000000  0.014986   0.436862   \n",
       "37746           zn  0.000000  0.204499  0.675232  0.000000   0.000000   \n",
       "37747       zodiac  0.261967  0.153140  0.159014  0.031348   0.060724   \n",
       "37748     zodiacal  0.000000  0.169362  0.000000  0.260252   0.000000   \n",
       "37749         zola  0.000000  1.000000  0.000000  0.000000   0.000000   \n",
       "37750       zombie  0.080676  0.350053  0.019235  0.057944   0.155826   \n",
       "37751        zonal  0.072611  0.020090  0.486750  0.020153   0.217128   \n",
       "37752         zone  0.179288  0.100752  0.141065  0.109328   0.126486   \n",
       "37753         zone  0.004461  0.484714  0.030800  0.195312   0.049666   \n",
       "37754       zoning  0.006965  0.020992  0.519584  0.127137   0.081749   \n",
       "37755          zoo  0.009278  0.165128  0.043182  0.104209   0.029027   \n",
       "37756   zoological  0.000000  0.000000  0.000000  1.000000   0.000000   \n",
       "37757    zoologist  0.000000  0.612791  0.000000  0.000000   0.000000   \n",
       "37758      zoology  0.000000  0.345957  0.165779  0.039704   0.000000   \n",
       "37759         zoom  0.015118  0.080315  0.022272  0.030470   0.133275   \n",
       "37760         zoom  0.101876  0.219795  0.053424  0.105393   0.084543   \n",
       "37761  zoroastrian  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37762       zoster  0.000000  0.479254  0.000000  0.193803   0.224521   \n",
       "37763           zr  0.000000  0.088415  0.082107  0.317017   0.084321   \n",
       "37764           zu  0.000000  0.942561  0.000000  0.000000   0.000000   \n",
       "37765     zucchini  0.000000  0.726349  0.000000  0.000000   0.000000   \n",
       "37766         zulu  0.040057  0.195927  0.000000  0.011110   0.186194   \n",
       "37767         zuni  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37768       zurich  0.052162  0.395481  0.061545  0.062029   0.039085   \n",
       "37769        zweig  0.125000  0.125000  0.125000  0.125000   0.125000   \n",
       "37770       zygote  0.031229  0.032365  0.502798  0.098713   0.027961   \n",
       "\n",
       "          HAPPY  INSPIRED       SAD  \n",
       "0      0.287111  0.084152  0.080778  \n",
       "1      0.161294  0.114729  0.120718  \n",
       "2      0.140957  0.111359  0.130342  \n",
       "3      0.109795  0.118774  0.175993  \n",
       "4      0.151350  0.120990  0.199363  \n",
       "5      0.050498  0.016048  0.075132  \n",
       "6      0.437545  0.271235  0.000000  \n",
       "7      0.118386  0.150157  0.066728  \n",
       "8      1.000000  0.000000  0.000000  \n",
       "9      0.181413  0.101312  0.239679  \n",
       "10     0.132364  0.096970  0.163165  \n",
       "11     0.102437  0.000000  0.039703  \n",
       "12     0.549403  0.040278  0.075256  \n",
       "13     0.161856  0.116179  0.107427  \n",
       "14     0.140741  0.102764  0.130273  \n",
       "15     0.125719  0.099549  0.187316  \n",
       "16     0.172776  0.262831  0.000000  \n",
       "17     0.145781  0.103076  0.119031  \n",
       "18     0.135690  0.088118  0.134123  \n",
       "19     0.211764  0.122687  0.111186  \n",
       "20     0.142700  0.104223  0.141839  \n",
       "21     0.093623  0.133566  0.200707  \n",
       "22     0.189092  0.119890  0.141929  \n",
       "23     0.141042  0.089399  0.148696  \n",
       "24     0.000000  0.000000  0.315764  \n",
       "25     0.147053  0.149305  0.068372  \n",
       "26     0.135229  0.090803  0.127738  \n",
       "27     0.160826  0.115615  0.085370  \n",
       "28     0.156021  0.098517  0.139551  \n",
       "29     0.225235  0.276742  0.123113  \n",
       "...         ...       ...       ...  \n",
       "37741  0.062997  0.128916  0.000000  \n",
       "37742  0.130776  0.296190  0.132752  \n",
       "37743  0.172760  0.202852  0.004991  \n",
       "37744  0.093548  0.181263  0.091721  \n",
       "37745  0.083081  0.016414  0.042127  \n",
       "37746  0.031301  0.000000  0.088968  \n",
       "37747  0.180759  0.108552  0.044497  \n",
       "37748  0.107396  0.157739  0.305251  \n",
       "37749  0.000000  0.000000  0.000000  \n",
       "37750  0.117568  0.169591  0.049107  \n",
       "37751  0.062171  0.098027  0.023069  \n",
       "37752  0.109752  0.106081  0.127249  \n",
       "37753  0.060091  0.087094  0.087863  \n",
       "37754  0.068175  0.011716  0.163682  \n",
       "37755  0.081399  0.023178  0.544599  \n",
       "37756  0.000000  0.000000  0.000000  \n",
       "37757  0.034841  0.000000  0.352367  \n",
       "37758  0.391538  0.035358  0.021665  \n",
       "37759  0.173237  0.536272  0.009041  \n",
       "37760  0.231974  0.114640  0.088355  \n",
       "37761  1.000000  0.000000  0.000000  \n",
       "37762  0.000000  0.000000  0.102421  \n",
       "37763  0.266152  0.123522  0.038465  \n",
       "37764  0.017552  0.010489  0.029398  \n",
       "37765  0.205299  0.068352  0.000000  \n",
       "37766  0.254680  0.296296  0.015736  \n",
       "37767  1.000000  0.000000  0.000000  \n",
       "37768  0.138862  0.187169  0.063667  \n",
       "37769  0.125000  0.125000  0.125000  \n",
       "37770  0.023802  0.272262  0.010870  \n",
       "\n",
       "[37771 rows x 9 columns]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depeche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma#PoS</th>\n",
       "      <th>AFRAID</th>\n",
       "      <th>AMUSED</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>ANNOYED</th>\n",
       "      <th>DONT_CARE</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>INSPIRED</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>115th</td>\n",
       "      <td>0.095784</td>\n",
       "      <td>0.043246</td>\n",
       "      <td>0.080320</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.082486</td>\n",
       "      <td>0.549403</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>0.075256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>55th</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546478</td>\n",
       "      <td>0.234474</td>\n",
       "      <td>0.219049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>a-line</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>abila</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067542</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.083031</td>\n",
       "      <td>0.038649</td>\n",
       "      <td>0.505564</td>\n",
       "      <td>0.286397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>absinthe</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732600</td>\n",
       "      <td>0.173359</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>absolutist</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.788576</td>\n",
       "      <td>0.042446</td>\n",
       "      <td>0.016196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>abstruse</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112527</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>0.308162</td>\n",
       "      <td>0.513133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>absurdly</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>aby</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>acacia</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043269</td>\n",
       "      <td>0.057065</td>\n",
       "      <td>0.604080</td>\n",
       "      <td>0.109819</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>accountancy</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>0.050546</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.149062</td>\n",
       "      <td>0.111601</td>\n",
       "      <td>0.542925</td>\n",
       "      <td>0.050867</td>\n",
       "      <td>0.066522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>accrued</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762661</td>\n",
       "      <td>0.237339</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>acoustician</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790788</td>\n",
       "      <td>0.209212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>activated</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>adapted</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>addendum</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189249</td>\n",
       "      <td>0.021968</td>\n",
       "      <td>0.145406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>adjoin</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>0.258047</td>\n",
       "      <td>0.591815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>adorned</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617772</td>\n",
       "      <td>0.101864</td>\n",
       "      <td>0.095163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>agronomist</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538176</td>\n",
       "      <td>0.461824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>alcazar</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535684</td>\n",
       "      <td>0.315845</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>alien</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231856</td>\n",
       "      <td>0.040469</td>\n",
       "      <td>0.075335</td>\n",
       "      <td>0.083120</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>aligning</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150907</td>\n",
       "      <td>0.124854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>alkaline</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623186</td>\n",
       "      <td>0.111204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>alt</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134151</td>\n",
       "      <td>0.083053</td>\n",
       "      <td>0.074962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624537</td>\n",
       "      <td>0.083297</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>amorphous</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596118</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.103314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>anachronism</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>0.118815</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>angled</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>anguilla</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768698</td>\n",
       "      <td>0.231302</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>anorexia</td>\n",
       "      <td>0.117303</td>\n",
       "      <td>0.026481</td>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524964</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.244234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>anti-semitic</td>\n",
       "      <td>0.058947</td>\n",
       "      <td>0.100071</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012348</td>\n",
       "      <td>0.564878</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.221008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36354</th>\n",
       "      <td>violated</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>0.051864</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>0.010839</td>\n",
       "      <td>0.664859</td>\n",
       "      <td>0.207412</td>\n",
       "      <td>0.030277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36365</th>\n",
       "      <td>viper</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36383</th>\n",
       "      <td>virtuoso</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303557</td>\n",
       "      <td>0.537294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36444</th>\n",
       "      <td>vocalization</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790788</td>\n",
       "      <td>0.209212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36486</th>\n",
       "      <td>voluptuous</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>0.130520</td>\n",
       "      <td>0.024608</td>\n",
       "      <td>0.027150</td>\n",
       "      <td>0.518953</td>\n",
       "      <td>0.284534</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36579</th>\n",
       "      <td>walk-in</td>\n",
       "      <td>0.016792</td>\n",
       "      <td>0.233040</td>\n",
       "      <td>0.020242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091115</td>\n",
       "      <td>0.524684</td>\n",
       "      <td>0.069599</td>\n",
       "      <td>0.044528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36669</th>\n",
       "      <td>warp</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049350</td>\n",
       "      <td>0.008587</td>\n",
       "      <td>0.133981</td>\n",
       "      <td>0.094129</td>\n",
       "      <td>0.577134</td>\n",
       "      <td>0.136820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36688</th>\n",
       "      <td>washable</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.061297</td>\n",
       "      <td>0.700790</td>\n",
       "      <td>0.164623</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36722</th>\n",
       "      <td>waterbird</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36861</th>\n",
       "      <td>weightiness</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36954</th>\n",
       "      <td>westinghouse</td>\n",
       "      <td>0.076095</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.025524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019659</td>\n",
       "      <td>0.514773</td>\n",
       "      <td>0.351545</td>\n",
       "      <td>0.008968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37006</th>\n",
       "      <td>whiplash</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37020</th>\n",
       "      <td>whisker</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37021</th>\n",
       "      <td>whiskered</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37027</th>\n",
       "      <td>whisperer</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010599</td>\n",
       "      <td>0.678947</td>\n",
       "      <td>0.310454</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37045</th>\n",
       "      <td>whitehead</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616986</td>\n",
       "      <td>0.383014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37157</th>\n",
       "      <td>wilt</td>\n",
       "      <td>0.149471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37193</th>\n",
       "      <td>windup</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.112446</td>\n",
       "      <td>0.023890</td>\n",
       "      <td>0.119222</td>\n",
       "      <td>0.646383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37253</th>\n",
       "      <td>wisteria</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616085</td>\n",
       "      <td>0.383915</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37295</th>\n",
       "      <td>woefully</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539655</td>\n",
       "      <td>0.094404</td>\n",
       "      <td>0.264581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37397</th>\n",
       "      <td>wormhole</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.759987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37399</th>\n",
       "      <td>worn</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37457</th>\n",
       "      <td>wren</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.840239</td>\n",
       "      <td>0.057066</td>\n",
       "      <td>0.025387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37651</th>\n",
       "      <td>yonder</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567984</td>\n",
       "      <td>0.432016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37673</th>\n",
       "      <td>yuletide</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.095187</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.108468</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.584068</td>\n",
       "      <td>0.191748</td>\n",
       "      <td>0.004628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37691</th>\n",
       "      <td>zap</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.504918</td>\n",
       "      <td>0.175433</td>\n",
       "      <td>0.190082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37701</th>\n",
       "      <td>zealotry</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710</th>\n",
       "      <td>zenith</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076636</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>0.202560</td>\n",
       "      <td>0.049908</td>\n",
       "      <td>0.527485</td>\n",
       "      <td>0.119577</td>\n",
       "      <td>0.006906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37711</th>\n",
       "      <td>zep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832877</td>\n",
       "      <td>0.167123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>zinsser</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706774</td>\n",
       "      <td>0.293226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lemma#PoS    AFRAID    AMUSED     ANGRY   ANNOYED  DONT_CARE  \\\n",
       "12            115th  0.095784  0.043246  0.080320  0.033227   0.082486   \n",
       "101            55th  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "128          a-line  0.000000  0.000000  0.480277  0.000000   0.000000   \n",
       "182           abila  0.000000  0.067542  0.018817  0.083031   0.038649   \n",
       "243        absinthe  0.000000  0.053181  0.000000  0.040860   0.000000   \n",
       "249      absolutist  0.021433  0.101608  0.000000  0.029740   0.000000   \n",
       "266        abstruse  0.000000  0.000000  0.112527  0.031033   0.308162   \n",
       "269        absurdly  0.000000  0.000000  0.330582  0.000000   0.000000   \n",
       "282             aby  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "287          acacia  0.000000  0.185767  0.000000  0.043269   0.057065   \n",
       "355     accountancy  0.020064  0.050546  0.008413  0.149062   0.111601   \n",
       "365         accrued  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "411     acoustician  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "448       activated  0.000000  0.000000  0.099021  0.000000   0.000000   \n",
       "490         adapted  0.000000  0.449538  0.000000  0.000000   0.000000   \n",
       "498        addendum  0.000000  0.189249  0.021968  0.145406   0.000000   \n",
       "538          adjoin  0.000000  0.135288  0.000000  0.014849   0.258047   \n",
       "595         adorned  0.000000  0.017138  0.000000  0.168064   0.000000   \n",
       "840      agronomist  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "953         alcazar  0.000000  0.000000  0.000000  0.148470   0.000000   \n",
       "991           alien  0.000000  0.231856  0.040469  0.075335   0.083120   \n",
       "1001       aligning  0.000000  0.000000  0.150907  0.124854   0.000000   \n",
       "1007       alkaline  0.000000  0.177037  0.000000  0.088573   0.000000   \n",
       "1105            alt  0.000000  0.134151  0.083053  0.074962   0.000000   \n",
       "1227      amorphous  0.000000  0.189980  0.000000  0.000000   0.000000   \n",
       "1268    anachronism  0.000000  0.000000  0.000000  0.196031   0.000000   \n",
       "1356         angled  0.000000  0.451574  0.000000  0.000000   0.000000   \n",
       "1367       anguilla  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "1445       anorexia  0.117303  0.026481  0.014755  0.008138   0.000000   \n",
       "1494   anti-semitic  0.058947  0.100071  0.040738  0.000000   0.012348   \n",
       "...             ...       ...       ...       ...       ...        ...   \n",
       "36354      violated  0.017966  0.051864  0.004916  0.011866   0.010839   \n",
       "36365         viper  0.000000  0.193818  0.000000  0.000000   0.000000   \n",
       "36383      virtuoso  0.000000  0.159148  0.000000  0.000000   0.303557   \n",
       "36444  vocalization  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "36486    voluptuous  0.000000  0.014234  0.130520  0.024608   0.027150   \n",
       "36579       walk-in  0.016792  0.233040  0.020242  0.000000   0.091115   \n",
       "36669          warp  0.000000  0.049350  0.008587  0.133981   0.094129   \n",
       "36688      washable  0.000000  0.032137  0.000000  0.041153   0.061297   \n",
       "36722     waterbird  0.000000  0.365668  0.000000  0.000000   0.000000   \n",
       "36861   weightiness  0.000000  0.347163  0.000000  0.000000   0.000000   \n",
       "36954  westinghouse  0.076095  0.003436  0.025524  0.000000   0.019659   \n",
       "37006      whiplash  0.000000  0.103029  0.000000  0.011309   0.000000   \n",
       "37020       whisker  0.000000  0.365668  0.000000  0.000000   0.000000   \n",
       "37021     whiskered  0.000000  0.365669  0.000000  0.000000   0.000000   \n",
       "37027     whisperer  0.000000  0.000000  0.000000  0.000000   0.010599   \n",
       "37045     whitehead  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37157          wilt  0.149471  0.000000  0.187090  0.000000   0.000000   \n",
       "37193        windup  0.029905  0.015547  0.112446  0.023890   0.119222   \n",
       "37253      wisteria  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37295      woefully  0.000000  0.101360  0.000000  0.000000   0.000000   \n",
       "37397      wormhole  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37399          worn  0.000000  0.267568  0.000000  0.000000   0.000000   \n",
       "37457          wren  0.019387  0.000000  0.018966  0.000000   0.038956   \n",
       "37651        yonder  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37673      yuletide  0.005890  0.095187  0.004939  0.108468   0.005072   \n",
       "37691           zap  0.000000  0.074567  0.000000  0.055000   0.000000   \n",
       "37701      zealotry  0.000000  0.347163  0.000000  0.000000   0.000000   \n",
       "37710        zenith  0.000000  0.076636  0.016928  0.202560   0.049908   \n",
       "37711           zep  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37737       zinsser  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "          HAPPY  INSPIRED       SAD  \n",
       "12     0.549403  0.040278  0.075256  \n",
       "101    0.546478  0.234474  0.219049  \n",
       "128    0.519723  0.000000  0.000000  \n",
       "182    0.505564  0.286397  0.000000  \n",
       "243    0.732600  0.173359  0.000000  \n",
       "249    0.788576  0.042446  0.016196  \n",
       "266    0.513133  0.000000  0.035144  \n",
       "269    0.669418  0.000000  0.000000  \n",
       "282    0.584599  0.000000  0.415401  \n",
       "287    0.604080  0.109819  0.000000  \n",
       "355    0.542925  0.050867  0.066522  \n",
       "365    0.762661  0.237339  0.000000  \n",
       "411    0.790788  0.209212  0.000000  \n",
       "448    0.712611  0.000000  0.188368  \n",
       "490    0.550462  0.000000  0.000000  \n",
       "498    0.622793  0.000000  0.020583  \n",
       "538    0.591815  0.000000  0.000000  \n",
       "595    0.617772  0.101864  0.095163  \n",
       "840    0.538176  0.461824  0.000000  \n",
       "953    0.535684  0.315845  0.000000  \n",
       "991    0.503538  0.000000  0.065682  \n",
       "1001   0.653543  0.000000  0.070696  \n",
       "1007   0.623186  0.111204  0.000000  \n",
       "1105   0.624537  0.083297  0.000000  \n",
       "1227   0.596118  0.110589  0.103314  \n",
       "1268   0.685154  0.118815  0.000000  \n",
       "1356   0.548426  0.000000  0.000000  \n",
       "1367   0.768698  0.231302  0.000000  \n",
       "1445   0.524964  0.064125  0.244234  \n",
       "1494   0.564878  0.002010  0.221008  \n",
       "...         ...       ...       ...  \n",
       "36354  0.664859  0.207412  0.030277  \n",
       "36365  0.806182  0.000000  0.000000  \n",
       "36383  0.537294  0.000000  0.000000  \n",
       "36444  0.790788  0.209212  0.000000  \n",
       "36486  0.518953  0.284534  0.000000  \n",
       "36579  0.524684  0.069599  0.044528  \n",
       "36669  0.577134  0.136820  0.000000  \n",
       "36688  0.700790  0.164623  0.000000  \n",
       "36722  0.634332  0.000000  0.000000  \n",
       "36861  0.652837  0.000000  0.000000  \n",
       "36954  0.514773  0.351545  0.008968  \n",
       "37006  0.796017  0.000000  0.089646  \n",
       "37020  0.634332  0.000000  0.000000  \n",
       "37021  0.634331  0.000000  0.000000  \n",
       "37027  0.678947  0.310454  0.000000  \n",
       "37045  0.616986  0.383014  0.000000  \n",
       "37157  0.663438  0.000000  0.000000  \n",
       "37193  0.646383  0.000000  0.052607  \n",
       "37253  0.616085  0.383915  0.000000  \n",
       "37295  0.539655  0.094404  0.264581  \n",
       "37397  0.759987  0.000000  0.240013  \n",
       "37399  0.732432  0.000000  0.000000  \n",
       "37457  0.840239  0.057066  0.025387  \n",
       "37651  0.567984  0.432016  0.000000  \n",
       "37673  0.584068  0.191748  0.004628  \n",
       "37691  0.504918  0.175433  0.190082  \n",
       "37701  0.652837  0.000000  0.000000  \n",
       "37710  0.527485  0.119577  0.006906  \n",
       "37711  0.832877  0.167123  0.000000  \n",
       "37737  0.706774  0.293226  0.000000  \n",
       "\n",
       "[791 rows x 9 columns]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depeche[(depeche['HAPPY']>.5) & (depeche['HAPPY']<1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma#PoS</th>\n",
       "      <th>AFRAID</th>\n",
       "      <th>AMUSED</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>ANNOYED</th>\n",
       "      <th>DONT_CARE</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>INSPIRED</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0#n</td>\n",
       "      <td>0.091040</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>0.167116</td>\n",
       "      <td>0.065456</td>\n",
       "      <td>0.135221</td>\n",
       "      <td>0.291064</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.083239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1#n</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.116173</td>\n",
       "      <td>0.118003</td>\n",
       "      <td>0.119131</td>\n",
       "      <td>0.138837</td>\n",
       "      <td>0.164305</td>\n",
       "      <td>0.105911</td>\n",
       "      <td>0.116004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10#n</td>\n",
       "      <td>0.131860</td>\n",
       "      <td>0.118659</td>\n",
       "      <td>0.121393</td>\n",
       "      <td>0.111911</td>\n",
       "      <td>0.138485</td>\n",
       "      <td>0.143611</td>\n",
       "      <td>0.106803</td>\n",
       "      <td>0.127277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100#n</td>\n",
       "      <td>0.170863</td>\n",
       "      <td>0.128193</td>\n",
       "      <td>0.104132</td>\n",
       "      <td>0.093903</td>\n",
       "      <td>0.107457</td>\n",
       "      <td>0.117811</td>\n",
       "      <td>0.106983</td>\n",
       "      <td>0.170658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000#n</td>\n",
       "      <td>0.147027</td>\n",
       "      <td>0.142151</td>\n",
       "      <td>0.076382</td>\n",
       "      <td>0.065647</td>\n",
       "      <td>0.060772</td>\n",
       "      <td>0.155713</td>\n",
       "      <td>0.094969</td>\n",
       "      <td>0.257339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000#n</td>\n",
       "      <td>0.094633</td>\n",
       "      <td>0.172659</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.586478</td>\n",
       "      <td>0.046655</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>0.075156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000th#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413456</td>\n",
       "      <td>0.249691</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100th#a</td>\n",
       "      <td>0.320738</td>\n",
       "      <td>0.146574</td>\n",
       "      <td>0.118702</td>\n",
       "      <td>0.081067</td>\n",
       "      <td>0.027685</td>\n",
       "      <td>0.114831</td>\n",
       "      <td>0.120825</td>\n",
       "      <td>0.069577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>101st#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10th#a</td>\n",
       "      <td>0.096738</td>\n",
       "      <td>0.064845</td>\n",
       "      <td>0.122668</td>\n",
       "      <td>0.099643</td>\n",
       "      <td>0.123646</td>\n",
       "      <td>0.186206</td>\n",
       "      <td>0.097122</td>\n",
       "      <td>0.209132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11#n</td>\n",
       "      <td>0.177897</td>\n",
       "      <td>0.096849</td>\n",
       "      <td>0.113096</td>\n",
       "      <td>0.099720</td>\n",
       "      <td>0.122297</td>\n",
       "      <td>0.137492</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>0.160386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>110th#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.841477</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>115th#a</td>\n",
       "      <td>0.099843</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.075163</td>\n",
       "      <td>0.031544</td>\n",
       "      <td>0.086844</td>\n",
       "      <td>0.553967</td>\n",
       "      <td>0.032245</td>\n",
       "      <td>0.076891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11th#a</td>\n",
       "      <td>0.104773</td>\n",
       "      <td>0.096564</td>\n",
       "      <td>0.110022</td>\n",
       "      <td>0.139173</td>\n",
       "      <td>0.155790</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.114702</td>\n",
       "      <td>0.119677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12#n</td>\n",
       "      <td>0.128105</td>\n",
       "      <td>0.113075</td>\n",
       "      <td>0.128914</td>\n",
       "      <td>0.135942</td>\n",
       "      <td>0.126543</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.091179</td>\n",
       "      <td>0.128798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>120#n</td>\n",
       "      <td>0.139767</td>\n",
       "      <td>0.068873</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>0.148782</td>\n",
       "      <td>0.100386</td>\n",
       "      <td>0.134019</td>\n",
       "      <td>0.092969</td>\n",
       "      <td>0.182512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120th#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182925</td>\n",
       "      <td>0.220941</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12th#a</td>\n",
       "      <td>0.081232</td>\n",
       "      <td>0.093718</td>\n",
       "      <td>0.120537</td>\n",
       "      <td>0.195558</td>\n",
       "      <td>0.130101</td>\n",
       "      <td>0.167340</td>\n",
       "      <td>0.101080</td>\n",
       "      <td>0.110434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13#n</td>\n",
       "      <td>0.117911</td>\n",
       "      <td>0.114335</td>\n",
       "      <td>0.145873</td>\n",
       "      <td>0.140897</td>\n",
       "      <td>0.123827</td>\n",
       "      <td>0.139510</td>\n",
       "      <td>0.084955</td>\n",
       "      <td>0.132693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13th#a</td>\n",
       "      <td>0.119130</td>\n",
       "      <td>0.118748</td>\n",
       "      <td>0.065316</td>\n",
       "      <td>0.163622</td>\n",
       "      <td>0.165593</td>\n",
       "      <td>0.158735</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>0.104504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14#n</td>\n",
       "      <td>0.118779</td>\n",
       "      <td>0.114322</td>\n",
       "      <td>0.124769</td>\n",
       "      <td>0.120012</td>\n",
       "      <td>0.140301</td>\n",
       "      <td>0.148692</td>\n",
       "      <td>0.097289</td>\n",
       "      <td>0.135835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>144#n</td>\n",
       "      <td>0.220330</td>\n",
       "      <td>0.026865</td>\n",
       "      <td>0.092837</td>\n",
       "      <td>0.196366</td>\n",
       "      <td>0.089387</td>\n",
       "      <td>0.102002</td>\n",
       "      <td>0.108862</td>\n",
       "      <td>0.163350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14th#a</td>\n",
       "      <td>0.075327</td>\n",
       "      <td>0.071715</td>\n",
       "      <td>0.143336</td>\n",
       "      <td>0.130565</td>\n",
       "      <td>0.177195</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.103293</td>\n",
       "      <td>0.120350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15#n</td>\n",
       "      <td>0.153667</td>\n",
       "      <td>0.106859</td>\n",
       "      <td>0.129874</td>\n",
       "      <td>0.114932</td>\n",
       "      <td>0.116199</td>\n",
       "      <td>0.141770</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.150182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150th#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15th#a</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.099977</td>\n",
       "      <td>0.160995</td>\n",
       "      <td>0.146957</td>\n",
       "      <td>0.153771</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.134384</td>\n",
       "      <td>0.079173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16#n</td>\n",
       "      <td>0.129046</td>\n",
       "      <td>0.121888</td>\n",
       "      <td>0.129867</td>\n",
       "      <td>0.114224</td>\n",
       "      <td>0.143428</td>\n",
       "      <td>0.145155</td>\n",
       "      <td>0.085415</td>\n",
       "      <td>0.130978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16th#a</td>\n",
       "      <td>0.079148</td>\n",
       "      <td>0.148972</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>0.150740</td>\n",
       "      <td>0.138293</td>\n",
       "      <td>0.165665</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.108250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17#n</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>0.116387</td>\n",
       "      <td>0.143710</td>\n",
       "      <td>0.106345</td>\n",
       "      <td>0.112504</td>\n",
       "      <td>0.162394</td>\n",
       "      <td>0.086611</td>\n",
       "      <td>0.132743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1728#n</td>\n",
       "      <td>0.102359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207744</td>\n",
       "      <td>0.237208</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>0.131382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37741</th>\n",
       "      <td>zionist#n</td>\n",
       "      <td>0.211496</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.079609</td>\n",
       "      <td>0.467741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063622</td>\n",
       "      <td>0.085382</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37742</th>\n",
       "      <td>zip#n</td>\n",
       "      <td>0.067417</td>\n",
       "      <td>0.118385</td>\n",
       "      <td>0.023838</td>\n",
       "      <td>0.125216</td>\n",
       "      <td>0.051532</td>\n",
       "      <td>0.215228</td>\n",
       "      <td>0.337812</td>\n",
       "      <td>0.060572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37743</th>\n",
       "      <td>zip#v</td>\n",
       "      <td>0.054779</td>\n",
       "      <td>0.262540</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.138454</td>\n",
       "      <td>0.230067</td>\n",
       "      <td>0.126386</td>\n",
       "      <td>0.172871</td>\n",
       "      <td>0.007835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37744</th>\n",
       "      <td>zipper#n</td>\n",
       "      <td>0.142038</td>\n",
       "      <td>0.138458</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.057806</td>\n",
       "      <td>0.244998</td>\n",
       "      <td>0.080788</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.134416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37745</th>\n",
       "      <td>zippy#a</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.397049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.434066</td>\n",
       "      <td>0.074455</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>0.047344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37746</th>\n",
       "      <td>zn#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214270</td>\n",
       "      <td>0.658172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37747</th>\n",
       "      <td>zodiac#n</td>\n",
       "      <td>0.218149</td>\n",
       "      <td>0.178216</td>\n",
       "      <td>0.159950</td>\n",
       "      <td>0.040204</td>\n",
       "      <td>0.047437</td>\n",
       "      <td>0.213275</td>\n",
       "      <td>0.090269</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37748</th>\n",
       "      <td>zodiacal#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112345</td>\n",
       "      <td>0.131013</td>\n",
       "      <td>0.323566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37749</th>\n",
       "      <td>zola#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37750</th>\n",
       "      <td>zombie#n</td>\n",
       "      <td>0.062554</td>\n",
       "      <td>0.304678</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.049255</td>\n",
       "      <td>0.177890</td>\n",
       "      <td>0.210793</td>\n",
       "      <td>0.129538</td>\n",
       "      <td>0.049859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37751</th>\n",
       "      <td>zonal#a</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.466977</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.201730</td>\n",
       "      <td>0.074319</td>\n",
       "      <td>0.101059</td>\n",
       "      <td>0.028351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37752</th>\n",
       "      <td>zone#n</td>\n",
       "      <td>0.192437</td>\n",
       "      <td>0.105927</td>\n",
       "      <td>0.140548</td>\n",
       "      <td>0.112391</td>\n",
       "      <td>0.105143</td>\n",
       "      <td>0.111882</td>\n",
       "      <td>0.095674</td>\n",
       "      <td>0.135997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37753</th>\n",
       "      <td>zone#v</td>\n",
       "      <td>0.080743</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>0.116389</td>\n",
       "      <td>0.068036</td>\n",
       "      <td>0.061397</td>\n",
       "      <td>0.106752</td>\n",
       "      <td>0.272045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37754</th>\n",
       "      <td>zoning#n</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>0.017420</td>\n",
       "      <td>0.397968</td>\n",
       "      <td>0.081404</td>\n",
       "      <td>0.061824</td>\n",
       "      <td>0.046326</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>0.384878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37755</th>\n",
       "      <td>zoo#n</td>\n",
       "      <td>0.015593</td>\n",
       "      <td>0.142238</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.125548</td>\n",
       "      <td>0.061472</td>\n",
       "      <td>0.088704</td>\n",
       "      <td>0.032166</td>\n",
       "      <td>0.457595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37756</th>\n",
       "      <td>zoological#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37757</th>\n",
       "      <td>zoologist#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37758</th>\n",
       "      <td>zoology#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329135</td>\n",
       "      <td>0.247484</td>\n",
       "      <td>0.075135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208538</td>\n",
       "      <td>0.085841</td>\n",
       "      <td>0.053867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37759</th>\n",
       "      <td>zoom#n</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.198976</td>\n",
       "      <td>0.052892</td>\n",
       "      <td>0.069552</td>\n",
       "      <td>0.130371</td>\n",
       "      <td>0.121173</td>\n",
       "      <td>0.381961</td>\n",
       "      <td>0.033366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37760</th>\n",
       "      <td>zoom#v</td>\n",
       "      <td>0.142619</td>\n",
       "      <td>0.201149</td>\n",
       "      <td>0.047917</td>\n",
       "      <td>0.088449</td>\n",
       "      <td>0.088214</td>\n",
       "      <td>0.195550</td>\n",
       "      <td>0.135623</td>\n",
       "      <td>0.100478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37761</th>\n",
       "      <td>zoroastrian#a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37762</th>\n",
       "      <td>zoster#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182689</td>\n",
       "      <td>0.234714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37763</th>\n",
       "      <td>zr#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092446</td>\n",
       "      <td>0.079865</td>\n",
       "      <td>0.312830</td>\n",
       "      <td>0.092277</td>\n",
       "      <td>0.278944</td>\n",
       "      <td>0.102788</td>\n",
       "      <td>0.040850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37764</th>\n",
       "      <td>zu#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037808</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>0.064168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37765</th>\n",
       "      <td>zucchini#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225280</td>\n",
       "      <td>0.077417</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37766</th>\n",
       "      <td>zulu#n</td>\n",
       "      <td>0.052390</td>\n",
       "      <td>0.247290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.129114</td>\n",
       "      <td>0.421433</td>\n",
       "      <td>0.124081</td>\n",
       "      <td>0.020174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37767</th>\n",
       "      <td>zuni#n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37768</th>\n",
       "      <td>zurich#n</td>\n",
       "      <td>0.058066</td>\n",
       "      <td>0.350413</td>\n",
       "      <td>0.096722</td>\n",
       "      <td>0.072960</td>\n",
       "      <td>0.053989</td>\n",
       "      <td>0.153212</td>\n",
       "      <td>0.140109</td>\n",
       "      <td>0.074530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37769</th>\n",
       "      <td>zweig#n</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37770</th>\n",
       "      <td>zygote#n</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>0.503211</td>\n",
       "      <td>0.107001</td>\n",
       "      <td>0.031009</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>0.244664</td>\n",
       "      <td>0.013727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37771 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Lemma#PoS    AFRAID    AMUSED     ANGRY   ANNOYED  DONT_CARE  \\\n",
       "0                0#n  0.091040  0.101889  0.167116  0.065456   0.135221   \n",
       "1                1#n  0.121637  0.116173  0.118003  0.119131   0.138837   \n",
       "2               10#n  0.131860  0.118659  0.121393  0.111911   0.138485   \n",
       "3              100#n  0.170863  0.128193  0.104132  0.093903   0.107457   \n",
       "4             1000#n  0.147027  0.142151  0.076382  0.065647   0.060772   \n",
       "5            10000#n  0.094633  0.172659  0.011131  0.003737   0.586478   \n",
       "6           1000th#a  0.000000  0.336853  0.000000  0.000000   0.000000   \n",
       "7            100th#a  0.320738  0.146574  0.118702  0.081067   0.027685   \n",
       "8            101st#a  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "9             10th#a  0.096738  0.064845  0.122668  0.099643   0.123646   \n",
       "10              11#n  0.177897  0.096849  0.113096  0.099720   0.122297   \n",
       "11           110th#a  0.000000  0.000000  0.000000  0.000000   0.841477   \n",
       "12           115th#a  0.099843  0.043502  0.075163  0.031544   0.086844   \n",
       "13            11th#a  0.104773  0.096564  0.110022  0.139173   0.155790   \n",
       "14              12#n  0.128105  0.113075  0.128914  0.135942   0.126543   \n",
       "15             120#n  0.139767  0.068873  0.132692  0.148782   0.100386   \n",
       "16           120th#a  0.000000  0.596134  0.000000  0.000000   0.000000   \n",
       "17            12th#a  0.081232  0.093718  0.120537  0.195558   0.130101   \n",
       "18              13#n  0.117911  0.114335  0.145873  0.140897   0.123827   \n",
       "19            13th#a  0.119130  0.118748  0.065316  0.163622   0.165593   \n",
       "20              14#n  0.118779  0.114322  0.124769  0.120012   0.140301   \n",
       "21             144#n  0.220330  0.026865  0.092837  0.196366   0.089387   \n",
       "22            14th#a  0.075327  0.071715  0.143336  0.130565   0.177195   \n",
       "23              15#n  0.153667  0.106859  0.129874  0.114932   0.116199   \n",
       "24           150th#a  0.000000  0.000000  0.664956  0.000000   0.000000   \n",
       "25            15th#a  0.073514  0.099977  0.160995  0.146957   0.153771   \n",
       "26              16#n  0.129046  0.121888  0.129867  0.114224   0.143428   \n",
       "27            16th#a  0.079148  0.148972  0.115399  0.150740   0.138293   \n",
       "28              17#n  0.139306  0.116387  0.143710  0.106345   0.112504   \n",
       "29            1728#n  0.102359  0.000000  0.089901  0.000000   0.207744   \n",
       "...              ...       ...       ...       ...       ...        ...   \n",
       "37741      zionist#n  0.211496  0.092150  0.079609  0.467741   0.000000   \n",
       "37742          zip#n  0.067417  0.118385  0.023838  0.125216   0.051532   \n",
       "37743          zip#v  0.054779  0.262540  0.007069  0.138454   0.230067   \n",
       "37744       zipper#n  0.142038  0.138458  0.006343  0.057806   0.244998   \n",
       "37745        zippy#a  0.021697  0.397049  0.000000  0.013710   0.434066   \n",
       "37746           zn#n  0.000000  0.214270  0.658172  0.000000   0.000000   \n",
       "37747       zodiac#n  0.218149  0.178216  0.159950  0.040204   0.047437   \n",
       "37748     zodiacal#a  0.000000  0.176748  0.000000  0.256329   0.000000   \n",
       "37749         zola#n  0.000000  1.000000  0.000000  0.000000   0.000000   \n",
       "37750       zombie#n  0.062554  0.304678  0.015433  0.049255   0.177890   \n",
       "37751        zonal#a  0.092033  0.019248  0.466977  0.016283   0.201730   \n",
       "37752         zone#n  0.192437  0.105927  0.140548  0.112391   0.105143   \n",
       "37753         zone#v  0.080743  0.273743  0.020895  0.116389   0.068036   \n",
       "37754       zoning#n  0.004442  0.017420  0.397968  0.081404   0.061824   \n",
       "37755          zoo#n  0.015593  0.142238  0.076682  0.125548   0.061472   \n",
       "37756   zoological#a  0.000000  0.000000  0.000000  1.000000   0.000000   \n",
       "37757    zoologist#n  0.000000  0.495878  0.000000  0.000000   0.000000   \n",
       "37758      zoology#n  0.000000  0.329135  0.247484  0.075135   0.000000   \n",
       "37759         zoom#n  0.011710  0.198976  0.052892  0.069552   0.130371   \n",
       "37760         zoom#v  0.142619  0.201149  0.047917  0.088449   0.088214   \n",
       "37761  zoroastrian#a  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37762       zoster#n  0.000000  0.478689  0.000000  0.182689   0.234714   \n",
       "37763           zr#n  0.000000  0.092446  0.079865  0.312830   0.092277   \n",
       "37764           zu#n  0.000000  0.880085  0.000000  0.000000   0.000000   \n",
       "37765     zucchini#n  0.000000  0.697303  0.000000  0.000000   0.000000   \n",
       "37766         zulu#n  0.052390  0.247290  0.000000  0.005517   0.129114   \n",
       "37767         zuni#n  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "37768       zurich#n  0.058066  0.350413  0.096722  0.072960   0.053989   \n",
       "37769        zweig#n  0.125000  0.125000  0.125000  0.125000   0.125000   \n",
       "37770       zygote#n  0.035650  0.034949  0.503211  0.107001   0.031009   \n",
       "\n",
       "          HAPPY  INSPIRED       SAD  \n",
       "0      0.291064  0.064975  0.083239  \n",
       "1      0.164305  0.105911  0.116004  \n",
       "2      0.143611  0.106803  0.127277  \n",
       "3      0.117811  0.106983  0.170658  \n",
       "4      0.155713  0.094969  0.257339  \n",
       "5      0.046655  0.009551  0.075156  \n",
       "6      0.413456  0.249691  0.000000  \n",
       "7      0.114831  0.120825  0.069577  \n",
       "8      1.000000  0.000000  0.000000  \n",
       "9      0.186206  0.097122  0.209132  \n",
       "10     0.137492  0.092261  0.160386  \n",
       "11     0.113821  0.000000  0.044702  \n",
       "12     0.553967  0.032245  0.076891  \n",
       "13     0.159300  0.114702  0.119677  \n",
       "14     0.147444  0.091179  0.128798  \n",
       "15     0.134019  0.092969  0.182512  \n",
       "16     0.182925  0.220941  0.000000  \n",
       "17     0.167340  0.101080  0.110434  \n",
       "18     0.139510  0.084955  0.132693  \n",
       "19     0.158735  0.104352  0.104504  \n",
       "20     0.148692  0.097289  0.135835  \n",
       "21     0.102002  0.108862  0.163350  \n",
       "22     0.178218  0.103293  0.120350  \n",
       "23     0.141770  0.086517  0.150182  \n",
       "24     0.000000  0.000000  0.335044  \n",
       "25     0.151228  0.134384  0.079173  \n",
       "26     0.145155  0.085415  0.130978  \n",
       "27     0.165665  0.093533  0.108250  \n",
       "28     0.162394  0.086611  0.132743  \n",
       "29     0.237208  0.231407  0.131382  \n",
       "...         ...       ...       ...  \n",
       "37741  0.063622  0.085382  0.000000  \n",
       "37742  0.215228  0.337812  0.060572  \n",
       "37743  0.126386  0.172871  0.007835  \n",
       "37744  0.080788  0.195154  0.134416  \n",
       "37745  0.074455  0.011679  0.047344  \n",
       "37746  0.032875  0.000000  0.094683  \n",
       "37747  0.213275  0.090269  0.052500  \n",
       "37748  0.112345  0.131013  0.323566  \n",
       "37749  0.000000  0.000000  0.000000  \n",
       "37750  0.210793  0.129538  0.049859  \n",
       "37751  0.074319  0.101059  0.028351  \n",
       "37752  0.111882  0.095674  0.135997  \n",
       "37753  0.061397  0.106752  0.272045  \n",
       "37754  0.046326  0.005739  0.384878  \n",
       "37755  0.088704  0.032166  0.457595  \n",
       "37756  0.000000  0.000000  0.000000  \n",
       "37757  0.028530  0.000000  0.475592  \n",
       "37758  0.208538  0.085841  0.053867  \n",
       "37759  0.121173  0.381961  0.033366  \n",
       "37760  0.195550  0.135623  0.100478  \n",
       "37761  1.000000  0.000000  0.000000  \n",
       "37762  0.000000  0.000000  0.103907  \n",
       "37763  0.278944  0.102788  0.040850  \n",
       "37764  0.037808  0.017940  0.064168  \n",
       "37765  0.225280  0.077417  0.000000  \n",
       "37766  0.421433  0.124081  0.020174  \n",
       "37767  1.000000  0.000000  0.000000  \n",
       "37768  0.153212  0.140109  0.074530  \n",
       "37769  0.125000  0.125000  0.125000  \n",
       "37770  0.029789  0.244664  0.013727  \n",
       "\n",
       "[37771 rows x 9 columns]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depeche_freq=pd.read_csv(\"data/DepecheMood_V1.0/DepecheMood_freq.txt\", sep='\\t')\n",
    "depeche_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma#PoS</th>\n",
       "      <th>AFRAID</th>\n",
       "      <th>AMUSED</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>ANNOYED</th>\n",
       "      <th>DONT_CARE</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>INSPIRED</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>absinthe#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761721</td>\n",
       "      <td>0.143115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>accrued#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807161</td>\n",
       "      <td>0.192839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>acoustician#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826408</td>\n",
       "      <td>0.173592</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>adhd#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182276</td>\n",
       "      <td>0.817724</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>adios#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043214</td>\n",
       "      <td>0.835120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>adorned#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.094897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.759185</td>\n",
       "      <td>0.035171</td>\n",
       "      <td>0.041934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>alca#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121257</td>\n",
       "      <td>0.878743</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>alcove#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>allegorical#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>altruistic#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197831</td>\n",
       "      <td>0.802169</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>amaranth#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.830637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>ancient#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073088</td>\n",
       "      <td>0.926912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>anesthesia#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060009</td>\n",
       "      <td>0.838699</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>anesthesiology#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>0.921838</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>anguilla#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807161</td>\n",
       "      <td>0.192839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>anselm#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.060176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092326</td>\n",
       "      <td>0.847498</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>antechamber#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075617</td>\n",
       "      <td>0.924383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>antelope#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034747</td>\n",
       "      <td>0.965253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>antichrist#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.147007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090219</td>\n",
       "      <td>0.762775</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>antiviral#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.830637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>aram#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027526</td>\n",
       "      <td>0.046541</td>\n",
       "      <td>0.776770</td>\n",
       "      <td>0.109673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>argent#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.830637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>arguing#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>0.921838</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>armless#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144631</td>\n",
       "      <td>0.855369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>artemia#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>astounded#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.056456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.771197</td>\n",
       "      <td>0.021383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>athenian#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034747</td>\n",
       "      <td>0.965253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>atresia#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060009</td>\n",
       "      <td>0.838699</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>autosomal#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144631</td>\n",
       "      <td>0.855369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>avarice#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35606</th>\n",
       "      <td>unrewarded#a</td>\n",
       "      <td>0.03001</td>\n",
       "      <td>0.026151</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.037925</td>\n",
       "      <td>0.052206</td>\n",
       "      <td>0.036110</td>\n",
       "      <td>0.794747</td>\n",
       "      <td>0.011556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35641</th>\n",
       "      <td>unseen#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245432</td>\n",
       "      <td>0.754568</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35691</th>\n",
       "      <td>unsullied#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.125419</td>\n",
       "      <td>0.813230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35794</th>\n",
       "      <td>updike#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.788392</td>\n",
       "      <td>0.211608</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35871</th>\n",
       "      <td>urbanisation#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35988</th>\n",
       "      <td>vainglorious#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121257</td>\n",
       "      <td>0.878743</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36015</th>\n",
       "      <td>valuable#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.830637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36023</th>\n",
       "      <td>vamp#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.226625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36083</th>\n",
       "      <td>vaughan#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807161</td>\n",
       "      <td>0.192839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36127</th>\n",
       "      <td>vending#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.196976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36176</th>\n",
       "      <td>verdure#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013124</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102553</td>\n",
       "      <td>0.822015</td>\n",
       "      <td>0.040277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36297</th>\n",
       "      <td>vieques#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807161</td>\n",
       "      <td>0.192839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36444</th>\n",
       "      <td>vocalization#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826408</td>\n",
       "      <td>0.173592</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36576</th>\n",
       "      <td>walesa#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097798</td>\n",
       "      <td>0.890587</td>\n",
       "      <td>0.011615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36828</th>\n",
       "      <td>wedded#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>0.783712</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36897</th>\n",
       "      <td>well-done#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119111</td>\n",
       "      <td>0.771113</td>\n",
       "      <td>0.109777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36919</th>\n",
       "      <td>well-spoken#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049193</td>\n",
       "      <td>0.020645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056786</td>\n",
       "      <td>0.823052</td>\n",
       "      <td>0.050323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37129</th>\n",
       "      <td>wildcat#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150682</td>\n",
       "      <td>0.849318</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37177</th>\n",
       "      <td>winder#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37202</th>\n",
       "      <td>wineskin#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034747</td>\n",
       "      <td>0.965253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37236</th>\n",
       "      <td>wiretap#v</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073088</td>\n",
       "      <td>0.926912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37280</th>\n",
       "      <td>witwatersrand#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.830637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37310</th>\n",
       "      <td>womanly#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202376</td>\n",
       "      <td>0.797624</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37318</th>\n",
       "      <td>wonderfulness#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>0.022272</td>\n",
       "      <td>0.074775</td>\n",
       "      <td>0.077199</td>\n",
       "      <td>0.035598</td>\n",
       "      <td>0.764376</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>woolen#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.906214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37380</th>\n",
       "      <td>worksheet#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200129</td>\n",
       "      <td>0.756964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37397</th>\n",
       "      <td>wormhole#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37498</th>\n",
       "      <td>wrongheaded#a</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.830637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37711</th>\n",
       "      <td>zep#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821551</td>\n",
       "      <td>0.178449</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>zinsser#n</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752216</td>\n",
       "      <td>0.247784</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lemma#PoS   AFRAID    AMUSED     ANGRY   ANNOYED  DONT_CARE  \\\n",
       "243          absinthe#n  0.00000  0.055164  0.000000  0.040001   0.000000   \n",
       "365           accrued#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "411       acoustician#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "525              adhd#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "533             adios#n  0.00000  0.000000  0.121665  0.000000   0.000000   \n",
       "595           adorned#a  0.00000  0.094897  0.000000  0.068813   0.000000   \n",
       "951              alca#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "960            alcove#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1029      allegorical#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1129       altruistic#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1141         amaranth#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1313          ancient#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1332       anesthesia#n  0.00000  0.000000  0.000000  0.101292   0.000000   \n",
       "1334   anesthesiology#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1367         anguilla#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1448           anselm#n  0.00000  0.060176  0.000000  0.000000   0.000000   \n",
       "1470      antechamber#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1472         antelope#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1502       antichrist#n  0.00000  0.147007  0.000000  0.000000   0.000000   \n",
       "1535        antiviral#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1729             aram#n  0.00000  0.027577  0.011912  0.000000   0.027526   \n",
       "1800           argent#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1812          arguing#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1852          armless#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "1909          artemia#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "2086        astounded#a  0.00000  0.056456  0.000000  0.000000   0.000000   \n",
       "2126         athenian#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "2154          atresia#n  0.00000  0.000000  0.000000  0.101292   0.000000   \n",
       "2327        autosomal#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "2341          avarice#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "...                 ...      ...       ...       ...       ...        ...   \n",
       "35606      unrewarded#a  0.03001  0.026151  0.011296  0.037925   0.052206   \n",
       "35641          unseen#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "35691       unsullied#a  0.00000  0.000000  0.000000  0.000000   0.061350   \n",
       "35794          updike#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "35871    urbanisation#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "35988    vainglorious#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36015        valuable#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36023            vamp#n  0.00000  0.226625  0.000000  0.000000   0.000000   \n",
       "36083         vaughan#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36127         vending#n  0.00000  0.196976  0.000000  0.000000   0.000000   \n",
       "36176         verdure#n  0.00000  0.000000  0.013124  0.022031   0.000000   \n",
       "36297         vieques#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36444    vocalization#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36576          walesa#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36828          wedded#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36897       well-done#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "36919     well-spoken#a  0.00000  0.000000  0.049193  0.020645   0.000000   \n",
       "37129         wildcat#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37177          winder#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37202        wineskin#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37236         wiretap#v  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37280   witwatersrand#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37310         womanly#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37318   wonderfulness#n  0.00000  0.025780  0.022272  0.074775   0.077199   \n",
       "37344          woolen#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37380       worksheet#n  0.00000  0.042908  0.000000  0.000000   0.000000   \n",
       "37397        wormhole#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37498     wrongheaded#a  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37711             zep#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "37737         zinsser#n  0.00000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "          HAPPY  INSPIRED       SAD  \n",
       "243    0.761721  0.143115  0.000000  \n",
       "365    0.807161  0.192839  0.000000  \n",
       "411    0.826408  0.173592  0.000000  \n",
       "525    0.182276  0.817724  0.000000  \n",
       "533    0.043214  0.835120  0.000000  \n",
       "595    0.759185  0.035171  0.041934  \n",
       "951    0.121257  0.878743  0.000000  \n",
       "960    0.093786  0.906214  0.000000  \n",
       "1029   0.093786  0.906214  0.000000  \n",
       "1129   0.197831  0.802169  0.000000  \n",
       "1141   0.169363  0.830637  0.000000  \n",
       "1313   0.073088  0.926912  0.000000  \n",
       "1332   0.060009  0.838699  0.000000  \n",
       "1334   0.078162  0.921838  0.000000  \n",
       "1367   0.807161  0.192839  0.000000  \n",
       "1448   0.092326  0.847498  0.000000  \n",
       "1470   0.075617  0.924383  0.000000  \n",
       "1472   0.034747  0.965253  0.000000  \n",
       "1502   0.090219  0.762775  0.000000  \n",
       "1535   0.169363  0.830637  0.000000  \n",
       "1729   0.046541  0.776770  0.109673  \n",
       "1800   0.169363  0.830637  0.000000  \n",
       "1812   0.078162  0.921838  0.000000  \n",
       "1852   0.144631  0.855369  0.000000  \n",
       "1909   0.093786  0.906214  0.000000  \n",
       "2086   0.150964  0.771197  0.021383  \n",
       "2126   0.034747  0.965253  0.000000  \n",
       "2154   0.060009  0.838699  0.000000  \n",
       "2327   0.144631  0.855369  0.000000  \n",
       "2341   0.093786  0.906214  0.000000  \n",
       "...         ...       ...       ...  \n",
       "35606  0.036110  0.794747  0.011556  \n",
       "35641  0.245432  0.754568  0.000000  \n",
       "35691  0.125419  0.813230  0.000000  \n",
       "35794  0.788392  0.211608  0.000000  \n",
       "35871  0.093786  0.906214  0.000000  \n",
       "35988  0.121257  0.878743  0.000000  \n",
       "36015  0.169363  0.830637  0.000000  \n",
       "36023  0.773375  0.000000  0.000000  \n",
       "36083  0.807161  0.192839  0.000000  \n",
       "36127  0.803024  0.000000  0.000000  \n",
       "36176  0.102553  0.822015  0.040277  \n",
       "36297  0.807161  0.192839  0.000000  \n",
       "36444  0.826408  0.173592  0.000000  \n",
       "36576  0.097798  0.890587  0.011615  \n",
       "36828  0.216288  0.783712  0.000000  \n",
       "36897  0.119111  0.771113  0.109777  \n",
       "36919  0.056786  0.823052  0.050323  \n",
       "37129  0.150682  0.849318  0.000000  \n",
       "37177  0.093786  0.906214  0.000000  \n",
       "37202  0.034747  0.965253  0.000000  \n",
       "37236  0.073088  0.926912  0.000000  \n",
       "37280  0.169363  0.830637  0.000000  \n",
       "37310  0.202376  0.797624  0.000000  \n",
       "37318  0.035598  0.764376  0.000000  \n",
       "37344  0.093786  0.906214  0.000000  \n",
       "37380  0.200129  0.756964  0.000000  \n",
       "37397  0.757568  0.000000  0.242432  \n",
       "37498  0.169363  0.830637  0.000000  \n",
       "37711  0.821551  0.178449  0.000000  \n",
       "37737  0.752216  0.247784  0.000000  \n",
       "\n",
       "[474 rows x 9 columns]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depeche_freq[((depeche_freq['HAPPY']>.75) & (depeche_freq['HAPPY']<1.0)) | ((depeche_freq['INSPIRED']>.75) & (depeche_freq['INSPIRED']<1.0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Getting 'happy' words programmatically has proved haphazard. I will use an API and then manually remove words that don't fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting joy words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_words=['joy', 'joyful', 'happy', 'happiness', 'excitement', 'amusement', 'bliss', 'charm', 'cheer', 'comfort', 'delight', 'elation', 'glee', 'humor', 'satisfaction', 'wonder']\n",
    "adjectives=[]\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "word_types=[adjectives, nouns, verbs]\n",
    "for word in compare_words:\n",
    "    r=requests.get('http://words.bighugelabs.com/api/2/050617425e370b0aa06ac00be6ed9cc9/'+word+'/json')\n",
    "    js=r.json()\n",
    "    for index, word_type in enumerate(['adjective', 'noun', 'verb']):\n",
    "        if word_type in js:\n",
    "            if 'rel' in js[word_type]:\n",
    "                for word in js[word_type]['rel']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'sim' in js[word_type]:\n",
    "                for word in js[word_type]['sim']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'syn' in js[word_type]:\n",
    "                for word in js[word_type]['syn']:\n",
    "                    word_types[index].append(word)\n",
    "adjectives=list(set(adjectives))\n",
    "nouns=list(set(nouns))\n",
    "verbs=list(set(verbs))\n",
    "bad_nouns=['damages', 'body substance', 'redress', 'spoken language', 'subject matter', 'liquid body substance', 'inflammation', 'speech communication', 'psychological state', 'substance', 'happening', 'voice communication', 'condition', 'physical object', 'positive stimulus']\n",
    "better_nouns=[noun for noun in nouns if noun not in bad_nouns]\n",
    "joy_words=adjectives+better_nouns+verbs\n",
    "len(joy_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting sadness words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words=['sad', 'sadness', 'dreary', 'bitter', 'sorrow', 'wistful', 'heartbroken', 'melancholy', 'mournful', 'pessimistic', 'somber', 'dismal']\n",
    "adjectives=[]\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "word_types=[adjectives, nouns, verbs]\n",
    "for word in compare_words:\n",
    "    r=requests.get('http://words.bighugelabs.com/api/2/050617425e370b0aa06ac00be6ed9cc9/'+word+'/json')\n",
    "    js=r.json()\n",
    "    for index, word_type in enumerate(['adjective', 'noun', 'verb']):\n",
    "        if word_type in js:\n",
    "            if 'rel' in js[word_type]:\n",
    "                for word in js[word_type]['rel']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'sim' in js[word_type]:\n",
    "                for word in js[word_type]['sim']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'syn' in js[word_type]:\n",
    "                for word in js[word_type]['syn']:\n",
    "                    word_types[index].append(word)\n",
    "adjectives=list(set(adjectives))\n",
    "nouns=list(set(nouns))\n",
    "verbs=list(set(verbs))\n",
    "bad_nouns=['negative stimulus', 'feeling', 'gustatory perception', 'taste perception', 'taste property' ,'gustatory sensation', 'humour', 'liquid body substance', 'taste sensation', 'bodily fluid', 'body fluid']\n",
    "better_nouns=[noun for noun in nouns if noun not in bad_nouns]\n",
    "if 'change taste' in verbs:\n",
    "    verbs.remove('change taste')\n",
    "sad_words=adjectives+better_nouns+verbs\n",
    "len(sad_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting anger words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words=['anger', 'angry', 'mad', 'fury', 'furious', 'hatred', 'annoyance', 'annoying', 'outrage', 'irritation', 'ire', 'rage']\n",
    "adjectives=[]\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "word_types=[adjectives, nouns, verbs]\n",
    "for word in compare_words:\n",
    "    r=requests.get('http://words.bighugelabs.com/api/2/050617425e370b0aa06ac00be6ed9cc9/'+word+'/json')\n",
    "    js=r.json()\n",
    "    for index, word_type in enumerate(['adjective', 'noun', 'verb']):\n",
    "        if word_type in js:\n",
    "            if 'rel' in js[word_type]:\n",
    "                for word in js[word_type]['rel']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'sim' in js[word_type]:\n",
    "                for word in js[word_type]['sim']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'syn' in js[word_type]:\n",
    "                for word in js[word_type]['syn']:\n",
    "                    word_types[index].append(word)\n",
    "adjectives=list(set(adjectives))\n",
    "nouns=list(set(nouns))\n",
    "verbs=list(set(verbs))\n",
    "bad_nouns=['negative stimulus', 'fashion', 'manic disorder', 'hysteria', 'intensity' ,'fierceness', 'craze', 'emotion', 'fad', 'abnormalcy', 'mythical creature']\n",
    "better_nouns=[noun for noun in nouns if noun not in bad_nouns]\n",
    "angry_words=adjectives+better_nouns+verbs\n",
    "len(angry_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting fearful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words=['fear', 'fearful', 'fright', 'frightened', 'afraid', 'despair', 'dread', 'horrified', 'panic', 'terror', 'terrified', 'worry', 'unease', 'uneasiness']\n",
    "adjectives=[]\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "word_types=[adjectives, nouns, verbs]\n",
    "for word in compare_words:\n",
    "    r=requests.get('http://words.bighugelabs.com/api/2/050617425e370b0aa06ac00be6ed9cc9/'+word+'/json')\n",
    "    js=r.json()\n",
    "    for index, word_type in enumerate(['adjective', 'noun', 'verb']):\n",
    "        if word_type in js:\n",
    "            if 'rel' in js[word_type]:\n",
    "                for word in js[word_type]['rel']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'sim' in js[word_type]:\n",
    "                for word in js[word_type]['sim']:\n",
    "                    word_types[index].append(word)\n",
    "            if 'syn' in js[word_type]:\n",
    "                for word in js[word_type]['syn']:\n",
    "                    word_types[index].append(word)\n",
    "adjectives=list(set(adjectives))\n",
    "nouns=list(set(nouns))\n",
    "verbs=list(set(verbs))\n",
    "bad_nouns=['negative stimulus', 'someone', 'feeling', 'status', 'veneration' ,'individual', 'monkey', 'brat', 'emotion', 'person', 'awe']\n",
    "better_nouns=[noun for noun in nouns if noun not in bad_nouns]\n",
    "fear_words=adjectives+better_nouns+verbs\n",
    "len(fear_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " 'no',\n",
       " 'neither',\n",
       " 'nor',\n",
       " 'but',\n",
       " 'however',\n",
       " 'although',\n",
       " 'nonetheless',\n",
       " 'despite',\n",
       " 'except',\n",
       " 'even though',\n",
       " 'yet']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions={'anger':angry_words, 'fear':fear_words, 'joy':joy_words, 'sadness':sad_words}\n",
    "negative_words=['not', 'no', 'neither', 'nor']\n",
    "emotion_opp={'anger':'joy', 'fear':'joy', 'joy':'sadness', 'sadness':'joy'}\n",
    "negative_X=[]\n",
    "negative_y=[]\n",
    "for emotion in emotions:\n",
    "    for neg_word in negative_words:\n",
    "        for emot_word in emotions[emotion]:\n",
    "            negative_X.append(neg_word+' '+emot_word+' '+neg_word+' '+emot_word+' '+neg_word+' '+emot_word+' '+neg_word+' '+emot_word)\n",
    "            negative_y.append(emotion_opp[emotion])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_X=[]\n",
    "not_y=[]\n",
    "no_X=[]\n",
    "no_y=[]\n",
    "\n",
    "lists=[not_X, not_y, no_X, no_y]\n",
    "words={'not': [not_X, not_y], 'no': [no_X, no_y]}\n",
    "for syn in test:\n",
    "    for word in words:\n",
    "        words[word][0].append(word+' '+syn+' '+word+' '+syn+' '+word+' '+syn+' '+word+' '+syn)\n",
    "        words[word][1].append('sadness')\n",
    "not_X=pd.Series(not_X)\n",
    "not_y=pd.Series(not_y)\n",
    "no_X=pd.Series(no_X)\n",
    "no_y=pd.Series(no_y)\n",
    "negation_X=not_X.append(no_X)\n",
    "negation_y=not_y.append(no_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_t6_negation_0(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1 = []\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(6, len(doc)):\n",
    "            X_1.append(doc[i-6:i])\n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_t6_negation(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_word)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1 = []\n",
    "    y_1 = []\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        for i in range(6, len(doc)):\n",
    "            X_1.append(doc[i-6:i])\n",
    "            y_1.append(negative_y1[index])\n",
    "    return X_1, y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy=np.array([0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad=np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " array([0, 0, 1, 0]),\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_y1=[]\n",
    "for emotion in negative_y:\n",
    "    if emotion=='joy':\n",
    "        negative_y1.append(joy)\n",
    "    else:\n",
    "        negative_y1.append(sad)\n",
    "negative_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_X1, negative_y1=np.array(transform_t6_negation(negative_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "negated_X=transform_t6_negation_0(negation_X)\n",
    "negated_X=np.array(negated_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "sads=[]\n",
    "for _ in range(len(negated_X)):\n",
    "    sads.append(sad)\n",
    "negated_y=np.array(sads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 0.1254 - acc: 0.7016\n",
      "Epoch 2/25\n",
      "620/620 [==============================] - 0s 523us/step - loss: 0.0091 - acc: 0.9823\n",
      "Epoch 3/25\n",
      "620/620 [==============================] - 0s 521us/step - loss: 0.0060 - acc: 0.9903\n",
      "Epoch 4/25\n",
      "620/620 [==============================] - 0s 508us/step - loss: 0.0050 - acc: 0.9935\n",
      "Epoch 5/25\n",
      "620/620 [==============================] - 0s 511us/step - loss: 0.0048 - acc: 0.9935\n",
      "Epoch 6/25\n",
      "620/620 [==============================] - 0s 511us/step - loss: 0.0042 - acc: 0.9935\n",
      "Epoch 7/25\n",
      "620/620 [==============================] - 0s 505us/step - loss: 0.0041 - acc: 0.9935\n",
      "Epoch 8/25\n",
      "620/620 [==============================] - 0s 458us/step - loss: 0.0038 - acc: 0.9935\n",
      "Epoch 9/25\n",
      "620/620 [==============================] - 0s 463us/step - loss: 0.0037 - acc: 0.9935\n",
      "Epoch 10/25\n",
      "620/620 [==============================] - 0s 453us/step - loss: 0.0038 - acc: 0.9935\n",
      "Epoch 11/25\n",
      "620/620 [==============================] - 0s 444us/step - loss: 0.0039 - acc: 0.9952\n",
      "Epoch 12/25\n",
      "620/620 [==============================] - 0s 453us/step - loss: 0.0031 - acc: 0.9952\n",
      "Epoch 13/25\n",
      "620/620 [==============================] - 0s 456us/step - loss: 0.0031 - acc: 0.9968\n",
      "Epoch 14/25\n",
      "620/620 [==============================] - 0s 445us/step - loss: 0.0028 - acc: 0.9968\n",
      "Epoch 15/25\n",
      "620/620 [==============================] - 0s 450us/step - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 16/25\n",
      "620/620 [==============================] - 0s 455us/step - loss: 0.0026 - acc: 0.9968\n",
      "Epoch 17/25\n",
      "620/620 [==============================] - 0s 447us/step - loss: 0.0027 - acc: 0.9968\n",
      "Epoch 18/25\n",
      "620/620 [==============================] - 0s 448us/step - loss: 0.0024 - acc: 0.9968\n",
      "Epoch 19/25\n",
      "620/620 [==============================] - 0s 460us/step - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 20/25\n",
      "620/620 [==============================] - 0s 466us/step - loss: 0.0024 - acc: 0.9968\n",
      "Epoch 21/25\n",
      "620/620 [==============================] - 0s 456us/step - loss: 0.0022 - acc: 0.9968\n",
      "Epoch 22/25\n",
      "620/620 [==============================] - 0s 444us/step - loss: 0.0022 - acc: 0.9984\n",
      "Epoch 23/25\n",
      "620/620 [==============================] - 0s 456us/step - loss: 0.0023 - acc: 0.9968\n",
      "Epoch 24/25\n",
      "620/620 [==============================] - 0s 444us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 25/25\n",
      "620/620 [==============================] - 0s 440us/step - loss: 0.0020 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6aaad940>"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_emotions.fit(negated_X, negated_y, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.8905169e-03,  1.7428547e-03, -2.0447250e-04,  9.9890691e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('I am not not happy', four_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (5688, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-e50dc3bd44ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfour_emotions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_X1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_y1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (5688, 1)"
     ]
    }
   ],
   "source": [
    "four_emotions.fit(negative_X1, negative_y1, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Further options</h3>\n",
    "\n",
    "I could \n",
    "\n",
    "1.Gather and combine many emotional labelled datasets to have larger training data\n",
    "\n",
    "2.Create new training data synthetically\n",
    "\n",
    "3.Create labelled data using word averages (auto-labelling) data\n",
    "\n",
    "4.Use Indico to get around 10,000 more labelled training data rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Idea:</strong>Duplicate input if length of input too short\n",
    "\n",
    "<strong>Idea:</strong>Branch out to several models depending on input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Indico's emotions</strong>:\n",
    "Anger, Fear, Joy, Sadness, Surprise\n",
    "\n",
    "<strong>ISEAR's emotions</strong>: Anger, Fear, Joy, Sadness, Disgust, Shame, Guilt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four emotions:\n",
    "Anger, Fear, Joy, Sadness\n",
    "\n",
    "Disgust goes with Anger, Surprise goes with Fear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plan for API</h3>\n",
    "\n",
    "1.For now, retrain model with just four emotions. Use ISEAR and Semeval for better accuracy. Redeploy with good formatting\n",
    "\n",
    "2.For the future, \n",
    "Download facebook data tagged with 'Wow' emoji predominantly for 'surprise' (This results in a low yield of actually surprised texts, about 150/ every 4000)\n",
    "To use Indico to find the surprising texts, in order to get to around 1000 surprising texts it will cost around $150. If we don't use multiple fake accounts.\n",
    "\n",
    "Alternative option: Get Facebook scraper approved in order to download facebook comments from \"wow\" stories (might have a higher yield of surprised texts)\n",
    "\n",
    "Continue looking for source of texts labelled with surprise (couldn't find yet)\n",
    "\n",
    "Redo four_emotions model with categorical cross entropy loss to get probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Input shape\n",
    "\n",
    "3D tensor with shape (batch_size, timesteps, input_dim)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:anger\n",
    "\n",
    "1:disgust\n",
    "\n",
    "2:fear\n",
    "\n",
    "3:guilt\n",
    "\n",
    "4: joy\n",
    "\n",
    "5:sadness\n",
    "\n",
    "6:shame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[anger, fear, joy, sadness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_json(text, model):\n",
    "    vec_text = transform_t6_input(text)\n",
    "    pred = model.predict(vec_text)\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    anger=pred[0]\n",
    "    fear=pred[1]\n",
    "    joy=pred[2]\n",
    "    sadness=pred[3]\n",
    "    response_dict={'anger': anger, 'fear': fear, 'joy': joy, 'sadness': sadness, 'surprise': 0}\n",
    "    return jsonify(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_str(text, model):\n",
    "    vec_text = transform_t6_input(text)\n",
    "    pred = model.predict(vec_text)\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    anger=pred[0]\n",
    "    fear=pred[1]\n",
    "    joy=pred[2]\n",
    "    sadness=pred[3]\n",
    "    response_dict = {'anger': anger, 'fear': fear, 'joy': joy, 'sadness': sadness, 'surprise': 0}\n",
    "    #return jsonify(response_dict)\n",
    "    return str(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(8, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'anger': 0.31061664, 'fear': 0.037538864, 'joy': 0.5914509, 'sadness': 0.06033083, 'surprise': 0}\""
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_str('I like pie', four_emotions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
