{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach to emulate:\n",
    "\n",
    "1.Input Layer\n",
    "\n",
    "2.Embedding layer\n",
    "\n",
    "3.BLSTM layer, with element-wise sum of forward/backward pass outputs\\\n",
    "\n",
    "Classification should have dropout  applied on the embedding layer, LSTM layer and penultimate layer. as Well as L2 regularization\n",
    "\n",
    "4.Attention Layer\n",
    "\n",
    "5.Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"]='theano'\n",
    "os.environ[\"KERAS_BACKEND\"]='tensorflow'\n",
    "import keras\n",
    "keras.backend.backend()\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU, Input, TimeDistributed\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "import lmdb\n",
    "from lmdb_embeddings.reader import LmdbEmbeddingsReader\n",
    "import lmdb_embeddings.exceptions as exceptions\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theano\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "        shape=(input_shape[-1],),\n",
    "        initializer='normal',\n",
    "        trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  \n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "        shape=(input_shape[-1],),\n",
    "        initializer='normal',\n",
    "        trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  \n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(np.dot(x, self.W))\n",
    "        ai = K.exp(eij)\n",
    "        product=tf.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        weights = ai/product \n",
    "        weighted_input = x*weights\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()\n",
    "classifier.add(Bidirectional(GRU(units=25, return_sequences=True), input_shape=(6,300)))\n",
    "classifier.add(AttLayer())\n",
    "classifier.add(Dropout(0.3))\n",
    "classifier.add(Dense(units=4, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=load_model('models/emotions_blstm_att_tf.h5', custom_objects={'AttLayer': AttLayer()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=LmdbEmbeddingsReader('data/lmdb_databases')\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/isear_plus_semeval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['not', 'no', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                         'even though', 'yet']\n",
    "stop = list(set(stopwords.words('english')))\n",
    "for neg in negative:\n",
    "    for stopword in stop:\n",
    "        if stopword==neg:\n",
    "            stop.remove(stopword)\n",
    "rm=['don\\'t', 'shouldn\\'t', 'doesn\\'t', 'didn\\'t']\n",
    "for r in rm:\n",
    "    stop.remove(r)\n",
    "exclude = set(string.punctuation)\n",
    "exclude.add('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join([ch for ch in stop_free if ch not in exclude])\n",
    "    re.sub(r'\\n', '', punc_free)\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_words(li):\n",
    "    total_vecs=[]\n",
    "    for word in li:\n",
    "        try:\n",
    "            vector = embeddings.get_word_vector(word)\n",
    "        except exceptions.MissingWordError:\n",
    "            # 'google' is not in the database.\n",
    "            vector= np.zeros(300, dtype='float32')\n",
    "        total_vecs.append(vector)\n",
    "    return np.array(total_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    y_1=np_utils.to_categorical(y)\n",
    "    #y_1=np.reshape(y_1, (-1, 4, 1))\n",
    "    return y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_splits(series):\n",
    "    word_splits=series.str.split(' ')\n",
    "    return word_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_6(X,y=None):\n",
    "    X=pd.Series(X).apply(clean).apply(input_duplicator_train)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_words)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1=[]\n",
    "    y_1=[]\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        print(len(doc))\n",
    "        for i in range(6, len(doc)):\n",
    "            X_1.append(doc[i-6:i])\n",
    "            if y is not None:\n",
    "                y_1.append(y.iloc[index])\n",
    "                y_1=transform_y(y_1)\n",
    "    if y is not None:\n",
    "        return np.array(X_1), np.array(y_1)\n",
    "    else:\n",
    "        return np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_duplicator_train(text):\n",
    "    splits=text.split(' ')\n",
    "    while len(splits)<7:\n",
    "        orig_doc=splits.copy()\n",
    "        for word in orig_doc:\n",
    "            splits.append(word)\n",
    "    return ' '.join(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_duplicator(text):\n",
    "    X=pd.Series(text).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits\n",
    "    num_docs=len(numbers_series)\n",
    "    for index, doc in enumerate(numbers_series):\n",
    "        while len(doc)<7:\n",
    "            orig_doc=doc.copy()\n",
    "            orig_doc=list(orig_doc)\n",
    "            doc=list(doc)\n",
    "            for word in orig_doc:\n",
    "                doc.append(word)\n",
    "                #doc=np.insert(doc,(len(doc)),word, axis=0)\n",
    "                #doc=np.append(doc, word, axis=1)\n",
    "            modified=True\n",
    "        numbers_series.iloc[index]=np.array(doc)\n",
    "    X_1 = []\n",
    "    if num_docs>1:\n",
    "        for index in range(0, num_docs):\n",
    "            doc=numbers_series.iloc[index]\n",
    "            for i in range(6, len(doc)):\n",
    "                X_1.append(doc[i-6:i])\n",
    "    else:\n",
    "        doc=numbers_series.iloc[0]\n",
    "        print(doc.shape)\n",
    "        for i in range(6, len(doc)):\n",
    "                X_1.append(doc[i-6:i])\n",
    "    return X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    X=transform(text)\n",
    "    prediction=classifier.predict(X)\n",
    "    prediction=np.mean(prediction, axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, y_1=transform(data['1'], data['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous best with theano\n",
    "classifier.evaluate(X_test, y_test)\n",
    "#Theano training is many, many times slower than tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Tensorflow\n",
    "classifier.evaluate(X_test, y_test)\n",
    "#Comparable results to without attention. This needs better implementation, closer to the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('''I do not happy''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('models/emotions_blstm_att_tf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hierachical Attention Network With Buckets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Segmentation\n",
    "text='Sentence #one... I hope it picks this up. Sentence LMFAO two! Sentence three?'\n",
    "tokens=nlp(text)\n",
    "for s in tokens.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bucketed_sequence import BucketedSequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from absl import app\n",
    "\n",
    "UNK = np.zeros(300)\n",
    "#FLAGS = flags.FLAGS\n",
    "\n",
    "'''flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
    "flags.DEFINE_integer('epochs', 20, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('lstm_units', 50, 'Number of LSTM units in RNN')\n",
    "flags.DEFINE_integer('dense_breadth', 64, 'Number of neurons in the dense ' +\n",
    "                     'layer')\n",
    "\n",
    "flags.DEFINE_integer('dataset_size', 4726, 'Size of training dataset')\n",
    "flags.DEFINE_integer('val_size', 1182, 'Size of validation set')\n",
    "flags.DEFINE_integer('buckets', 4, 'Number of buckets to use (run with ' +\n",
    "                     '0 to disable)')'''\n",
    "\n",
    "'''flags.DEFINE_integer('seqlen_mean', 50, 'Sequence length mean (drawn ' +\n",
    "                     'from normal distribution)')\n",
    "flags.DEFINE_integer('seqlen_stddev', 200, 'Sequence length standard ' +\n",
    "                     'deviation (drawn from normal distribution)')'''\n",
    "\n",
    "batch_size=64\n",
    "epochs=100\n",
    "lstm_units=25\n",
    "dense_breadth=64\n",
    "buckets=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple network (GRU + Dense)\n",
    "inp = Input(shape=(None, 300), dtype=\"float32\", name=\"in\")\n",
    "lstm = Bidirectional(GRU(lstm_units, return_sequences=False,\n",
    "            name=\"lstm\"))(inp)\n",
    "dense = Dense(dense_breadth, kernel_initializer='normal',\n",
    "              activation='relu')(lstm)\n",
    "outputs = Dense(4, kernel_initializer='normal')(dense)\n",
    "model = Model(inputs=inp, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(seqs, maxlen):\n",
    "    # NOTE: prepends data\n",
    "    padded = np.array(pad_sequences(seqs, maxlen=maxlen, value=UNK, \n",
    "                                    dtype=seqs[0].dtype))\n",
    "    return np.vstack([np.expand_dims(x, axis=0) for x in padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model):\n",
    "    # Prepare data\n",
    "    X,y=transform(data['1'], data['0'])\n",
    "    len_train=[x.shape[0] for x in X[:4000]]\n",
    "    len_val=[x.shape[0] for x in X[4000:]]\n",
    "    sequence_lengths = [x.shape[0] for x in X]\n",
    "    X = pad(X, np.max(sequence_lengths))\n",
    "    X_train=X[:4000]\n",
    "    X_test=X[4000:]\n",
    "    y_train=y[:4000]\n",
    "    y_test=y[4000:]\n",
    "    if buckets > 0:\n",
    "        # Create Sequence objects\n",
    "        train_generator = BucketedSequence(buckets, batch_size,\n",
    "                                           len_train, X_train, y_train)\n",
    "        val_generator = BucketedSequence(buckets, batch_size,\n",
    "                                         len_val, X_test, y_test)\n",
    "\n",
    "        model.fit_generator(train_generator, epochs=epochs,\n",
    "                            validation_data=val_generator,\n",
    "                            shuffle=False, verbose=True)\n",
    "    else:\n",
    "        # No bucketing\n",
    "        model.fit(x=x_train, y=y_train, epochs=epochs,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  batch_size=batch_size, verbose=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=transform(data['1'], data['0'])\n",
    "len_train=[x.shape[0] for x in X[:4000]]\n",
    "len_val=[x.shape[0] for x in X[4000:]]\n",
    "sequence_lengths = [x.shape[0] for x in X]\n",
    "X = pad(X, np.max(sequence_lengths))\n",
    "X_train=X[:4000]\n",
    "X_test=X[4000:]\n",
    "y_train=y[:4000]\n",
    "y_test=y[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 54, 300)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X,y=None):\n",
    "    X=pd.Series(X).apply(clean)\n",
    "    splits=word_splits(X)\n",
    "    numbers_series=splits.apply(vec_words)\n",
    "    num_docs=len(numbers_series)\n",
    "    X_1=[]\n",
    "    y_1=[]\n",
    "    for index in range(0, num_docs):\n",
    "        doc=numbers_series.iloc[index]\n",
    "        X_doc=[]\n",
    "        for word in doc:\n",
    "            X_doc.append(word)\n",
    "        X_1.append(np.array(X_doc))\n",
    "        if y is not None:\n",
    "            y_1.append(y.iloc[index])\n",
    "        \n",
    "    if y is not None:\n",
    "        y_1=transform_y(y_1)\n",
    "        return np.array(X_1), np.array(y_1)\n",
    "    else:\n",
    "        return np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5908, 54, 300)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_lengths = [x.shape[0] for x in X]\n",
    "padded_x = pad(X, 54)\n",
    "padded_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sizes, bucket_ranges = np.histogram(sequence_lengths,\n",
    "                                                   bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 4 non-empty buckets\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "bucket_ranges\n",
    "x_seq=X_train\n",
    "input_shape = (1,) if len(x_seq.shape) == 2 else x_seq.shape[2:]\n",
    "output_shape = (1,) if len(y.shape) == 1 else y.shape[1:]\n",
    "actual_buckets = [bucket_ranges[i+1] \n",
    "                          for i,bs in enumerate(bucket_sizes) if bs > 0]\n",
    "actual_bucketsizes = [bs for bs in bucket_sizes if bs > 0]\n",
    "bucket_seqlen = [int(math.ceil(bs)) for bs in actual_buckets]\n",
    "num_actual = len(actual_buckets)\n",
    "print('Training with %d non-empty buckets' % num_actual)\n",
    "#print(bucket_seqlen)\n",
    "#print(actual_bucketsizes)\n",
    "bins = [(np.ndarray([bs, bsl] + list(input_shape), dtype=x_seq.dtype),\n",
    "              np.ndarray([bs] + list(output_shape), dtype=y.dtype)) \n",
    "             for bsl,bs in zip(bucket_seqlen, actual_bucketsizes)]\n",
    "assert len(bins) == num_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 28, 41, 54]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4716, 1082, 97, 13]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_bucketsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 4716, 300]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[14,4716]+list(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 4 non-empty buckets\n",
      "Training with 4 non-empty buckets\n",
      "Epoch 1/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 11.7223 - acc: 0.18 - ETA: 1s - loss: 11.9340 - acc: 0.22 - ETA: 1s - loss: 12.2881 - acc: 0.22 - ETA: 0s - loss: 12.2155 - acc: 0.22 - ETA: 0s - loss: 12.2992 - acc: 0.22 - ETA: 0s - loss: 12.1223 - acc: 0.22 - ETA: 0s - loss: 12.2588 - acc: 0.22 - ETA: 0s - loss: 12.3457 - acc: 0.22 - ETA: 0s - loss: 12.2610 - acc: 0.22 - ETA: 0s - loss: 12.2072 - acc: 0.22 - ETA: 0s - loss: 12.2965 - acc: 0.22 - ETA: 0s - loss: 12.3253 - acc: 0.21 - ETA: 0s - loss: 12.3659 - acc: 0.22 - ETA: 0s - loss: 12.3748 - acc: 0.22 - ETA: 0s - loss: 12.3480 - acc: 0.22 - ETA: 0s - loss: 12.3430 - acc: 0.23 - ETA: 0s - loss: 12.2925 - acc: 0.23 - ETA: 0s - loss: 12.2677 - acc: 0.23 - ETA: 0s - loss: 12.2104 - acc: 0.23 - ETA: 0s - loss: 12.1734 - acc: 0.23 - ETA: 0s - loss: 12.0970 - acc: 0.23 - 1s 23ms/step - loss: 12.1583 - acc: 0.2317 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 2/100\n",
      "65/65 [==============================] - ETA: 3s - loss: 11.7223 - acc: 0.18 - ETA: 2s - loss: 7.3493 - acc: 0.1491 - ETA: 2s - loss: 9.1946 - acc: 0.223 - ETA: 1s - loss: 9.4458 - acc: 0.238 - ETA: 1s - loss: 9.8372 - acc: 0.244 - ETA: 1s - loss: 10.0863 - acc: 0.24 - ETA: 1s - loss: 10.3556 - acc: 0.24 - ETA: 1s - loss: 10.6679 - acc: 0.25 - ETA: 1s - loss: 10.9269 - acc: 0.23 - ETA: 0s - loss: 11.0575 - acc: 0.22 - ETA: 0s - loss: 11.1628 - acc: 0.23 - ETA: 0s - loss: 11.3878 - acc: 0.23 - ETA: 0s - loss: 11.4719 - acc: 0.23 - ETA: 0s - loss: 11.5763 - acc: 0.23 - ETA: 0s - loss: 11.6816 - acc: 0.23 - ETA: 0s - loss: 11.7961 - acc: 0.23 - ETA: 0s - loss: 11.7619 - acc: 0.23 - ETA: 0s - loss: 11.8549 - acc: 0.23 - ETA: 0s - loss: 11.8859 - acc: 0.23 - ETA: 0s - loss: 11.8869 - acc: 0.23 - ETA: 0s - loss: 11.9197 - acc: 0.23 - 2s 23ms/step - loss: 11.9431 - acc: 0.2328 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 3/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 10.3257 - acc: 0.26 - ETA: 1s - loss: 7.3493 - acc: 0.1491 - ETA: 1s - loss: 9.9495 - acc: 0.159 - ETA: 1s - loss: 10.7675 - acc: 0.18 - ETA: 1s - loss: 11.0918 - acc: 0.20 - ETA: 0s - loss: 11.4004 - acc: 0.20 - ETA: 0s - loss: 11.5792 - acc: 0.20 - ETA: 0s - loss: 11.7585 - acc: 0.21 - ETA: 0s - loss: 11.8236 - acc: 0.21 - ETA: 0s - loss: 11.8574 - acc: 0.21 - ETA: 0s - loss: 11.8852 - acc: 0.21 - ETA: 0s - loss: 11.9861 - acc: 0.21 - ETA: 0s - loss: 12.0379 - acc: 0.21 - ETA: 0s - loss: 12.0587 - acc: 0.22 - ETA: 0s - loss: 12.0611 - acc: 0.22 - ETA: 0s - loss: 12.0585 - acc: 0.22 - ETA: 0s - loss: 12.0416 - acc: 0.22 - ETA: 0s - loss: 12.0084 - acc: 0.22 - ETA: 0s - loss: 11.9733 - acc: 0.22 - ETA: 0s - loss: 11.9486 - acc: 0.22 - ETA: 0s - loss: 11.9372 - acc: 0.23 - 2s 23ms/step - loss: 11.9179 - acc: 0.2297 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 4/100\n",
      "65/65 [==============================] - ETA: 1s - loss: 11.5849 - acc: 0.21 - ETA: 1s - loss: 12.1515 - acc: 0.21 - ETA: 1s - loss: 12.1605 - acc: 0.20 - ETA: 1s - loss: 12.2145 - acc: 0.20 - ETA: 0s - loss: 12.3210 - acc: 0.21 - ETA: 0s - loss: 12.3562 - acc: 0.21 - ETA: 0s - loss: 12.3934 - acc: 0.22 - ETA: 0s - loss: 12.2946 - acc: 0.22 - ETA: 0s - loss: 12.3807 - acc: 0.23 - ETA: 0s - loss: 12.3838 - acc: 0.23 - ETA: 0s - loss: 12.3633 - acc: 0.22 - ETA: 0s - loss: 12.3264 - acc: 0.22 - ETA: 0s - loss: 12.3530 - acc: 0.22 - ETA: 0s - loss: 12.3347 - acc: 0.22 - ETA: 0s - loss: 12.3509 - acc: 0.22 - ETA: 0s - loss: 12.3157 - acc: 0.22 - ETA: 0s - loss: 12.2596 - acc: 0.23 - ETA: 0s - loss: 12.2626 - acc: 0.23 - ETA: 0s - loss: 12.2344 - acc: 0.23 - ETA: 0s - loss: 12.1953 - acc: 0.23 - ETA: 0s - loss: 12.1670 - acc: 0.23 - ETA: 0s - loss: 12.0982 - acc: 0.23 - 2s 24ms/step - loss: 11.9095 - acc: 0.2323 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 5/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 13.8515 - acc: 0.31 - ETA: 1s - loss: 12.5923 - acc: 0.25 - ETA: 1s - loss: 12.3764 - acc: 0.24 - ETA: 1s - loss: 12.0382 - acc: 0.22 - ETA: 0s - loss: 12.2242 - acc: 0.22 - ETA: 0s - loss: 12.1673 - acc: 0.22 - ETA: 0s - loss: 12.2211 - acc: 0.22 - ETA: 0s - loss: 12.3404 - acc: 0.23 - ETA: 0s - loss: 12.2726 - acc: 0.22 - ETA: 0s - loss: 12.2970 - acc: 0.22 - ETA: 0s - loss: 12.3719 - acc: 0.22 - ETA: 0s - loss: 12.3260 - acc: 0.22 - ETA: 0s - loss: 12.3275 - acc: 0.22 - ETA: 0s - loss: 12.3284 - acc: 0.22 - ETA: 0s - loss: 12.3180 - acc: 0.22 - ETA: 0s - loss: 12.3404 - acc: 0.22 - ETA: 0s - loss: 12.3207 - acc: 0.22 - ETA: 0s - loss: 12.0746 - acc: 0.22 - ETA: 0s - loss: 12.0256 - acc: 0.22 - ETA: 0s - loss: 11.9974 - acc: 0.22 - ETA: 0s - loss: 11.9543 - acc: 0.22 - ETA: 0s - loss: 11.9464 - acc: 0.23 - 2s 24ms/step - loss: 11.9095 - acc: 0.2307 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 6/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 12.5923 - acc: 0.28 - ETA: 1s - loss: 11.0812 - acc: 0.25 - ETA: 1s - loss: 11.1316 - acc: 0.24 - ETA: 1s - loss: 11.4769 - acc: 0.27 - ETA: 1s - loss: 11.3051 - acc: 0.27 - ETA: 1s - loss: 11.2186 - acc: 0.27 - ETA: 1s - loss: 11.3242 - acc: 0.26 - ETA: 1s - loss: 11.5633 - acc: 0.24 - ETA: 1s - loss: 11.8313 - acc: 0.24 - ETA: 0s - loss: 11.8529 - acc: 0.23 - ETA: 0s - loss: 11.9444 - acc: 0.23 - ETA: 0s - loss: 11.8497 - acc: 0.23 - ETA: 0s - loss: 11.8926 - acc: 0.23 - ETA: 0s - loss: 11.9221 - acc: 0.23 - ETA: 0s - loss: 11.9787 - acc: 0.23 - ETA: 0s - loss: 11.9981 - acc: 0.23 - ETA: 0s - loss: 12.0423 - acc: 0.23 - ETA: 0s - loss: 12.0914 - acc: 0.22 - ETA: 0s - loss: 12.1149 - acc: 0.23 - ETA: 0s - loss: 12.1090 - acc: 0.23 - ETA: 0s - loss: 12.1412 - acc: 0.23 - ETA: 0s - loss: 12.1143 - acc: 0.23 - 2s 24ms/step - loss: 12.1751 - acc: 0.2312 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 7/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 12.0886 - acc: 0.40 - ETA: 2s - loss: 11.5849 - acc: 0.32 - ETA: 1s - loss: 11.2827 - acc: 0.31 - ETA: 1s - loss: 11.1531 - acc: 0.27 - ETA: 1s - loss: 11.1372 - acc: 0.27 - ETA: 1s - loss: 11.2872 - acc: 0.27 - ETA: 1s - loss: 11.2683 - acc: 0.25 - ETA: 1s - loss: 11.5315 - acc: 0.25 - ETA: 1s - loss: 11.6976 - acc: 0.25 - ETA: 0s - loss: 11.6594 - acc: 0.24 - ETA: 0s - loss: 11.7707 - acc: 0.23 - ETA: 0s - loss: 11.8556 - acc: 0.23 - ETA: 0s - loss: 12.0284 - acc: 0.24 - ETA: 0s - loss: 12.0533 - acc: 0.24 - ETA: 0s - loss: 12.0737 - acc: 0.24 - ETA: 0s - loss: 12.0434 - acc: 0.24 - ETA: 0s - loss: 12.0275 - acc: 0.23 - ETA: 0s - loss: 12.0492 - acc: 0.23 - ETA: 0s - loss: 12.0990 - acc: 0.23 - ETA: 0s - loss: 12.1268 - acc: 0.23 - ETA: 0s - loss: 12.1570 - acc: 0.24 - 2s 23ms/step - loss: 12.1499 - acc: 0.2461 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 8/100\n",
      "65/65 [==============================] - ETA: 1s - loss: 12.5923 - acc: 0.26 - ETA: 1s - loss: 12.9700 - acc: 0.23 - ETA: 1s - loss: 12.4484 - acc: 0.23 - ETA: 1s - loss: 12.4160 - acc: 0.21 - ETA: 0s - loss: 12.4484 - acc: 0.21 - ETA: 0s - loss: 12.1775 - acc: 0.21 - ETA: 0s - loss: 12.1605 - acc: 0.21 - ETA: 0s - loss: 12.2460 - acc: 0.21 - ETA: 0s - loss: 12.2685 - acc: 0.22 - ETA: 0s - loss: 12.2381 - acc: 0.22 - ETA: 0s - loss: 12.2181 - acc: 0.22 - ETA: 0s - loss: 12.2694 - acc: 0.22 - ETA: 0s - loss: 12.2865 - acc: 0.22 - ETA: 0s - loss: 12.3459 - acc: 0.22 - ETA: 0s - loss: 12.3606 - acc: 0.22 - ETA: 0s - loss: 12.3043 - acc: 0.22 - ETA: 0s - loss: 12.3521 - acc: 0.22 - ETA: 0s - loss: 12.3031 - acc: 0.22 - ETA: 0s - loss: 12.2574 - acc: 0.22 - ETA: 0s - loss: 12.2271 - acc: 0.23 - ETA: 0s - loss: 12.1947 - acc: 0.23 - 2s 23ms/step - loss: 12.1667 - acc: 0.2312 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - ETA: 1s - loss: 13.0960 - acc: 0.29 - ETA: 1s - loss: 12.2145 - acc: 0.21 - ETA: 1s - loss: 12.3044 - acc: 0.21 - ETA: 0s - loss: 12.1802 - acc: 0.20 - ETA: 0s - loss: 12.2229 - acc: 0.22 - ETA: 0s - loss: 12.3007 - acc: 0.22 - ETA: 0s - loss: 12.3952 - acc: 0.22 - ETA: 0s - loss: 12.4663 - acc: 0.21 - ETA: 0s - loss: 12.4160 - acc: 0.22 - ETA: 0s - loss: 12.3923 - acc: 0.22 - ETA: 0s - loss: 12.4493 - acc: 0.22 - ETA: 0s - loss: 12.4915 - acc: 0.22 - ETA: 0s - loss: 12.4606 - acc: 0.22 - ETA: 0s - loss: 12.4637 - acc: 0.22 - ETA: 0s - loss: 12.3157 - acc: 0.22 - ETA: 0s - loss: 12.3763 - acc: 0.22 - ETA: 0s - loss: 12.3200 - acc: 0.22 - ETA: 0s - loss: 12.3075 - acc: 0.22 - ETA: 0s - loss: 12.2702 - acc: 0.22 - ETA: 0s - loss: 12.2395 - acc: 0.23 - ETA: 0s - loss: 12.2067 - acc: 0.23 - 1s 23ms/step - loss: 12.1583 - acc: 0.2307 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 10/100\n",
      "65/65 [==============================] - ETA: 1s - loss: 11.7223 - acc: 0.18 - ETA: 1s - loss: 11.8081 - acc: 0.24 - ETA: 1s - loss: 11.4819 - acc: 0.25 - ETA: 1s - loss: 11.6021 - acc: 0.27 - ETA: 1s - loss: 11.4979 - acc: 0.27 - ETA: 1s - loss: 11.2605 - acc: 0.26 - ETA: 1s - loss: 11.5620 - acc: 0.26 - ETA: 0s - loss: 11.7051 - acc: 0.25 - ETA: 0s - loss: 11.7795 - acc: 0.24 - ETA: 0s - loss: 11.8236 - acc: 0.24 - ETA: 0s - loss: 11.9119 - acc: 0.24 - ETA: 0s - loss: 12.0644 - acc: 0.24 - ETA: 0s - loss: 12.1676 - acc: 0.24 - ETA: 0s - loss: 12.1318 - acc: 0.24 - ETA: 0s - loss: 12.1282 - acc: 0.24 - ETA: 0s - loss: 12.1445 - acc: 0.24 - ETA: 0s - loss: 12.1735 - acc: 0.23 - ETA: 0s - loss: 12.1293 - acc: 0.23 - ETA: 0s - loss: 12.1467 - acc: 0.23 - 1s 22ms/step - loss: 11.9347 - acc: 0.2323 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 11/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 10.3257 - acc: 0.26 - ETA: 1s - loss: 8.8146 - acc: 0.1797 - ETA: 1s - loss: 10.5145 - acc: 0.19 - ETA: 0s - loss: 11.2911 - acc: 0.22 - ETA: 0s - loss: 11.6321 - acc: 0.22 - ETA: 0s - loss: 11.7864 - acc: 0.22 - ETA: 0s - loss: 11.7108 - acc: 0.22 - ETA: 0s - loss: 11.7288 - acc: 0.22 - ETA: 0s - loss: 11.8761 - acc: 0.23 - ETA: 0s - loss: 11.9519 - acc: 0.23 - ETA: 0s - loss: 12.0111 - acc: 0.23 - ETA: 0s - loss: 12.0183 - acc: 0.22 - ETA: 0s - loss: 12.0564 - acc: 0.22 - ETA: 0s - loss: 12.0836 - acc: 0.22 - ETA: 0s - loss: 12.0233 - acc: 0.22 - ETA: 0s - loss: 11.9941 - acc: 0.22 - ETA: 0s - loss: 12.0017 - acc: 0.22 - ETA: 0s - loss: 11.9752 - acc: 0.22 - ETA: 0s - loss: 11.9342 - acc: 0.22 - 1s 22ms/step - loss: 11.9179 - acc: 0.2328 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 12/100\n",
      "65/65 [==============================] - ETA: 2s - loss: 9.8220 - acc: 0.203 - ETA: 2s - loss: 10.9133 - acc: 0.24 - ETA: 1s - loss: 10.8797 - acc: 0.24 - ETA: 1s - loss: 11.0452 - acc: 0.25 - ETA: 1s - loss: 10.9972 - acc: 0.25 - ETA: 1s - loss: 11.2186 - acc: 0.26 - ETA: 1s - loss: 11.2168 - acc: 0.28 - ETA: 1s - loss: 10.5027 - acc: 0.25 - ETA: 1s - loss: 10.8230 - acc: 0.25 - ETA: 1s - loss: 11.0637 - acc: 0.25 - ETA: 1s - loss: 11.2128 - acc: 0.24 - ETA: 0s - loss: 11.4687 - acc: 0.24 - ETA: 0s - loss: 11.6146 - acc: 0.24 - ETA: 0s - loss: 11.5890 - acc: 0.24 - ETA: 0s - loss: 11.6771 - acc: 0.23 - ETA: 0s - loss: 11.7541 - acc: 0.24 - ETA: 0s - loss: 11.7838 - acc: 0.23 - ETA: 0s - loss: 11.8029 - acc: 0.23 - ETA: 0s - loss: 11.8542 - acc: 0.23 - ETA: 0s - loss: 11.8439 - acc: 0.23 - ETA: 0s - loss: 11.8701 - acc: 0.23 - ETA: 0s - loss: 11.9020 - acc: 0.23 - ETA: 0s - loss: 11.9149 - acc: 0.23 - 2s 24ms/step - loss: 11.9347 - acc: 0.2338 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 13/100\n",
      "65/65 [==============================] - ETA: 4s - loss: 10.0738 - acc: 0.26 - ETA: 1s - loss: 13.0960 - acc: 0.21 - ETA: 1s - loss: 12.7362 - acc: 0.20 - ETA: 1s - loss: 12.5671 - acc: 0.22 - ETA: 1s - loss: 12.5563 - acc: 0.23 - ETA: 0s - loss: 12.5478 - acc: 0.22 - ETA: 0s - loss: 12.5545 - acc: 0.23 - ETA: 0s - loss: 12.4390 - acc: 0.23 - ETA: 0s - loss: 12.4430 - acc: 0.22 - ETA: 0s - loss: 12.3908 - acc: 0.22 - ETA: 0s - loss: 12.4244 - acc: 0.22 - ETA: 0s - loss: 12.4174 - acc: 0.22 - ETA: 0s - loss: 12.4244 - acc: 0.23 - ETA: 0s - loss: 12.4364 - acc: 0.22 - ETA: 0s - loss: 12.4076 - acc: 0.22 - ETA: 0s - loss: 12.3713 - acc: 0.22 - ETA: 0s - loss: 12.3888 - acc: 0.22 - ETA: 0s - loss: 12.3570 - acc: 0.22 - ETA: 0s - loss: 12.3339 - acc: 0.22 - ETA: 0s - loss: 12.3037 - acc: 0.22 - ETA: 0s - loss: 12.2294 - acc: 0.22 - ETA: 0s - loss: 12.2005 - acc: 0.23 - ETA: 0s - loss: 12.1734 - acc: 0.23 - 2s 24ms/step - loss: 12.1835 - acc: 0.2307 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 14/100\n",
      "65/65 [==============================] - ETA: 1s - loss: 11.0812 - acc: 0.25 - ETA: 1s - loss: 11.3330 - acc: 0.25 - ETA: 1s - loss: 11.1316 - acc: 0.25 - ETA: 1s - loss: 11.2611 - acc: 0.25 - ETA: 1s - loss: 11.2211 - acc: 0.26 - ETA: 1s - loss: 11.1728 - acc: 0.27 - ETA: 1s - loss: 11.2827 - acc: 0.27 - ETA: 1s - loss: 11.6942 - acc: 0.24 - ETA: 1s - loss: 11.8109 - acc: 0.24 - ETA: 0s - loss: 11.8257 - acc: 0.24 - ETA: 0s - loss: 11.9172 - acc: 0.23 - ETA: 0s - loss: 11.9465 - acc: 0.23 - ETA: 0s - loss: 11.9623 - acc: 0.23 - ETA: 0s - loss: 11.9812 - acc: 0.23 - ETA: 0s - loss: 12.0081 - acc: 0.23 - ETA: 0s - loss: 12.0516 - acc: 0.23 - ETA: 0s - loss: 12.0399 - acc: 0.22 - ETA: 0s - loss: 12.1018 - acc: 0.22 - ETA: 0s - loss: 12.1268 - acc: 0.23 - ETA: 0s - loss: 12.1763 - acc: 0.23 - 1s 23ms/step - loss: 12.1919 - acc: 0.2317 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 15/100\n",
      "65/65 [==============================] - ETA: 1s - loss: 11.8367 - acc: 0.21 - ETA: 1s - loss: 12.7811 - acc: 0.25 - ETA: 1s - loss: 12.8441 - acc: 0.26 - ETA: 1s - loss: 12.7182 - acc: 0.24 - ETA: 1s - loss: 12.6891 - acc: 0.23 - ETA: 0s - loss: 12.6219 - acc: 0.22 - ETA: 0s - loss: 12.4843 - acc: 0.22 - ETA: 0s - loss: 12.4109 - acc: 0.22 - ETA: 0s - loss: 12.3752 - acc: 0.22 - ETA: 0s - loss: 12.4244 - acc: 0.22 - ETA: 0s - loss: 12.3676 - acc: 0.22 - ETA: 0s - loss: 12.3466 - acc: 0.22 - ETA: 0s - loss: 12.3180 - acc: 0.22 - ETA: 0s - loss: 12.3558 - acc: 0.22 - ETA: 0s - loss: 12.3207 - acc: 0.22 - ETA: 0s - loss: 12.0678 - acc: 0.22 - ETA: 0s - loss: 12.0371 - acc: 0.22 - ETA: 0s - loss: 12.0041 - acc: 0.22 - ETA: 0s - loss: 11.9943 - acc: 0.22 - ETA: 0s - loss: 11.9852 - acc: 0.22 - ETA: 0s - loss: 11.9530 - acc: 0.23 - 1s 23ms/step - loss: 11.8843 - acc: 0.2312 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 16/100\n",
      "65/65 [==============================] - ETA: 3s - loss: 10.3257 - acc: 0.26 - ETA: 1s - loss: 8.8775 - acc: 0.1992 - ETA: 1s - loss: 10.2177 - acc: 0.20 - ETA: 1s - loss: 10.6531 - acc: 0.20 - ETA: 1s - loss: 10.9843 - acc: 0.21 - ETA: 0s - loss: 11.3479 - acc: 0.21 - ETA: 0s - loss: 11.5489 - acc: 0.22 - ETA: 0s - loss: 11.5446 - acc: 0.22 - ETA: 0s - loss: 11.6717 - acc: 0.21 - ETA: 0s - loss: 11.8367 - acc: 0.21 - ETA: 0s - loss: 11.8997 - acc: 0.21 - ETA: 0s - loss: 11.9941 - acc: 0.22 - ETA: 0s - loss: 11.9913 - acc: 0.22 - ETA: 0s - loss: 12.0350 - acc: 0.22 - ETA: 0s - loss: 12.0787 - acc: 0.22 - ETA: 0s - loss: 12.0585 - acc: 0.22 - ETA: 0s - loss: 12.0505 - acc: 0.22 - ETA: 0s - loss: 12.0215 - acc: 0.22 - ETA: 0s - loss: 11.9985 - acc: 0.22 - ETA: 0s - loss: 11.9608 - acc: 0.22 - ETA: 0s - loss: 11.9372 - acc: 0.23 - 1s 23ms/step - loss: 11.9179 - acc: 0.2312 - val_loss: 11.9619 - val_acc: 0.2227\n",
      "Epoch 17/100\n",
      "13/65 [=====>........................] - ETA: 1s - loss: 11.7223 - acc: 0.18 - ETA: 1s - loss: 12.3175 - acc: 0.22 - ETA: 0s - loss: 12.0759 - acc: 0.20 - ETA: 0s - loss: 12.0410 - acc: 0.2159"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-fd9e6354998b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-107-1775ce731b66>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     19\u001b[0m         model.fit_generator(train_generator, epochs=epochs,\n\u001b[0;32m     20\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                             shuffle=False, verbose=True)\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# No bucketing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicholas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=transform(data['1'], data['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "#sentence_input = Input(shape=(None,), dtype='int32')\n",
    "#embedded_sequences = embedding_layer(sentence_input)\n",
    "#l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "sentence_input= Input(shape=(None, 300))\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(sentence_input)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    " \n",
    "#review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_input = Input(shape=(7,None), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "han=Sequential()\n",
    "han.add(Bidirectional(GRU(units=100, return_sequences=True), input_shape=(6,300)))\n",
    "han.add(TimeDistributed(Dense(200)))\n",
    "han.add(AttLayer())\n",
    "classifier.add(Dropout(0.3))\n",
    "classifier.add(Dense(units=4, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>anger, fear, joy, sadness</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
