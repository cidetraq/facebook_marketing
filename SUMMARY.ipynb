{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Emotion Detection From Text using Neural Networks: My Progress to Date</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "Before landing on neural networks, I tried other approaches like SVM and Naive Bayes classification, but they performed poorly or not at all. I don't know if that's because they just don't work for this approach or that I implemented them wrong. Probably the latter. They are supposed to work well for polar sentiment classification (positive vs. negative sentiment)- does somebody like this product or hate it? \n",
    "\n",
    "I turned to neural networks some time afterwards and soon got promising results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 1: 6-emotion LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Tools used: \n",
    "keras, scikit-learn, nltk, matplotlib, numpy, pandas, gensim</i>\n",
    "\n",
    "1. Get Vector representations of words. I used ones pre-compiled from wikipedia. This won't have all the slang present in tweets, so when out-of-dictionary words come up, the input is ignored. \n",
    "\n",
    "2. Clean and preprocess- Stopwords, taking into account negation\n",
    "3. Write functions to transform string input into vectors\n",
    "4. Read in the data. \n",
    "5. Neural Network\n",
    "Create a multi-layer LSTM with softmax output layer to output some function output similar to probabilities for each emotion. \n",
    "Initial model had 48% test accuracy. \n",
    "With revisions and more timesteps, I got it up to 87% test accuracy. \n",
    "\n",
    "<h3>Data</h3>\n",
    "I will cut straight to the point: The biggest challenge for my project is scarcity of training data. I have only 7-10k rows at max to train on (row being a sentence or collection of sentences) meaning that there's not enough language variation present for the model to pick up on nuances of speech like negation and sarcasm. Of course, even with gigantic training sets, this very human-like part of speech comprehension is just outside the grasp of cutting edge NLP technology. I initially thought I could make some limited progress here but I've been stymied. \n",
    "\n",
    "I moved onto 4-emotion LSTM which is my current best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 2: Data Augmentation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in order to get my neural network to recognize negation (I am not happy is different than I am happy) I created fake data with \"not\" + emotional words and assigned the correct emotion to each pair. For example, one synthetic row would be 'not joyful not joyful not joyful' (repeated three times because of the format of my LSTM) and the emotion for that synthetic row would be sadness. \n",
    "I created around 3000 examples of this data, fed it to the LSTM, and presto! It could recognize negation. \n",
    "\n",
    "I didn't want to do this approach much more because that would mean I would have to create data to represent variation in human language, which I felt would be too hard-coded and old fashioned for the 21st century. So I tried to find techniques that would help my LSTM automatically recognize nuances in speech even with small training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 3: Spacy NLP</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used part-of-speech tagging, dependency parsing in sentence structure, along with the existing format of six timesteps. This was a pain to program because it would seem the parts aren't made for each other. When I finally got it to run, the end result was at least 10 percentage points less accurate than my previous model despite it having more structured information for each timestep. And no, it did not pick up on negation. So I moved onto Attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 4: Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what I was talking about a month ago that I thought might help me break through. I implemented it and as far as I can tell it's comparable, but less accurate than my previous best. \n",
    "\n",
    "Here's attention in a nutshell: \n",
    "\n",
    "First, the LSTM layer produces a matrix H of output vectors of length T (T being the sentence length, because attention is implemented per-sentence.)\n",
    "\n",
    "H is within (dimension of word vectors * sentence length) dimensional space. \n",
    "\n",
    "The representation of the sentence is formed like such: \n",
    "\n",
    "M= tanh(H), tanh being an activation function which is steep between -1 and 1, but converges to -1 and 1 past x=2, -2. So strongly negative inputs will be mapped strongly negative and zero inputs will be mapped near zero. (whatever that means in application, I'm not sure.)\n",
    "\n",
    "alpha= softmax(transpose of weight, which gets updated with every pass by the attention mechanism * M from the previous step)\n",
    "softmax is another activation function\n",
    "\n",
    "r=H*transpose of alpha\n",
    "\n",
    "finally, \"We obtain the final sentence-pair represnetation used for classification  from \n",
    "\n",
    "h* = tanh(r)\n",
    "\n",
    "The classifier takes h* as input and uses softmax activation function to return values between 0 and 1.\n",
    "\n",
    "Attention can also be computed on entire documents consisting of multiple sentences. \n",
    "\n",
    "What does attention do? It focuses attention of the classifier on the most relevant word in the sentence or the most relevant sentence in the document to improve accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Future Plans, Ideas</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now I am more focused on the DASH pollen project just because it's a change of pace and its current progress requires hands-on applications of code, rather than a bunch of scouting around for options like I have done with this emotions project. \n",
    "\n",
    "That being said, here are my ideas for the future of the emotions project: \n",
    "\n",
    "1. Do some more hard-coded augmentation of data. I don't like this idea very much but it might be the most fruitful in the short term to get working results for several common cases of language. \n",
    "2. Google BERT. BERT is a framework for Natural Language Processing with several pre-trained models that are state of the art in several tasks. There might be a way to integrate a pre trained model with this project. \n",
    "3. (In the next ~3 years) Graph-based AI. This is a very recent development in AI that promises to more closely emulate human reasoning. I haven't seen any papers using it so far. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
